Thank you, everybody. Thank you, Joelle. It's true, I'm kind of the, I have the most gray
hair here. I've been around for a long time and I want to kind of talk about the long
view a bit. Actually, so this is the challenge. First, let me say, though, it's really exciting
just to talk to all you young people learning about the field, learning about, wanting to
get into AI and reinforcement learning. And it's just like, it's a special opportunity.
I really appreciate it. And so when I do this, I have to think, what can I try to communicate
to you? Because, you know, we have really a brief amount of time and the field is immense.
I've written a whole book on the subject. And so what do we, what can I try to do? Now,
my topic normally today is temporal difference learning. And certainly I'm going to talk
about that. But I also want to, you know, just speak to you as young people entering
the field and just tell you what, the way I think about it from a sort of a longer view.
And well, so I think the first thing to be said along those lines is that this is a really
special time. And we can look forward now to, to doing an amazing thing, which is maybe
understanding the way the mind works, the way intelligence works. This is, this is like
a monumental event, not just, you know, this century, you know, for thousands of years,
maybe in the history of the, of the earth, when intelligent beings, animals, things that
can replicate themselves, finally come to understand the way they work well enough to
by design create intelligence. And so the big thing that's going to happen is that we're
going to come to understand how the mind works, how intelligence can work, how it does work.
And just the fact of our understanding it is going to change the world. It's going to
change, obviously, there'll be lots of applications, but just, it'll change ourselves, our view
of ourselves, what we do, what we play with, what we work at, everything. It's a big event.
It's a big event. And we have to, we should keep that in mind. And as I say, you know,
I'm not saying it's going to happen, you know, tomorrow. It's not, but you know, 10 years
it could happen, 30 years it could happen. And then the great sweep of history, that's
a small amount of time. It's going to happen within your lives with high probability. So,
so the first thing we have to ask is why is this happening? And I think it's really comes
down to like Moore's law. Okay, so I took a slide from Kurzweil. This is his standard
slide of the increasing computation per dollar. And it's just reaching and a point where
it starts to compete with the computation that we can do with our own brains, with
natural systems. Okay, that's happening now. And it's just going to continue increasing
dramatically. I'm sure you've seen these kind of slides. Here we have time, years along
the bottom. And on the other, we have computations per second per dollar. Okay, and you see it's
a log scale. So, of course, a straight line would be exponential increase. And Kurzweil
argues that it's actually slightly super exponential. But the point is, the computations become
available to us has become enormous. And that changes things. It's the rise of deep learning.
And all that is purely because of GPUs and computation getting cheaper. And that will
only continue and it will become more extreme as a vastly increased computation. So, that
alters everything. And it really is profound. I'm just going to try to slide to say what
I conclude from this. Because I think it's an answer to a question for AI that's had
for 60, 70 years. So, the answer, I'm not going to, I'm just going to do a couple slides
like this. But I guess I should, one last bit of preface is that my talk today is going
to be like half, these are all these big things. And like I'm starting to do now. And half
like really small things where you go back to the foundations, real stuff that's basic
that we need to understand. I think the details, the big pictures is important. But the details
also really matter. And like we heard a lot of specific algorithms early today, exactly
how they work is really important. The details matter, but the big picture matters. Sometimes
we lose sight of the big picture. Sometimes something becomes so obvious that we overlook
it. I think the computation is as long as that lines. And its implication, its implication
in a phrase is that methods that scale with computation are the future of AI. Okay? And
that's, that's by scale computation I mean is we get more computer power, those methods
become more powerful. And this has not always been true. But certainly it's true of learning
and search methods. These are what we call general purpose methods. And it's the answer
to this, one of the oldest questions in AI for 60 years old. Do we want weak methods?
That's what they call them in the old days. They call general purpose methods weak. Because
they just, they just use the data or use computation they can only, they're, they're general purpose.
They're general, they're weak. Okay? That's what they call them in the old days. The strong
ones were the ways that would lose human knowledge and human expertise and human insight to make
their system so much better. Okay? Now, probably you guys are thinking that's crazy. The general
purpose ones. Are you thinking that with me? General purpose? How many of you guys go human
insight? Oh, some of both? Okay. Yeah. That's, that's, that's nice compromising sort of
position. And so all today I'm going to try, or all in this big part of the talk, I'm going
to try to talk about, I'm going to present strong views. Okay? Really maybe you should
do compromises and nuances, but it's good to talk about strong views because they give
you a working hypothesis. They give you a point of view. And you know, you can say this is a
strong point of view. You don't have to believe it. You have to say, well, that's a strong
point of view that I should be thinking about. And then maybe you have several of them. Anyway,
so I'm going to present the strong point of view that this is all this question has been
answered and it's been answered in favor of the weak methods. And yeah. And now I don't want you
to, I could talk about this all day, but I'll refrain from it. I'll just note the next thing
to say is that you may, you're thinking you're good. You're thinking you're into deep learning
or reinforcement learning. So you're on the right side of history. But I'm not sure that's
right. Okay? Because we take things like any kind of supervised learning, even model free
reinforcement learning, the thing I love more than anything, right? It's only weekly scalable.
If I had, you know, a million times it's computation, I'm still limited in my model
for reinforcement learning by how fast I can gather data. And, you know, I'm only learning a
value function, maybe a policy. What's the big deal? It's a tiny object and some map from states
to what to do. Okay? And that's not a big thing. I don't care how many features you have. It's
and super, certainly for supervised learning, it's only weekly scalable because it requires
people to provide data sets. Okay? And they become the bottleneck. If you have, you know,
you get people to label things on the web, things scale, but only weekly. Only as fast
you can gather the data. And eventually that becomes a bottleneck. You really want to be
able to learn from raw data. You want to be scalable. Okay? So if these things that we love
are not scalable, what is scalable? What would be fully scalable? Well, my answer is simple. It's
what I call prediction learning. Prediction learning means learning to predict what will happen.
Okay? And so it's the unsupervised supervised learning because we have targets. We're
supervised learning. We just wait and we see what does happen. And that's our target. Okay?
And so you don't need to have human labeling or you have a target, but you don't need a human to
provide it. Just get it from the data. So you see unsupervised supervised learning. But anyway,
it's definitely the scalable model free learning. And maybe it's the scalable model free learning.
And prediction learning is at the heart of all of our control methods where you learn value
functions. I think maybe that's sort of the argument I wanted, the big argument about temporal
difference learning is that it's the scalable model free method. And of course, I haven't given you
even a step towards that yet. I'm just saying that the idea of predicting, of predicting,
of having learning that predicts is the key problem. It's what we should be thinking about.
And you may say, well, deep learning, supervised learning is all about prediction learning,
but that's not true. It's all, it's about predicting what the other, what the label will be. But
it's not about predicting over time. It's not about predicting where you have to wait and see
what happens. And that makes all the difference. Okay, so, so with all these words, I want to
ground us a little bit and remind us what the data, what the data looks like. So I put in the
slide of, this is what real life is like. Real life is a temporal stream. We have things like
we're playing soccer and we have to make actions at every moment. We're maybe a hyena being chased
by a lion and trying to predict whether it's going to live or die. We're maybe a baseball player
and we're, our eyes are watching this tiny little ball flash by us really fast. I have to swing
at just the right moment to hit the ball. Or maybe you're talking to someone, you're trying to
predict what they will do. So this is the data streams that AI should be learning to deal with.
And so we should always keep this in mind. When I say learning to predict, well, I think, I do
think that hyena, trying to predict, you know, fear, fear is your, is your prediction of are you
going to die? Okay, so he's trying to predict it. Several times it looks good and bad. And at the
end here, it's not looking so good. Okay, so with those prefaces, let me start talking about
temporal difference learning. Temporal difference learning. Temporal difference learning, it's a
method for learning to predict. It's why we use reinforcement learning to predict future war,
value functions. It's basically the center of the core of many methods that you know about Q
learning and SARSA, TD Lambda, deep Q networks, TD Gammon, the world champion, backgammon player,
using deep reinforcement learning from 15, no, 25 years ago. My God, 25 years ago,
deep reinforcement learning, 1992. But not all reinforcement learning methods. So AlphaGo,
it happens not to use TD learning, uses other kinds of reinforcement learning. The helicopter
thing that you might have heard, that doesn't use it. And then there's sort of pure, like Peter's
talk was interesting because he talked about policy based methods and some of those don't use
temporal difference learning, but eventually he would get to it and put it in to make things better.
So it's sort of, yeah. So it's sort of optional. It feels like it's optional. Many people will say
it's optional, but I do want to argue that you do want to use it, that really you should always be
using it. It's ubiquitous. And okay, it seems to me how the brain works, what else do they, oh,
now it can be used to predict anything. It's for general method for prediction learning,
not just for rewards. Okay? So don't be fooled by, and reinforce them. When you read all about TD,
it's all about predicting value functions and reward. But we can use it for anything. And since
my talk is just about TD, I want to be sure to think about this general use. Question?
Not saying that it's key to AlphaGo, but don't they use TD learning to train
value functions that they use to evaluate the states?
Yeah, I've forgotten exactly the details. The initial learning system was learned from
actual outcomes, actual games, and they would go all the way to the final outcome. They would make
a prediction, look at a position, make a prediction of how am I going to win or lose,
and they would go ahead and see who actually won the game at the end.
And they would use who actually won instead of a temporal difference learning. So that's
what we want to work for. Do they wait and see who actually won? Do they see the outcome or the
return? Or do they update a guess from a guess? Okay? Let me make that clear. So that's my next
slide. As a TD learning, what is TD learning? Basically, it's learning a prediction from another
later prediction. Okay? So if AlphaGo is looking at a position, make a prediction, and then zooming
all the way to the end of the game, see who won, and then it's not TD. It's not learning that
prediction from a prediction. On the other hand, if you make a prediction from a position,
and then you make one move, and you see your next prediction, and you use that next prediction as
to form a target, then you're doing TD learning. Okay? So the quick word, quick phrase is we are
learning a guess from a guess. Okay? Sounds a bit dangerous, doesn't it? What would constrain it?
What would tie it down? Okay? But that is the idea. We want to learn an estimate from an estimate.
And we have to talk about whether this is good or not. Okay? The TD error, the TD error is the
difference between two predictions, two temporally successive predictions. Right? So if you're playing
your game, you say, I think I'm winning, then you take another move, you know, and now I think I'm
losing, you try to learn from that. You don't know who actually is going to win yet, but you try to
learn from the temporal difference in your predictions. Okay? Now, after that, it's the same
as what you're all used to. You just have an error, and you send it back proper through or whatever
you're doing. And so it's just really, where does the error come from? Or where does the target come
from? Does the target come from the end of the game, or does the target come from the next
prediction? Okay? So here's the example of TD Gammon, originally 1992, and he had a chest, a
back end position, and he would send that into a neural network, which would filter through
actually just a single, well, he had many versions, the standard version was a single hidden layer,
and he'd end up with this probability of winning. And the error then was the probability of winning
in one position minus the probability of winning in the next position. So we look at the change
in the estimated probability of winning, and that was used as the error that would be back
propagated through it. And this would learn just sticking in the corner, playing against the
self, learning from itself, from its own trial and error, and it came out to be
competitive with the world's best players, really the best in the world.
Okay, so that's familiar. Now, I'm trying to get to this question. This question is,
do you need to use TD Learning? Because this is all this is to motivate. The motivation is the
most important. Do I need to use TD Learner? Can I get away with it? Because if you go to the field
now, maybe even in reinforcement learning, you'll find a good fraction of the people
don't believe in TD Learning. They think they can get away without it. And so it's a real question
we should all be asking. Do we need it? I want you to understand. I want you, even as people
learning about the field, to be able to engage with this question and know the basic facts
pertinent to whether we need to use TD Learning. So I will skip over Atari.
And you've seen that already. So TD Learning, when do we need it? Okay. Well, it's only relevant
on multi-step prediction problems. That's the first thing. Only when the thing predicted is
multiple steps in the future. So obviously, if you're predicting down from the game,
that's multiple steps in the future. But if you're predicting a label or in AlphaGo where the data
was the initial date, at least, was here's a guess, and then I'll just see who won the game.
And if you just use who won the game, then it's essentially a one step prediction.
Okay. So now I want to say that it's really broadly applicable. I said it's only applicable
when you have multi-step predictions, but really everything you want to do is going to be multi-step
prediction. If you want to predict, oh, I have some examples, multi-step prediction. If you want to
predict the outcome of the game, if you want to predict what a stock market index will be, well,
really, you could just predict what it's going to be, but you'll get more data on every day
after that, and you'll make new predictions. So you'll make repeated predictions about a
long-term outcome. If you want to predict it will be the next president. You can predict that
every day as a new event happens. If you want to predict who the U.S. will go to war against next.
So all these are long-term predictions. They don't jump to the end.
Now, even if you want to predict a sensory observation, if you want to predict just the
very next sensory observation, that would not be multi-step prediction. But if you were to predict
even 10 steps ahead or a discounted measure of the future, those are long-term or multi-step
predictions. And we think about, go back to the real world, the hyena and the lions, or the
conversation, or Messi playing soccer. He's got to make long-term predictions. It's not,
what's the next thing? Am I going to make this goal? Will I get around that fellow?
Where's the wall going to be in a few milliseconds from now? All these things are multi-step predictions.
Now, can we treat it? Can we just use our one-step methods? Sure, things happen bit by bit,
but ignore that. Just wait and see what happens. See who won the game and use our one-step methods.
And, or can you learn a one-step model and then compose your model? Okay? And the answer is that
we really can't do these things. And I want to try to give some sense of that today. I'm really
going to talk mainly about this first question. Can we think of the multi-step case as one big step,
or do we have to deal with it bit by bit? And I want to do one slide on the second bit. The second
bit is, can we learn one-step predictions like a model and then iterate them to get a multi-step
prediction when you need to? And I just, I just want to say that I think it's a trap, okay? And I
don't know if I could really properly explain this to you, but I think it's a trap. I think it's
enough to model the world to make like a low-level simulation of the world, to make like a, I think
treat the world as a mark of decision process and model the transition probabilities, or treat the
world as a, as a, as a engineering model where we just have to learn the velocities and the
effect of the, on the accelerations or actions and then integrate this low-level differential
equation. These, this is all a trap. These short-term models and then iteration is, it feels good
because we know, we know that if it can be done perfectly on the one step, then it can be done
perfectly for however far we want to look into the future. But there's two problems. First of all,
we can't do it perfectly. And when we do it imperfectly with it, because we're always going to
have an approximation, then when we try to iterate them, we, we get a propagation of errors, a
compounding of our errors, and we get a useless long-term prediction. And secondly, of course,
it's exponentially complex, because as we look ahead, each step, there'll be many possibilities,
the world is stochastic, and also our actions, maybe we have, we have different choices we might
look at for our actions. So it quickly becomes computationally intractable, and it will always
be computationally intractable to try to look ahead many small steps into the future, like to
try to, try to iterate your model of physics to get to, you know, how much fun will I have going
to Montreal and taking the summer school? Okay? It's crazy. And it just doesn't, there's no future
in that, that way of thinking. It's a trap, and lots of people are, in my opinion, are falling
into it. Okay. But let's go back to the other side. The other side, remember, I'm just going
backwards here. We have these two things, two ways to get away from TD, if we don't want to get away,
if we don't like TD. Can we learn to model and iterate it? That's the second one. Or the first
one. Can we think of it as a one-step thing and just do the one-step thing? Okay. So the one-step
thing, I'll do it, I'm going to do it in, I'm about to transition to my low-level part of the talk,
but I don't want to try to answer it here. Just at the high level, and then maybe we can even take
questions after this, this high-level part of the talk. Can't we just use our familiar one-step
supervised learning method and, and reinforce them? Maybe these are known as Monte Carlo methods. I
mean, you just roll it out or whatever, see what happens, and use that what happens as a target.
Okay. So this has costs. Number one cost is, is you have to, you make, you're making this
prediction, then you're rolling it out to the end. And really, you're going to make a prediction
at every moment in time. So you've got to remember all whole mess of predictions. As you, as you go
out to the end to see the outcome, or really, if you have returns, you, you get the outcomes of
different, different steps, different outcomes, you have to relate them back to the, to the earlier
situations. It's horribly complex. It's nasty. Okay. It's, first of all, you need to remember all
the things you did. Think about yourself. Maybe you're, you're the lion there. You're trying to
make a good prediction. Okay. And what do you have? You have all the stuff swirling around you. You
have the hyenas running away. You have a glimpse of him. You're, you're feeling of your feet. You have
all the stuff that you can sense. And, and then you want to relate that to how well it's going,
how, how good you should feel about this chase. Okay. And so what is it sensible to think,
or just be much better if you can do it now, if right now, when all the stuff is in your mind
and in your sensors, learn if it's going well or going poorly and learn now, as opposed to
wait, wait five seconds later when you have had, you know, a whole number, a large number of frames
of different, different sensations and different patterns of sensation. You've forgotten, you
know, if you had to wait five seconds till the end of it, it's too late. You can't remember all
that. Whatever you remembered will be a tiny shadow of the real vivid representation you had
at the time it happened. Okay. And of course the computation is, is, is poorly distributed. You
can't learn now. So what are you doing now? Later you'll find an outcome. You'll have to do all the
learning then. Just a poor temporal distribution. And you can avoid these problems with special
methods. And that's what really what TD is about, specialized method for the multi-step case.
And another reason that you don't want to wait is that sometimes you never know the target.
Like, I don't know, let's say you're playing your chess game and there's a fire alarm and you never
finished your game. So you never see a final outcome. But if you're TD, you know, maybe you
thought you were winning and then, you know, it was going really poorly. And so you did something
bad. You can learn without waiting until you're checkmated. Maybe the fire alarm just like one
move away from the checkmate. So technically the game never ended. And, but you can learn a lot
from your experience. So obviously we could try to ignore all these things. Think of them as nuisances.
But I think of them as clues. These are hints from nature, but how we should proceed. Okay.
Okay. So now I'm going to get down to it more technically.
But I hope you're starting to see the view I'm trying to present.
We really need to learn from new predictions so that we can do it as we go along.
And I think it's really ubiquitous in all the different kinds of learning we're looking at in AI.
Okay. So I'm going to use notations a little bit different than what we've heard earlier today.
That's what I call my new notation. I use it in the second edition of the
Reinforcement Learning Textbook. The big thing is that we're trying to use the
statisticians convention that random variables are capital letters and instances are lowercase
letters. So all of the things that happen that make up life are capital letters, right? Because
they're the random events that have actually happened. They're not possibilities. They're
whatever happened. So S0 is the first state. A0 is the first action taken in that state.
R1 is the first reward that depends on that state. I'm taking that action in that state.
And then at the same time as we get the reward, we get the new state. So I like to give them
the same temporal index. R1 and S1, they occur together. They're jointly determined in fact.
Okay. And then life goes on and on. And that's the data. That's all we have
in terms of data. We have trajectories and maybe a single trajectory. And we're interested in
classically in the return. And the return is a sum of rewards. And I'm not going to use capital
R for the return because capital R is actually the actual rewards, the sequence of rewards.
And so I'm going to define the return. I need a new letter. I'm calling it G, capital G,
because it's a random event. Whatever sum of rewards after time t actually was,
we're going to call that G of t. G of t is the return. And as we note here, if I can use this
thing, this dot just means that this is a definition. It's not a statement of something
that's true because it follows from other things that I've said. It's a definition. So
G of t, the return, is the sum of the future rewards with the discount rate is the most
common way of dealing with it. And if that of course can be written, now we're going to use
equality because it's not a definition, that equals the first reward plus gamma times the sum of all
the later rewards. We're just taking one factor of gamma out of all of these guys. So this is no
no gamma. This one is one gamma out. This guy is two gammas. We just take the one out.
And then the rest of this is the sum of future rewards from one time step later. So it's a
bit like what we started with. It's just like the same thing. It's G of t plus one. This is just
a definite. This is a true equality here. Any return can be written as the first reward plus
gamma times the next return. And that is going to be the basis for our temporal difference
learning because we're going to use this as a target. We're going to use the next reward
plus gamma times the next return essentially as a target. I guess that's going to be explained
right next. We look at a state value function. Since we're using capital letters for the random
variables, I can't use capital V for the true value of the function. It's got to be lowercase.
It's a function of the policy. So V pi of s or s is any particular state. It's an instance,
any state, lowercase s. Its value is the expectation of the random variable, the return,
if we started in stat state s. So what we can expect the return could be under pi,
pi is the policy. So it's some way of picking the actions. Of course, the value of state depends
on what you do. If you dance at the top of the Grand Canyon, it might be bad. But if you
can sedately walk up to the railings, it's good. So policies, values depend on policies.
So then, since this is the return, we can just use the above equation. The return can be
written as the first reward plus gamma times the rest of the return. And then since we're
taking the expectation of this, this is the expected next reward and expected value of the
next state. So that naturally leads to the notion of an error. We can compare the estimate,
estimated value of a state at some time to the reward and the estimated value of the next state.
So that's going to be our TD error. This is what we're going to use to replace
the normal conventional error. Now this V is a random variable. This is our estimate.
And our estimate will depend on what happens. And so that is random. The estimated values
are a random value or a random function. And so it's capital. And that's our TD error.
You got it? The TD error? Is that clear? Good. Okay. So now let's talk about our methods.
I want to contrast supervised learning. And remember I said supervised learning is called
Monte Carlo in this context. So what exactly is that? That's we take our estimated value
function for the state that we run into V of S of t. Okay. We're going to update it based upon
some experience. Okay. Here's the experience. Here we are at S of t. And this is the tree of the
things that might happen. Like we might pick either of these two actions. And if we did pick
this action, either of these two states might arise. So yeah. The black dots are actions.
The open dots are states. So basically this is the tree of all the features that might happen.
And in this case, we're imagining that there are terminal states. We're basically adding things up
until we reach a terminal state. So here is a particular trajectory that might happen. Let's
say it did happen. From that state, we went this, this, this, this, this, and then we terminated.
Okay. So we now, once we've terminated, we know what g is. We know what the return is. And we can do
this update rule. We can compare our estimate for the state at time t up here to the actual return.
And we make that, that error. And then we do an increment. I should, this is the step size alpha.
So there's a number like 0.1. So we increment towards, towards this target. Okay. That's,
that's a standard, it'd be a Monte Carlo learning rule, a supervised learning rule. And that's
the, the competition for the TD method. The simplest TD method looks instead like this.
We only look ahead one step. We're at S of t. We look ahead at one. We see the reward that happens
and the next state that happens. And based on those, we form this TD error, which is again,
it's comparing, comparing, we're updating the, the estimate of this for the state at time t.
So we're going to make an error between what we were guessing for that state and this
new target, the reward plus gamma times the estimate value of the next state. Okay.
Now you've probably also heard about dynamic programming and you can think, you can put dynamic
programming in the same figure, the same kind of figure. And if you were, the dynamic program
version looks like this because it's not considering a single line through the possible tree. It's
considering all possibilities. It's considering both actions and both possible next states. So
this is where you need the model of the world because although you, you know, you're probably
picking each, each action, the probability that the world will give you possible next states will
be known only to the world. But in dynamic programming, you assume you know all that.
So in dynamic programming, the equation is that the value, the estimated value for a state
is moved towards the expectation of the first reward and the expectation of gamma times the
value of the next state. Okay. So there's this expectation and that's what makes it dynamic
programming because you see me know all the probabilities. You can figure out that expectation.
It doesn't give you the answer because your value will still be, you're still learning a guess
from a guess. You're learning your new estimate, still from your old estimate. But that's dynamic
programming. So, so really we can say the following. What's special about team methods is they bootstrap
and sample. So bootstrapping is this idea that your target involves a guess and an existing
prediction. Okay. So Monte Carlo, Monte Carlo, the whole point is that it doesn't bootstrap.
It's just looking all the way to the end and seeing what the return is. There's no,
there's no estimates playing a role in the return. Dynamic programming also bootstraps.
Dynamic programming says look ahead one step and look at the expected value of the next state.
And back it up. So you're only, you're using your estimates and your estimates gradually get
better. TD of course also is using your estimate. Yeah. It's like Monte Carlo and TD are learning
methods. I guess that's my next point. The learning methods, Monte Carlo and TD, they sample. They
sample what happens because you don't know how the world works. And dynamic programming just,
they're not sampled. This uses the expectation. It assumes you know what will happen, what could
happen. So those are the two basic dimensions. Whether you're sampling and therefore learning
and whether you are bootstrapping. You're using your, your, your bootstrapping, your estimate
from other estimates. You're learning guesses from guesses. And so TD prediction,
basically I'm just saying this is, this is the update you saw before erupting.
The Monte Carlo is here and the TD is there. So just the contrast is that one the target is the
actual return and the other is the target is this sort of one step estimate of what the return will
be. Okay. Now let's think, let's do an example. So here I am. I'm coming home after working
a hard day at the office and I'm trying to guess how long it will take me to get home.
Okay. So I'm, I'm leaving my office. It's Friday at six o'clock. I have some other features and I
make a guess of how long it will take. So I will, I'm going to guess it'll take 30 minutes to get home.
Okay. So, and that's, that's my prediction of my total time because I haven't gone, you know,
I'm just starting now. So my elapsed time is zero. Now, as I come out of the, I come out of my
building, go to the parking lot and I see it's raining. Okay. And it's raining, you know, it's
going to take me longer because everyone drives slower in the rain. So I think, well, first of
all, it's already, I've already spent five minutes just getting down from my office into the parking
lot. And, and I also think it's going to take me longer. I think it's going to take me 35 minutes
from now for a total of 40 minutes. Okay. So I, what, what I want you to see, the first thing you
want to see is that my guess about how long it's going to take me and my guess about the total
time to go home, it's constantly changing. As I get more information, I, I revised my estimates.
Okay. So to carry the example through, I get, I get, I, I start getting my car, I drive on the
highway. Turns out I didn't take, didn't take so long as I thought. I've, I've, I've spent 20 minutes
total now. And I think it'll only take me 15 more to go home. It wasn't so bad within the rain.
And so that's 35 minutes total. This, this car, this column is my total estimate as it goes up and
down. And then I get stuck behind a truck on a secondary road. And so I think it's going to
take me longer. And then I reached my home street and I think it'll take me 43 minutes. And it does
take me 43 minutes. Okay. So that's a possible thing that might happen. A possible trajectory.
And what I want you to ask is what you might learn from that. Okay. So if you're doing a Monte
Carlo methods, you just say, well, it took me 43 minutes to get home. That's the answer. So all my
estimates, my first initial estimate of 30 minutes, that's going to be moved towards 43 minutes.
That's the error. And in fact, all of these will be moved up towards 43 minutes because whatever
guess I made at each point in time, it should be moved towards what actually happened or whatever,
whatever is remaining in the future at that point. Okay. Now, if you're using a TD method,
if you're using your, your guests can learn a guess from a guest, then something very different
happens. So even some of the signs change. So your first prediction will move up because you start
out at 30 and then after you found out it's raining, so you'll move up. But this one, for example,
will move down. And the actual, all the errors are different. All the errors are different.
And the long, long, long rounded law will wash out. But for all actual learning is a law,
is very different. Okay. Now, I also want you to think about the computational consequences.
Okay. If you're doing TD, then, you know, when you're here and you go to the next stage,
you get an error and you can update right away. You can say, well, why did I make that
prediction? What are my features there? How should I change those? What are the contents
of my deep network that led me to make that prediction? I need to change those. And that's
true at each step. And you, so when you go from here to here to here, you can update this guy,
and then you can forget about it. But you're never going to update him again. Whereas in Monte
Carlo, you have to remember why you made each one of these predictions until you get to the end,
then you have to go back and say, well, okay, why did I make that one? And then adjust its
weights, know it with knowledge of its feature vector and the contents of your network,
and so on. Yeah. And it's terrible. It's distribution because you keep, all this time,
you're doing nothing. You're driving home, but you can't do any learning. Okay. You can only
wait till the end. You know the answer, and then you can go back to all the earlier things and
learn them. So the distribution of computation is poor. The memory is poor. It's just kind of
inconvenient. It's much more convenient if you do it as you go along. And you think about it.
You're in your car. You're trying to drive home. You get stuck behind a truck. Do you say,
you say, you say, this is bad. You know, I say, it's going to take me longer than I thought. I was
too optimistic before. You don't say, well, you know, maybe this truck will disappear. And
you don't say, hold the whole judgment. You could hold judgment until you get home.
But, you know, my feeling is I'm learning as I go along and I'm responding to what I see.
And we actually do learn as we go along. Okay. So I think I've said these things in TD with
Monte Carlo. You can be fully incremental learning as you go along. You can learn before you know
the final outcome. This means you need less memory and less peak computation. You don't have to do
it all at the end. You can even learn if you don't, if you never find out how long it takes you
to actually go home. You know, maybe you're, get a phone call and you're, you're called away for
something important and you never find out. But you can learn without knowing the final,
the final outcome. Now, when you do the math, both of these methods will converge. And
but so the only question is, which is faster? Okay. This is the only question, but it's a big
question. Okay. So I don't know. Let's just do a simple experiment and find, find out. Okay. So
here's a trivial experiment, famous, famous, I don't know. I did this a long time ago. Just a random
walk and meant to test the idea of which one of these is better. So we're going to, we're going
to have five states and we're just going to have an estimate for each state of what the outcome will
be. Okay. This random walk, it takes 50 steps right and left and you start in the middle and you
go back and forth, back and forth, back and forth until you end at one side. Okay. If you end at
this side, you get a zero and you do get zeros all along the way for your reward. But if you know
what you get a non-zero is if you end on the right side, you get a, you get a reward of one. Okay. So
are you with me? What's the correct prediction? So the correct prediction, there's, there's no
discounting here. So we're just trying to predict the sum of the rewards up until the end. What's
the correct prediction for the start state C? You're in C. What's the correct prediction for the
expected value of your return? Gamma squared. Gamma squared. Gamma is one. So the expected,
expected return. So if you, if you end, if you go blah, blah, blah, blah, blah, and you end on this
side, the return is one. If you end on the other side, the return has to be zero. Now you start in
the middle. What do we expect the return to be? You know, by symmetry, it's going to be like 0.5.
Okay. And state B, I don't know, it's going to be less than 0.5.
And state A, still less. Anyone want to guess what they are? The true values of all the states?
C is definitely a half. What do you think B is?
Guess, just guess. A third. Yeah, I thought it was a third. And the next one is a sixth.
Yeah, these just go by sixths. 1, 6, 2, 6, 3, 6, 4, 6, 5, 6. And those are plotted here. This
line is supposed to be the true values. So a state A truly has a true value. It's 1, 6.
And state B is a true value that's 1 third. This has a true value of 1 half.
And so forth. And these other lines are the estimated values from applying TD to it.
So TD, you do have to care about the initial conditions because it's making a guess from
a guess, right? So your guesses, you know, affect things. They either pollute things or
or brilliantly provide good guesses and value, okay? So the initial guess is 0. So at time at
episode 0, all of the estimated values, excuse me, all the estimated values are half. Because
since there could be 0s and 1s for possible returns, it seemed reasonable to start the
estimated values all at a half. What then happens to be right for the middle state,
but it's quite a bit wrong for the other states. And then we're going to do episodes.
And we're going to learn on every time step. And we're going to update the states according
to the TD rule. And after one episode, we have this darker line. This is the episode number.
After one episode, we have these values for our estimated values of the five states, right?
So what do you know happened on the first episode?
You ended on this side because, well, what's going to happen? What is this TD rule going to do?
What is the TD error going to do? Let's say that we start in the middle and we go either way,
we move around. Well, what's the TD error going to be? It's going to be the reward. The reward
beginning is going to be 0. And then it's gamma's 1. So we forget about gamma. And then we just
basically the change in the value. And if you went from, say, this state to this state, what's
the change in value? Zero because the estimated values are all a half. And so we went from one
state from an estimate of a half to another state with an estimate of a half. So as we go all this
bouncing all around, nothing is going to happen, really, until we run into one of the ends.
Well, we can run into this end or we can run into that end. If we run into
this end, we'll go from a state that was a half to, oh, the terminal state. The terminal state
always, by definition, has a value of zero. So over here, if you did this transition,
you get a reward of one. The starting state here would be a half. And you get the thing that has
zero. So it'll be one minus a half. It'll be positive a half. Anyway, the estimate of state
E would go up. And that didn't happen because here's the estimate of state E. It's still at a half.
Instead, what happened is we ended on this side and we went from here that had an estimated value
of a half to this thing which has an estimated value of zero, the terminal state. And so we went from
a half to zero and our RTD error is minus a half. So we moved down from a half towards zero. And you
can actually see how far we moved. We actually moved from a half to 0.45. And so our step size,
alpha, was one tenth. We can understand this algorithm. It's very simple. And then as you get
more episodes, you get closer and closer to the true value after 10 episodes. You get the blue line
after 100 episodes. You get close to the true values. You never get exactly to the true values
because there's always randomness in the individual episode. And alpha is non-zero. It's a tenth. And
so you keep bumping around, bubbling around the true values. So that's an example. Now let's compare
now Monte Carlo versus TD on this problem. And we have to draw whole learning curves now. And
we have to worry about what's the value of the step size. So what I'm showing you in this is a
learning curve, meaning the x-axis is time or episode number. And the y-axis is some measure
of error. It's actually the root mean square error averaged over the five states. And many
iterations of the whole experiment. I guess 100 iterations of the whole experiment. So
as I said, they're going down. Everything's getting better over time. But things will not go to
zero because we have the step size, one tenth, or whatever the step size is. It's always going to be
there. And we're always going to have some residual error. I don't know which one should we look at
first. Maybe Monte Carlo. It's simpler. Monte Carlo, you just wait until you know what the final
return is, and then you do your update for all the states that were visited.
So if you take a nice slow learning rate, 100th, we just gradually move down towards zero error.
And it's actually this alpha equals 100th. They're very slow. We'll actually get the closest in the
long, long, long run to zero error. But it's very slow. So you might want to go faster. If we take
alpha as 150th, we go down faster. But we're not, we're going to start to bubble. And we try going
as fast as .04. And we do go, the initial part is fastest. But now we're definitely bubbling.
And you can't really do better. There's no step size which will do better than the ones
that are shown here. If you try to go faster, you're going to, you know, you may be a little bit
fast at the very beginning, but you're going to level out at a higher level. Okay? And for TD,
we see a similar pattern, but all the numbers are lower. Okay? So here's our, one of the,
the slowest one I'm showing here is .05. And it goes, it's slowest, but it gets lower. And then
other ones are faster. And of course, they, they bubble more. And they, they don't get as low in the
long run. Okay? Now, if we have the long, someone may ask you, you may, some of you may be wondering,
what's going on with that TD stuff? Because it seems like they go down and they start to come up
again. Is anybody wondering that? Anybody wondering that? Yeah, it's, it's weird, isn't it? And
it's real. It's not a bug in my program. That's the first thing to be sure of. Yeah, it's, it has to
do with the fact that actually starting out the estimates at a half is, is not, is not stupid.
It's actually a, a, a reasonable guess. If you started all the estimates out at
something really bad, then you wouldn't see that bounce. Like all the bounces, we go down,
then we seem to bounce, we come up a little bit higher. And that bounces really interesting. It
has to do with the fact that we do have some, some effect of the initial estimates in TD.
And whereas we don't really, at least not as much for Monte Carlo. Okay. So, so this is just a
random walk. And I've sort of, I've been systematic about the random walk. And I don't know, the big
picture is that TD is faster. Okay. There's a bounce, okay, whatever. But it's still much faster.
But this is just one problem. This is just a random walk. Okay. Maybe there's something special
about the random walk. Or maybe if I did that on, you know, Atari games,
I would get a more fundamental result. Now I like to do simple things. Question?
Oh no, they, they, they all converge. Even with a, with a non, well, with a non zero,
if you don't reduce the step size, then you don't expect anything to converge, right? They would
converge in the mean. Okay. And all of them will converge to a mean that depends on the step size
and higher step sizes would be higher and lower step size would be lower convergence point.
Yeah. The convergence properties are roughly the same in both cases.
Um, I want, I want to, I want to ask now, can I say anything about the general case,
not for the random walk? Can I say general case? Actually no. No, I'm going to do,
I'm going to do that in a minute. But first I'm going to do the random walk again under this,
this setting kind of called batch updating. Okay. Batch updating means we, we take some
training set like 100 episodes or 10 episodes or whatever, and we present it over and over again
until it does converge. So even for a finite step size, we will get complete convergence if we
repeatedly present the same training set. Okay. Because there's no randomness in random samples,
you're just putting the same data over and over again, and you will converge the two methods
TD and Monte Carlo converge to two different things. And um, this is for constant alpha.
These diagrams converge to different things. As long as your step size is small enough,
it won't depend on, on your step size. Yeah. All step sizes, as long as they're small enough
so that you don't diverge, will converge to the same thing. Okay. And they converge to
different things. The two algorithms converge to different things. So we can ask um, on this problem
which one converges to a better thing if we present the data over and over again to the algorithm.
Okay. And here's the results on the random walk again. We have, we have different numbers of
different sizes are our training sets. We're increasing that along here. But for each case,
say, say with 50, a training set of 50 episodes, we present those 50 over and over and over again
until we converge and we measure the, the asymptotic error that is independent now of the step size.
So I, I can now eliminate this, the effect of the step size and just
get a measure of, you know, which algorithm is giving me a better result on this problem.
Okay. And TD is faster in, by this measure. I mean, we're doing a lot more computation.
We have to go to convergence. We have to repeatedly present things, none of which I like,
but uh, is getting us uh, insight into what the real difference between the two algorithms.
So it's like TD is moving towards a better place even on a single
example as suggested by the initial results. And if you, if you go over and over again,
you can get that. Okay. Now, this again is all random walk and you have to ask if this is,
happens on all problems. So one approach would be to do all problems and that's obviously not
satisfactory. Uh, so, so what can you do instead? You can try to prove a theorem. Okay. And, and,
you can also try to get insight. I guess I'm going to try to get insight first
and then we'll do the, the formal result. So let's try to get insight into us as people.
I want you to, you, you to be the predictor. Imagine you were having some experience. So
I imagine you were experiencing a training set of these eight, eight episodes. These are all very
short episodes, right? So most of them are episodes like B zero means I'm in state B and then I get
a reward of zero and the end of the, that's the end of the episode. Okay. Or I see state B,
I get a reward of one and that's the end of the episode. The only non-trivial episode is this first
one where I'm in state A, I get a zero and then I go to state B and for B I get a reward of zero
and that's the end of the episode. Okay. So that's the data you see. Just these eight episodes
and I want you to tell me what, what prediction would you make? Okay. The first question is what
prediction would you make for state B? If you found yourself in state B, what would you guess
for the expected return ahead of you from state B? Say again?
Three quarters. I agree because what, what, how do we do that? We said, well, I was in state B all
eight times and six of them ended up with a one and two of them ended up with a zero. So you're
going to guess three quarters. Okay. Okay. That was an easy one. What about state A? State A,
it's really much more uncertain. We've only been in state A once.
And what are you going to ask for state A? Just take a moment and think about it.
What are you going to guess the return if you find yourself again in state A?
What is the estimated value? What would you estimate the value of state A as? Okay. Now,
I'm going to say right away that this is a question there's multiple good answers to. Okay. So I'd
let's like someone to raise their hand and give me one, give me one answer and why it's a good
answer. Okay. How about you? You always, if you always go from A to B and the only time you've
seen A, we went from A to B and B has value 75 percent, three quarters, then A should also.
That sort of makes sense. What's another good answer?
Yeah. I don't know if everyone heard that. You said you've seen A once. Every time you saw it,
the return was zero. So, you know, why not predict zero? Okay. Now, those two answers,
those two answers are the two, the Monte Carlo's answers and TD's answers.
Okay. So, we could say zero. That's what Monte Carlo would say. Monte Carlo just looks at what
happened. I was in A once and the outcome was zero. So, I should predict zero. Monte Carlo.
Now, the other one, the other one is what TD predicts. It's also what you would predict and
this is the gentleman explained what was going on in his head. He was saying, well, I'd seen
A go to B and all that sort of stuff. He was building in his head this model. He was saying,
I'd seen, the only time I'd seen A, it went to state B. By the way, the reward was zero on that
transition. So, let's guess that happens every time. And then in B, B I saw A times and six out
of the eight went one way to a one and then stopped and two out of the, I saw it eight times. Six
out of the eight went to a one and two out of the eight went to a zero. So, I'm building this in my
head. Okay. This is like, and this has a name. This is called the maximum likelihood model of the
MDP. It just means what you'd get by counting. Say, how often do you go from here, turn those
into probabilities. Okay. This is the maximum likelihood model of the underlying Markov process.
And then if you take this model and you solve it, if this is the true world, then the true value
is three quarters. Okay. And so, this is the general phenomenon of what TD does
if you present a training set over and over again to it. It gets the answer that you would get if
you collected all the data, made a maximum likelihood model of the world, and then solved that model
with dynamic programming or with any method. The true solution if that model was the reality.
Okay. And so, that's, well, that's, that's why TD is, is, can be faster, can be better,
because it's using this Markov property that saying, you know, and I've gotten to be, I know,
B is a Markov state. And whereas, my director says, I don't care about what happened in between,
I ended up with a, getting a zero. Okay. So, to summarize that,
the prediction that best matches the training data is the Monte Carlo estimate.
Best matches the training data. Remember the train data? Okay. And if you saw, you saw A once,
and it ended up with a zero. So, you want to match the training data, the right prediction
is the value of A is zero. That is the prediction that will best match the data. Okay. Now, of
course, I want to tell you, we don't want to match the data. We don't want to, we don't want to
minimize the mean square error on the training set. Weird, huh? It seems like we should want to
minimize the mean square error on the training set. And that's why I've gone through at some
length this example with you guys. So, I want you to have some intuition of why we don't want to
minimize the mean square error on the training set. So, what can I offer you if you, if I can't
offer you minimize the mean square error on the training set, it's going to be minimizing the
mean square error on future experience. Because we don't really care about the training set,
past experience. We care about the future. And so, we think, if we believe we have real states here,
we would think that the estimate, the value of A is three quarters will actually be a better match
to future data. Okay. If we get a new experience with state A, it's probably going to be end in a
one, three quarters chance of being in a one. Okay. So, it's interesting. Now, we have to really
distinguish between minimizing error on the training set, minimizing error on the future.
These are different things. And TD can be faster because it can take advantage of the state
property and match future experience better. Now, even as I said that, you may be able to get
immediately a sense of possible limitation of TD methods. As I said, they're going to take
advantage of the state property that I know when I get to B. It doesn't matter how I got to B.
But in real life, you don't normally have complete state knowledge. You have incomplete
state knowledge. If any time you're using function approximation, here we're just using discrete
states. Any time you're using function approximation, you're going to have imperfect knowledge,
imperfect state information. And so, in the end, it's going to be a mix. It's going to be a question
which is going to win in practice. But in the end, it's going to be TD that wins in practice. I'm
thinking. Okay? In the end, Ed. Okay. Okay. So, yeah. Good. Good time for questions.
So, usually, when you introduce the problem, originally, in the beginning of your talk,
you used V to predict the return, right? Yes.
But in this example, V is predicting the reward of each state.
No. V is still predicting the sum of the rewards from the state to the end.
And so, you remember the example was A is followed by zero, is followed by B is followed by another
zero, is followed by termination. So, the right, we're still trying to predict the cumulative reward
until the end. Okay? Thank you for clarifying that. Question? Yes. Thank you for this possible
disadvantage that you are so fitting the micro model. Yeah.
I said it can be a drawback. I'm coming back to the initial comparison of this initial idea
that to be scalable, to really have this computing to be non-lockout. Yes? Yes.
You said that. I mean, I was thinking that, I mean, if you have an explicit representation
of the states, then the memory overhead of MC is linear. Because you have the state there,
you can put just a number of things. And remember who they are. It's linear. And of course,
the computation is not localized. That's okay. But then if you have an implicit representation,
if you point to what you said about function, having a function approximation, then of course,
in this case, TD is not going to be subliminal. Yes, depending on the size of the representation
of the function. But then, then they obey this function, this compact representation can be
expensive. So this is something there of the space and the locality. It's hard to have both of them.
So let's think that through. So let's assume that instead of having a table look-up, this is
all table look-up. But instead of that, we have a complicated neural network. Right? And so then
when we get a new error, we have to back propagate through the network. We have to do somewhat
expensive computation. But it's not, should we consider that expensive? I'm going to say no.
Because even the back propagation of one error, back propagation through the network,
that complexity is the same, it's the same order as a forward pass to the network.
So we already spent the, we had to make the forward pass in order to get the prediction.
And so there's an equivalent complexity to do the update. So even though it's, it's a bunch of
weights, it's, it's, we should consider that cheap. Okay? Okay. It's, it's linear in the size of the
network. Good. Good. Any other questions? Good. Okay, so,
so I've just done one step methods, tabular methods, model free methods. All these qualifiers can be
generalized. But even here in the simplest case, one step method meeting, we're looking from one
step to the next step rather than one step to five steps ahead like in AC3. But there, we can see the
basic ideas. And it's tabular, tabular is easy to think about, but it all, all the ideas really do
generalize to the, to the network case. Complicated function approximator. We've seen the basic things
is that we're going to bootstrap and sample, combine aspect of dynamic programming like to
carry a little Carlo. The TD methods are computationally congenial, just a little bit of work on each
step. And you don't have to wait until the end and then do a whole bunch of work. And if the world
is truly Markov, then TD methods are faster. That's what we see. And it has to do with the
past data versus the future data. Now, before I go into a somewhat new thing, I like to also try
to summarize where we are in terms of pictorially. Okay. What today, we've talked about contrasting
TD method, a one step TD method, which is like, this is what I, this is a little picture is what I
use to summarize the method. That means the thing at the top is updating it. And this says, I go
ahead one action and one next state. And I use my estimate here to improve this guy. This is like
a picture of the algorithm. And the same kind of picture for Monte Carlo is you want to estimate,
improve the estimate of this guy's value. You go ahead one state on action state, action state,
all the way to the end. And you see the final outcome. And then you back all that up. Okay. So,
and this is like a dimension. You can occupy intermediate methods. You can do two, two step
methods, three step methods, four step methods, five step methods. This is like an infinite step
method where you go all the way to the end of the episode. And then there's the parameter lambda
you might have heard about in TD lambda, the eligibility trace parameter. It's really a boot
strapping parameter. It determines, it's not the number of steps, but it's analogous to the number
of steps. And so, this is really a dimension and we can occupy any point along this dimension.
Okay. And that's, now there's a second dimension, which is are we going to use planning? Okay. Are
we going to use knowledge of the model of the world? Okay. Dynamic programming. Dynamic programming
is this corner. It means we're still going to do one step method. We're only, in dynamic point,
you only have one step. And use your estimates at one step and look ahead into the future.
Okay. And so, that's moving along the top here. It says keep, keep these short
backups, one step backups, but instead of doing a sample, do all the possibilities.
And that's dynamic programming. And then there's a fourth corner where the analogous of Monte Carlo,
but with planning, is like exhaustive search. We consider all the possibilities all the way
to the end. And so, we can get these four corners as plastic methods and then we can occupy the
area in between them. And that's kind of a big space of reinforcement learning methods,
although it's, it's certainly not the whole space. Okay. Now,
where should we go next? I don't have time to present all that I wanted to say,
but let me just sort of, we've done a good group here. I can almost sort of, sort of wrap up
and talk about the future from here. But let me just tell you some of the things that we,
if we had more time, we might talk about. Okay. First, we would talk about estimating instead
of state values. We talked about estimating action values because as you know, really for control,
you want to do the action values. And it's not that different. You would just estimate q pi instead
of v pi. And it's going to be lowercase because that's, this is the true value function q pi.
And you would estimate it with, with an action value estimate, since it's just tabular, I can
say a big q of s a for the actual state and action counter. And then this update, this update is
essentially the, the TD update. It's just done on state action pairs rather than on states, right?
This is still a TD error. This is my written estimate. This is the estimate for the next state.
This is the SARSA algorithm. That's, that's quite straightforward. And so the SARSA algorithm ends
up being that rule, that update done over and over again. And, and some examples. Q learning,
Q learning is, is, is almost the same. The rule is, you know, we're doing updating a state,
a state action value. The new one is the old one plus a step size times a TD error. But the TD
error is a little bit different. The TD error, we, you know, we're comparing our original estimate to
next reward plus something of the next state. But it's the max of our possible actions at the next
state. That's Q learning. I like to draw this picture. This is its picture. The picture says,
you know, I'm updating a state action, all the possible estimated action values take the max of
them. That's this, this part of the rule, right? Max or all possible a. And, and then I back up the
max to improve the estimate of this guy at the top. That's Q learning. It's a TD algorithm with
that particular target. And this is a nice example. CliffWalk is a nice example comparing
SARSA and Q learning. SARSA is an on policy method. Q learning is an off policy method. And
we see that actually here, the, the, the y-axis or the, yeah, the y-axis is reward per episode.
SARSA will actually work out better. I don't have time to explain this example.
I wouldn't, I didn't want to do something yet. You guys didn't really understand. Better to
skip over it. Okay. Here's one sort of new algorithm. Expect this SARSA.
Expected SARSA. If we look at the picture, right, in Q learning, you take all these possible things
you might do and you take the best of them. Take the max. The arc means max. And if you have,
if you don't have an arc, then it means expectation. Okay. So, expect SARSA. You don't take the best
of the things you might do. You take the, the expectation based on how you would actually
do them according to your policy. So here we are. We're summing over all the things we might do.
How likely are we to do it under our policy? Which we know. We know our policy.
We know how likely we are. Like maybe it's epsilon greedy. And we take the expectation.
The action value times our likelihood of, of, of doing that action. And we back up that. Okay. And
that's, that's arguably an improved version of SARSA. And it can also be made an off policy version
of SARSA. And there are some other novelties. You can, you can do an off policy version of
Expected SARSA. And I've used the word off policy a couple of times without explaining it.
I'm sorry about that. But off policy means that you're learning about a policy that's different
than the one you're following. Okay. And on policy means you're learning about the same policy as
the one you're following. The same one that's generating the data. So the way to remember it
is that on policy is almost one policy. And in on policy methods, there is only one policy.
It's a policy you're doing. It's a policy you're learning about. But very often these want to
be different. Like you want to do something that is more exploratory. And you might want to learn
the optimal policy. Okay. So if you're going to learn the optimal policy, but you're going to
actually get your data in an exploratory way, which is not going to be optimal. Then you have two
policies. Okay. And then you're in the realm of off policy learning. Q learning does this,
but off policy learning is, is theoretically more difficult and more challenging for our methods.
Okay. That's, that's off policy. Okay. So, so I basically just extended these things to control.
Okay. Now, we've seen some, some methods that can do the on policy case and the off policy case.
We didn't talk about double Q learning. Okay. So I've talked a lot about, about do we want to
use Monte Carlo or do we want to use TD? Okay. And it's, there's a sense in which we don't have to
choose because if you use an algorithm like, if you use TD, you can parametrically vary lambda or
vary the, the, the height of your backups to get, to give you any intermediate point between
one step TD and Monte Carlo. You can get both. And a key, a key way of doing this is with the
parameter lambda, the bootstrapping parameter, which I didn't really talk about, but it is
a way to vary parametrically between TD and Monte Carlo. So if land equals zero,
which is the left side of all these graphs, that's pure TD, pure bootstrapping. Okay.
And if land is one, that means you're not doing any bootstrapping. That's Monte Carlo.
Okay. Okay. So now all these graphs have lambda across the bottom. So it's basically like this,
to pure, to, to no bootstrapping. And they all have a measure of performance on the top,
where in all cases, lower is better. Okay. So it's like, it's like a mountain car and you want to
have a few steps to get to the top of the hill. Okay. So, and what you see looking at this is that,
you know, performance depends on lambda. This is the, this is the random walk. And it's actually
not best at, at land equals zero. Pure TD is not the best. You can do better if you do some
amount of TD, intermediate between pure TD and Monte Carlo.
But if you go all the way to land equals one, then things get really bad. That's like the worst
case in general. And that's the pattern. Land equals zero is Monte Carlo. And Monte Carlo has
really high variance and it has to be ruled out.
It's not very happy if you are committed to Monte Carlo. You can do TD and say, oh,
I can pick any step in between. That's what you, you want to have this facility of doing some
bootstrapping. And that's sort of some evidence for that, even though this is old data, I think,
you know, like Peter would agree that, that Monte Carlo is, is, is not really an efficient strategy
to do it in a pure way. Okay. And now another, I want to give one slide
also for taking questions on, on the linear case, a case with a real function approximation.
I'm not going to go to nonlinear networks, but I want to go to something which we talk so much
theoretically about, which is the linear case. So suppose we're doing linear function approximation.
Linear function expression means that our estimated, estimated value is formed as an
inner product between a parameter, a weight vector and a feature vector. Okay. So feature vectors are
phi, phi for feature, phi of t is, is, is our feature vector for the state at time t.
And the parameter vector is theta. And that might think about as the weights of your,
of your network of, this is a linear network. Okay. So we take the inner product, and so
this transpose thing means the inner product. So theta inner product with phi is our estimated value
of state t. It's the, it's the estimated value of the state at time t because this is the feature
vector for the state at time t. Okay. So this is our estimate, this is the estimated value
of time t. This is our estimated value at time t plus 1. So this really is a td error.
And this is a td rule. The td rule is that the parameters are the old parameters plus step size
times our td error. And that's the, the gradient in, in the general, in general nonlinear cases
would be a gradient of, of the, of the prediction with respect to the parameters. And the linear
case is just the feature vector phi, phi, excuse me. Okay. So that rule should be fairly familiar
to you now. It's just a td rule using a stochastic gradient descent. It's, lots can be said about
that. But that's, that's the standard t linear td t0. And if you look at this, you can of course
write it like this. You can take the phi, the phi and carry it inside. Here it's there and you
carry it inside here with a little some transposy stuff. You can write it like that. And this is a
vector. If you take the expectation, we're going to take the expectation. Okay. So in expectation,
the, the new feature vector is the old one plus a step size. And this, this thing, this thing,
in expectation, what is it? Okay. Well, I'm just going to make some names for it. This thing is a
vector. And this thing is a matrix times theta. Okay. So b is what I'm going to call that vector.
So that vector b is just the expected value of, of this thing. It's, it is a well-defined vector.
You don't know it, but it's, it's, it's there. And this thing is a matrix because it's an outer
product of the feature vector with a change in the feature vectors. So the expectation of that
matrix is what I'm going to call a. So that means let's me write a whole expectation like this.
And I'm interested in the, in what happens at the fixed point. Where will this settle
at an expectation? Where will it converge? Where will the expected update be zero? Well, the expected
update is basically this part. So I want to know when is that zero? Well, that's going to be zero
when b equals a theta. Or when b minus a theta is zero. Okay. And that, that theta, that's a special
theta for which this is true. So b minus a theta equals zero. I'm going to call that theta td because
it's the fixed point that td converges to, the linear td converges to. Okay. And then you can
you can just compute it. So b is, is, is a times theta. And so you have to take the inverse of a
and you get the td fixed point is the a inverse b. And which, which by the way is a, is a, is a,
is a key to another algorithm. Lee's Square's algorithm says estimate a directly, even though
it's a matrix. Take its inverse and also estimate b directly and then multiply them together to get,
that's how Lee's Square's td works. Okay. But, but this way of computing what, what, what the
algorithm converges to. And then you can say something theoretical about it. This is your,
our guarantee that we get that the mean square value error measure how off the, the values are
is bounded by an expansion times the, the mean square value error of the best theta. So this means
that we don't find the best theta. Okay. But we do get an, an expansion of it.
Anyways, that's what the theory would look like if we had more time to talk about it.
I just wanted to mention quickly some, some of the frontiers, some things that people are working
on now. So off policy prediction is a big area people are working on trying to generalize to,
to the off policy case. Also we'd like to talk about non, have some theory for the case of
nonlinear function approximation. There's just a little bit of that. There's also very little
convergence theory for control methods period. And I think Chava maybe we'll talk about that
tomorrow. And we also like to say things beyond convergence. We'd like to like to, you know,
how well can you do in a finite amount of time? How fast do you converge? Now when you combine
TDE with, with deep learning, a lot of different issues come up. And I think there's just a lot
of uncertainty. Do we really need a replay buffer? So that one of the, the folk theorems that, oh,
especially you have instability and correlation, quote, correlation. So we need this thing called
the replay buffer. But I think it's really, there's lots of questions about what happens
when we combine TDE with deep learning. And finally, the idea of predicting things other
than reward. Remember I started with that. We might want, this TDE is a general prediction method,
multi-step prediction methods. We want to use it to predict other things. And in particular,
we want to learn it to use, learn a model of the world. So in conclusion, I guess what I want to
say is something like this. The TDE learning is a uniquely important kind of learning. Anyway,
maybe it's ubiquitous. We're always going to be using it. And I think this may be true. It's a
hypothesis. So anyway, it's learning to predict. We're just perhaps the only scalable kind of
learning. It's a kind of learning that's specialized for general multi-step prediction,
which may be the key to perception, modeling the world, the meaning of our knowledge.
It's key ideas to take advantage of the state property, which can make it fast and efficient,
but can also make it asymptotically biased. And it's other key claim to fame is that it's
computationally cheap, congenial, and we're only beginning to use to explore different ways to
use it for things other than reward. Thank you very much.

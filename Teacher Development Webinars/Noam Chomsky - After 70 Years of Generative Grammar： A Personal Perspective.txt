Yes, you can start.
What did you say?
Good to go.
Hello everyone, welcome to our session today. This is a very special session because of our guest but also because TESOL Tampico talks is collaborating with teacher development webinars.
My friend Amunalai is here. My name is Jorge Torres Almasan and we want to welcome you to this unique session.
Amunalai is going to introduce himself and he's also going to introduce our special guest. I mean he needs no introduction but he's going to go for the protocol.
Amunalai, over to you.
Bismillahirrahmanirrahim, assalamu alaikum. My name is Zaman al-Dawtrand and I'll start with the couplet of Shah Latif, the famous poet of Sindh.
He says,
It translates as those worshipers who search the ocean and find different jewels from the depths. Latif says they are the ones who have found profound ones.
So, it's my singular honor player and privilege to introduce a professor, Noam Chomsky, considered the founder of a modern linguistics.
Noam Chomsky is one of the most cited scholars in modern history, among his groundbreaking books are syndicate structures, language in mind, aspects of the theory of syntax and the minimalist program, each of which has met distinct
contributions to the development of the field. He has received numerous evas including the Kyoto Prize in basic sciences, the Helmholtz medal and the Ben Franklin medal in computer and cognitive science.
Chomsky introduced the Chomsky hierarchy, generative grammar and the concept of a universal grammar which underlies all human speech and is best in the innate structure of the mind, our brain.
Noam Chomsky has not only transformed the field of linguistics, his work has influenced a field such as cognitive science, philosophy, psychology, computer science, mathematics, child education and anthropology.
Professor Chomsky is one of the most influential public intellectuals in the world. He has written more than 100 books, and his website is www.chomsky.com, certainly right said by the New York Times magazine, the most important intellectual alive.
So ladies and gentlemen, Professor Noam Chomsky for you.
Over to you sir.
Sorry for the technical problems.
Times brief there's a lot I would like to cover.
So I'll have to keep to a sketchy rather informal account we can extend things in the question period.
The long term goal of theoretical inquiry is explanation, not just description description is hard enough descriptive inquiries seek to show that this is the way things are explanation seeks to show why things are this way and not some other way.
It's a much more ambitious endeavor, but it is the goal to keep in mind in the case of language inquiry proceeds at two levels. One level is concerned with in languages, the second and higher level is concerned with the general faculty of language.
Just refer to it as FL faculty of language.
FL is the innate endowment that enables a language to be acquired and used. The theory of FL is called universal grammar, you G traditional term it's adapted to a new context.
At the first level the study of individual languages inquiry seeks an adequate grammar of the language, the description of the properties of the language, an adequate grammar must at the very least provide a listing technically a recursive enumeration of the grammatical sentences.
Beyond that, it must yield what has been called the basic property of language. Each language is a digital infinity of hierarchically structured expressions, each of which determine determines perhaps constitutes a thought.
Each of which can optionally be externalized in some sensory motor medium.
The whole thesis would be the generation of thought by the internal language is entirely separated from externalization, which would be an ancillary process. I'll actually assume this return to pretty strong reasons to suppose that it's correct.
In conception, the internal language consists of the compositional rules that satisfy the basic property, along with what's called formal semantics more accurately term logical syntax important matter that all put aside here.
The system for generating thought, we can call I language, it's internal individual in the technical sense intentional within us meaning we're interested in the actual system coated in the brain, not just one that yields the same output.
Well, understood in this way, the study of language conforms to a long tradition from classical Greece, classical India, up through the 19th century, throughout the core of the tradition, regarded language as fundamentally a system of thought as audible
in the phrase of the 19th century, linguist William Dwight Whitney, we know that the restriction to sound is too narrow. This tradition was swept aside in the 20th century by behaviorist and structuralist currents, and it was forgotten.
It's still very little known modern approaches in the 20th century, take language to be at its root, a system of communication, rather than a system of thought system that perhaps evolved from animal communication.
Recent work seems to me to indicate quite clearly that the tradition was on the right track and should be carried forward. That's a controversial view today, but I think it's justified, and I'll adopt it here.
Now returning to explanation, the task arises only at the higher level, inquiry into FL, the Faculty of Language, seems that any human infant can acquire any language with comparable facility.
If so, FL is a shared human property. There's also by now substantial evidence that FL is unique to humans, that its fundamental properties have no analog in the animal world.
If so, FL is a true species property shared by humans, unique to human essentials. FL must enable any human infant to acquire any language to condition on UG that has been called explanatory adequacy since the 1950s.
Sometimes it's called Plato's problem. It was raised in a certain form by Plato, as it was formulated by Bertrand Russell. It's the problem of how we can know so much, given so little evidence in technical terms of linguistics, it's the problem of poverty of stimulus.
In the early days of the generative enterprise in mid 20th century, it was recognized that the poverty of stimulus problem was quite serious, and that standard formulas within behaviorist frameworks will get nowhere.
There are contemporary variants with supercomputers and astronomical amounts of data, but they do no better. These early conclusions were established more firmly as serious experimental inquiry was undertaken in later years into language acquisition.
It was found that the problem of poverty of stimulus is far more severe than had been assumed. By now it's been quite solidly established that the knowledge of language of a two to three year old child goes far beyond what they exhibit in behavior,
and also that the gap between data available and knowledge attained is vast, greatly deepens Plato's problem. These results seem to suggest that the faculty of language must be rich enough to yield complex knowledge from impoverished data,
very rich therefore, while at the same time, FL must somehow accommodate the wide variety of languages that poses a problem. That problem deepens and we turn to another condition that FL must satisfy, sometimes called Darwin's problem.
How can FL have evolved under the conditions of human evolution, though evidence is fragmentary. There are some plausible conclusions about these conditions. Modern humans appeared about two to 300,000 years ago.
There's now genomic evidence that the small group of humans began to separate, not long after they appeared, all share FL, which was presumably already in place.
Compared to the appearance of modern humans, there's virtually no indication of significant symbolic activity in the archaeological record. Not long after their appearance, there's very rich evidence, note that these numbers are barely a flick of an eye in evolutionary time.
All of this suggests that FL appeared in a narrow window of time, pretty much along with modern humans. And there's quite strong evidence that it hasn't changed since. Well, if all that's the case, we expect FL to be very simple.
So we therefore face a conundrum. FL must be rich enough to handle Plato's problem. At the same time, simple enough to deal with Darwin's problem and the apparent wide diversity of languages all determined by FL lurks in the background.
These are basic conditions that be satisfied by UG. The conditions appear to be contradictory. And much of the search for explanatory theory in past years has been guided by this apparent contradiction.
For the first time, I think we can now begin to see how the conundrum might be resolved. So let's turn to that.
Let's begin with the problem of diversity. That problem would be greatly reduced if diversity of language is sequestered in a particular component of the overall system. The natural place to look is in externalization,
excluding the language operations that yield the thought system. That move has initial plausibility for I language, the POS, the repository of stimulus problems are overwhelming, generally insurmountable.
For externalization, at least there's some direct evidence what's heard or seen, though this hasn't been firmly established, research is tending towards showing that the variety complexity and easy mutability of language is indeed sequestered in externalization,
leaving the system of generative generation of thought relatively fixed, be completely fixed, we might someday discover that would vindicate an ancient tradition.
If this turns out to be true for language, it wouldn't be very surprising. Externalization, some sensory motor medium, is not strictly speaking language, rather to combination of the internal system for generation of thought,
and sensory motor systems that have nothing to do with language. They were in place long before language appeared, haven't changed since.
So we might expect that mapping the internal language system into completely unrelated sensory motor systems would be a complex task, solvable in many ways, easily subject to at least superficial change.
And so it seems to be. Well, if this is basically correct, then the poverty of stimulus problems reduce largely to externalization.
These topics have been the subject of extensive research since the principles and parameters framework was formulated 40 years ago. That was the first approach that offered at least a potential framework for accounting for
acquisition of language in a feasible manner. It's been very important work, much of it by the late Ken Hale and his students, which has shown that languages that appear to be radically different are actually cast to the same mold at deeper levels.
One high point in this research was Mark Baker's work on showing that on developing a hierarchy of parameters. Another major step forward was later taken by Ian Roberts, who's shown that very simple algorithms based on elementary
cognitive principles can feasibly zero in on choice of parameters tested this approach with thousands of languages of great typological variety. I think we can fairly say that the problem of externalization systems is fairly well in hand, at least in principle, partly
in practice. And if externalization is the primary, maybe the only locus of variation, a large part of the conundrum is on its way to resolution, namely variability and learnability.
Let's proceed beyond. Let's turn to some concrete examples. I'll begin with the most fundamental property of human language, one that is quite surprising in many ways, and has rich consequences that are not always sufficiently appreciated.
There's a very cold structure dependence. Let's briefly review some standard cases. So, begin with the operation of construal, for example, finding out what verb phrase an advert modifies.
In the sentence, the man who fixed the core, carefully packed his tools. It's actually ambiguous. Could mean he fixed the core carefully, or he carefully packed his tools. Now, place the adverb in the front, carefully, the man who fixed the core, packed his tools.
It's ambiguous, can only mean he carefully packed his tools. Actually, that poses a puzzle. The advert in initial position has to find a verb phrase to modify.
But in doing so, we ignore the simplest computation, find the closest verb phrase, which would be fix the core. We reflexively ignore that and choose the more remote verb phrase, packed his tools.
It's clear if you think about the structure of the sentence. The phrase, packed his tools, is actually closer to the advert than fixed his core, fixed the core, in the abstract structure. And that's what we attend to, though we never hear it.
There's actually more involved, but that's enough to bring out the basic puzzle. We ignore the simple computation on linear order of words. We reflexively carry out a computation on abstract structure.
Take a second example. Anaphors terms that don't refer in themselves, but fine to find an earlier antecedent that determines what they refer to. Well, the simplest algorithm is to seek the closest eligible antecedent.
As in sentences like the boys expect the girls to like each other means the girls like each other, not the boys. Sometimes, however, that fails as in sentences like the friends of the boys like each other.
Here it's the friends who like each other, not the boys, the closest antecedent. So once again, we ignore the simplest algorithm, which relies on what we hear words in linear order.
And we use an algorithm that applies to abstract structures, and it's quite complex. First, we have to identify the subject noun phrase, then we identify the core element within the noun phrase, choose that as the antecedent.
That's what we do reflexively.
Final example, take verbal agreement sentence like the bombing of the city cities is a crime, not are a crime. The bombings of the city are a crime, not is a crime.
So again, we reflexively ignore the simplest operation, adjacency, and again carry out an operation that looks quite complex construct the abstract structure, subject predicate, locate the head of that structure.
That's what we do reflexively.
Well, these observations generalize structure dependence holds of all constructions in all languages.
What that means is that from infancy and on through life, we reflexively ignore 100% of what we hear words in linear order. We attend only to what we never hear abstract structures generated by the mind and operations on these structures.
Furthermore, experimental work has shown that the principle is known to children as early as they can be tested two years old or less.
Notice that this curious property is restricted to I language, the generation of thought principle doesn't hold for externalization.
Well, the only plausible conclusion is that I language and externalization are distinct systems for I language, computation ignores what is externalized.
Furthermore, all of this must be part of the innately fixed faculty of language property of Eugene conclusion further supports the thesis that I language which is language proper is a system of generation of thought, as was assumed in the millennia old tradition.
Prior to the behaviorist structuralist revision in the 20th century. Well, it's plain why languages use linear order for externalization. It's required by the speech organs, which cannot produce structures.
In fact, sign language, which is the resources of visual space available does not adhere strictly to linearization.
We call that the sensory motor organs used for externalization are unrelated to language in their nature and evolution. So it's not surprising that I think which pure language ignores the properties that are imposed by these non linguistic organs.
Well, with this in mind, we can turn to the why questions. Why does Eugene for structured abundance. The optimal answer would be that this follows from the simplest forms of Eugene, using the simplest operation for generating expressions.
The simplest operation is unbounded binary set formation. It's the basis for what's called merge in the literature, and in fact, structure dependence does follow directly from merge.
So to put it metaphorically, when mother nature created language, she found the simplest possible solution, which is incidentally, the way evolution works.
Similarly, all of science, so it appears to driving intuition of science, since Galileo is by no so broadly verified that Einstein called it the miracle creed that guides all serious inquiry.
In terms of language, it's called the strong list thesis. Reliance on merge as the soul structure building operation has many other crucial empirical consequences won't be able to go into it in the time available but there's fair amount in the literature.
Reliance on merge as the soul structure building operation yields the core of the basic property, generating an infinite array of hierarchically structured expressions.
We can easily add that the primitive elements of the system or elementary concepts, elaborated by merge these become phrases, we can translate straightforwardly to event semantics.
We can also add the syntactic exponents of participants in events program along these lines. If it can be fully carried out with complete to generate the theory of generation of thought should add that the evolutionary origin of the most elementary concepts that have to serve as primitives is a complete
and will probably remain so there are no analogs in the animal world, no record of their development.
Well, there are two logical possibilities for merge of X and Y, either the two are distinct or one is part of the other technically, it's called a term of the other.
The two options are called external and internal merge. External merge yields combination, as in read books, internal merge yields displacement, as in which books did he read, where which books is understood to be the object of read.
The strong minimalist thesis should not permit an operation of deletion, which would allow for richer systems.
Externalization deletion does take place within narrow conditions, but that does not apply to I language, which we may therefore assume has no deletion operations that has consequences.
Let's take again simple cases like which books did you read the I language representation is which books did you read which books, which is interpreted directly as for which books X, you read the books X.
External merge automatically yields the phenomenon of displacement with reconstruction. It's a topic that's been extensively studied. It falls out directly from the optimal system of generation.
What this structure dependence does what reaches our mind in this case is which books, did you read which books, but which reaches the ear and reaches consciousness is which books did you read externalization removes all but one occurrence, which has to remain to indicate that the operation
that's a general economic issue, radically reducing both mental computation and execution of articulatory motions, actually the result of these simple operations to maximize computational efficiency yield problems for perception and parsing.
What are called filler gap problems. The parser finds a wh phrase like which books, and it has to find a gap that fills it for interpretation turns out to be quite complex, one of the major problems in parsing.
The problem would be radically reduced if the gap were filled, but mother nature didn't care about that when designing language. She insisted on finding the most elegant solution, even though it poses difficulties for language use quite serious ones.
The sentence has become more complex. This is one of many cases in which computational efficiency conflicts with communicative efficiency.
And in all cases, communicative efficiency is sacrificed. It's more evidence for the traditional conception of languages, fundamentally a system of thought.
All of this is consistent with how evolution proceeds quite generally. Schematically, we can distinguish three stages. First, some disruption takes place, maybe a mutation or drift or gene transfer.
Or a bacterium accidentally swallowing another microorganism. That's the breakthrough that led to complex cells. That's why we're not all bacteria.
The second stage is reconstruction. Nature reconstructs the new entity in the simplest way, observing the miracle cream, paying no attention to how the new entity might be used.
The third stage is winnowing. The outcomes that reproduce more effectively prevail. That's natural selection. Well, that suggests a reasonable scenario for language evolution.
First, some minor rewiring of the brain took place, yielded the new property of recursive enumeration. Next step is reconstruction. Nature devises the simplest, most elegant way to organize the new system. Strong minimalist thesis.
The semantic theory becomes available, possibly with more primitive origins. Computational procedures are given part of natural law. The winnowing stage is never reached, possibly because of lack of time.
Possibly because the optimal system is so delicately designed that it's either all or none. So we're approaching an interesting conclusion.
The internal language might be perfect on a common human possession. That's the strong minimalist thesis proposed 25 years ago was not then regarded as realistic, rather as a long term goal that might guide research.
Recent work does suggest something more audacious. Theses might indeed be true as evolutionary theory leases to suspect, along with science generally, and as empirical inquiry increasingly suggests.
Well, there's a lot more to talk about. Many new, many other possibilities, but I don't think there's time to talk about it here.
The crucial point is that the internal language seems to keep to the strong minimalist thesis throughout, with no departure necessary, even for pretty complex cases.
Well, remains to show how far we can extend such reasoning. Needless to say that's an extremely challenging task.
Nothing like it has ever risen in the study of language and thought. There are promising early steps far beyond what could have been imagined just a few years ago.
Final remark, strong minimalist thesis has several functions. One is a disciplinary function sharply restricts the options for description, and thus deepens explanation.
It also has an enabling function. It provides options for what I language might be for what kinds of subsystems might exist. An interesting question just coming into focus is to explore whether I languages make use of all the possibilities that are enabled by the strong minimalist thesis,
which may be more than just a guideline for inquiry, as assumed in the past, but may actually express fundamental truth about the nature of language and thought, the most distinctive possessions of this strange species of ours.
Thank you.
Thank you, Mr Chomsky. Amala, do we have any questions? Did you select some?
So, yeah, I see this question.
What extent does the UG remain available to second language accusations?
If you can see the chat, Mr Chomsky.
I see questions here. First one I see is, what's your take on views of data linguistics that focus on the data instead of UG and internalization, and also criticize the syntactic theory as well.
What's your take on Annie Chomsky in School of Thought? Well, I can't comment on all of them. There's plenty of criticism. You have to pick them up and look at them.
But where's data linguistics is concerned? It's fine. You can collect a lot of data and can look at it, can find surf properties of it, doesn't tell you very much, just as in any other field.
You look at a lot of data, you don't find much. If you want to study the laws of motion in physics, you don't collect data about leaves blowing in the wind.
What you do is careful experimentation, including thought experiments to focus on the crucial principles.
Now, right now, data linguistics is very popular.
There are studies that you've seen many of them in the press, a lot of excitement, GPT3 and others.
They take astronomical amounts of data, maybe 50 terabytes of data, have thousands, maybe a trillion parameters, supercomputers running on them.
And with all of that data, they can find surface properties of the expressions in some vast amount of data.
They string things together like this. It looks more or less like language. It tells you absolutely nothing, zero.
The proof of that is that the same systems work exactly as well for impossible languages.
So for languages, say that violates structured abundance, which are impossible for humans.
It's just fine. It's very much as if I were to go to a physics conference and say I have a terrific new theory.
It accounts for all the particles that have been discovered, even the ones that are possible and haven't been discovered.
It's so simple that I can express it in two words. Anything goes.
Sorry, I don't get the Nobel Prize because the same theory accounts for everything that's not a possible particle.
So it tells you precisely nothing. All of these things that are achieving vast excitement and oppress basically do nothing.
I mean, there are approaches based on the same techniques, deep learning that do do something.
So for example, the Google translate is based on these techniques and it's useful.
There's been some success in handling protein folding by these methods, but these data linguistics approaches are absolutely worthless.
They don't achieve anything useful. They don't and they tell you nothing about language by definition.
Basically, it's a way to use up a lot of the energy in California and to develop public relations for the Silicon Valley, but it's doing nothing else.
Well, the next to what extent does UG remain available to second language acquisition.
And secondly, is it true that movement from the specifier VP the specifier of TP is not motivated by the chest case checking property nominative case.
Well, first, to what extent does UG remain available to second language acquisition.
There's a good deal of research on that by people working on second language.
My own view is that decisive conclusions have not been reached.
There's an it may actually vary among people remember there's a lot of individual variation and variation as to how the second languages are acquired.
A second language that's acquired through immersion.
The best way if possible is going to be acquired very differently than one that's acquired by reading a grammar or going to a class.
And it's possible that UG functions in the first case, much more effectively than it does in the second.
These are basically open research questions, non trivial ones for teachers.
Is it true that movement from the specifier VP, the specifier TP is not motivated by the case checking principle.
Well, again, that's an internal technical question I my own view is basically that it's not.
The reason why the nominative case couldn't be assigned to the specifier of VP. I think there are other reasons for this having to do with the, what's called EPP the
and ECP the empty category principle which are very, which are tied together and they're very their properties of certain languages, their properties of languages that don't automatically delete the freely delete an empty subject.
So Spanish and English are quite different this way.
Spanish, you don't have to pronounce the surface subject inspect TP in English you do English is one of a relatively small number of languages where you have to pronounce it.
Spanish is one of the much larger category of so called no subject languages in which you don't.
Spanish, the movement is not obligatory. It's optional for other reasons for semantic reasons.
But I think when we look at all these things together they really have to do with what's called labeling theory, it's been discussed for the last 10 years or so I don't have time to go into the details.
Well, how does generative grammar differ from descriptive grammar depends what you mean by descriptive grammar. If you mean what was meant by it and structuralist linguistics.
Descriptive grammar. What I studied when I was a grad student under graduate 80 75 years ago was regarded as a taxonomic science. That's what it was called.
There are procedures of analysis which you use that find the units of a language and the way in which they are organized relative to one another. That was descriptive grammar.
If there's anything new it's just by what's called analogy. I don't think any of that can be sustained. A generative grammar does something quite different.
It tries to tell you what all the possible structures are for the entire language. It's quite different from a taxonomic grammar. Furthermore, the elements that enter into it can't be acquired by procedures.
That was shown years ago. Now, from another point of view, the generative grammar is a descriptive grammar, but a very different kind from the descriptive grammars of structural linguistics.
What do you see as the future of linguistics in the futuristic field in Pakistan? Well, controversial subject question, of course, my own feeling is pretty much what I tried to describe.
I think there's a future for linguistics as quite a new field, a field that for the first time is able to provide genuine explanations for the basic phenomenon of language instead of describing them, which is hard enough.
There are a couple of examples. There are many more that can be carried through. It's a major breakthrough for the future. You can't predict the future of science, so we don't know. But that's my personal opinion.
Let's see the future differently. What about the future in Pakistan? Well, simple answer to that depends on Pakistani linguists and other linguists who are interested in studying or do other native languages of Pakistan.
For the future of the linguistic field as a general field, it'll depend on developments within Pakistan, in Pakistani universities and research centers. Many things changed in the last 70 years.
What new parameters must we take into consideration for effective language teaching?
First of all, 70 years ago there were no parameters. The assumption in structural linguistics explicitly stated over and over was that any languages can vary virtually without limits, and each individual language must have to be studied in its own terms with no
assumptions about other languages. That was in the mid-1950s the basic doctrine that was sometimes called the Boazian doctrine, rightly or wrongly. Well, that's very far from true.
I should say that something similar was believed in biology at about the same time. It was assumed that organisms vary virtually without limit. Each one has to be studied on its own.
In biology, that's now known to be completely false. In fact, there are very narrow constraints on possible organisms, so narrow that some have even speculated that there's a universal genome.
The apparent variety of organisms is just superficial working out of some of the possibilities determined by the fixed form for organisms. I think something along those lines has happened with regard to our concept knowledge of language.
About 40 years ago, there was an innovation in linguistic theory suggesting that the basic theory in our minds has fixed unchanging principles, like some of those I discussed, and parametric variation, a set of parameters.
So that would mean that language acquisition is kind of like a question-answering game. The infant asks for each parameter, or is it set this way or that way? Is it set as an old subject language like Spanish, or like a language that requires an explicit subject like English?
Yes or no question can be answered in a very small amount of data. Does the language have verb preceding object, like English and Spanish, or does it have verb following object, like Japanese and many others?
Again, answer a small number of evidence. We now have recent work of the kind I mentioned, like Ian Roberts, which shows how the infant can work through the system of parameters very efficiently to fix on the exact language.
I want to look at what look like the plausible set of parameters. The best work I know is by Ian Roberts and by Giuseppe Lungabardi, who teaches at York University in England, has done extensive work on the variety of possible parameters and what they tell us about the history of linguistics.
That's a new topic that he's innovated, and it tells us quite a lot. You can get much deeper knowledge of the relationships among languages at a much deeper level if you look at shared borders than in the standard way.
So that's a source you can look in.
How does UG theory play in this surviving languages like Romanian, maintaining its Latin roots along with neighbouring Slavic languages? You could say the same about English, about 60% of the vocabularies, romance.
A lot of the grammars, Germanic, say the same about French. French has a lot of Germanic properties, but it's a romance language. So it has, say, a clinic movement like the romance languages, but it's like English and German in requiring an overt subject.
UG is exactly what studied by UG. How are the range of parameters determined for particular languages? They all seem to share the same principles at an underlying level. In fact, the same is true of languages.
This is the Amazon or Papua and Guinea tribes that haven't had other contact for tens of thousands of years. When they're studied carefully, they seem to have the same underlying principles, different parametric choices.
There is a phenomenon of called Sprachbund languages commonly accept some features of neighbouring languages, even if they're related, not closely related, which is what happened with Romanian.
They picked up some of the properties of neighbouring Slavic languages in these geographic areas of which involve interaction among people. In the case of English, it's pretty straightforward.
The Norman conquest in 1066 turned English into a mixture of romance and Germanic with a lot of romance vocabulary. In fact, the majority, but Germanic Germanic literature.
Given the natural, the recent advances in natural language processing and AI, what prospects do you see for SLA?
As I mentioned, the advances in natural language processing and AI, well, they may be useful for some purposes. Just tell us absolutely nothing about language.
If they happen to be useful for some purpose, say Google Translate, Life Translation, Transcription, SLA, that's fine. Use whatever is useful. But if you're interested in the nature of language, the nature of cognition, human cognitive processes, this work just tells you basically nothing for the reasons that I mentioned.
Does the human brain contain a limited set of constraints for organizing language? Undoubtedly it does. We don't know very much about it because we don't know very much about the human brain.
It's a very hard topic to study. The brain altogether is a hard topic to study, even for tiny organisms. So if you take an ant, which has a brain the size of a tiny, a minute brain, you need a microscope to see it.
We have no idea how it carries out highly complex computations that humans can't carry out so the desert ants in my backyard can navigate in a way that a human can't. They use computations that are inaccessible to us.
We have to duplicate it with complicated instruments. Even the ant brain is very hard to understand. Human brain is much harder because we cannot do experiments, the kind of experiments that immediately come to mind.
You can't do them for ethical reasons. So we happen to know a fair amount about the human visual system, but that's from experiments.
It's monkeys, which rightly or wrongly we've allowed ourselves to carry out. And they have about the same visual system they do. But you can't do that for language because there's no other organism.
No other organism has even the rudiments of human language. So there's nobody to study. And you can't study the human brain for ethical reasons. So it's a very difficult topic. Nevertheless, there are some achievements.
One of the most important ones, in fact, has to do with structure dependence, what I mentioned, the property I mentioned. There is research created by Andrea Moro, fine linguists in Italy, Milan research group, which was able to show that
here's the paradigm they used. They took subjects, say, whose native language was German, and they gave them two kinds of invented languages. One invented language was based on an existing language that they didn't know, maybe Italian.
The other database was very simple language, which used principles that you don't have in language, like linear order. So a language in which negation is the third word of a sentence.
It's very simple to work out. Well, it turns out, when the subjects were given an inventive language based on an actual language, the norm, the language areas of the brain, language dedicated areas of the brain function normally.
When they were given an invented language that violated structure dependence, even with trivial algorithms, there was diffuse activity, the brain, the language areas were not activated. They solved it as a puzzle.
That tells you that the brain, it tells you something about what we expect to be true, that the brain is structured in such a way as to be available for language the way it is.
It sets conditions on what language must be. One of them are the conditions that conform to the fundamental principle of structure dependence. There are a few other things like this, which are quite interesting, but it's a hard topic.
What are my thoughts on systematic functional grammar?
I don't think it tells us much about the questions that I've been discussing here. It may be useful as indicated in the question by studying the wider context in which language is used.
So there is a broad context in which language is used, which is not studied by the generative grammar, which investigates sentence grammar. It's a perfectly sensible topic, and maybe functional grammar can provide some help with that.
It does all to the good. So if I say something like, John went to the library, he bought a book. John went to the bookstore, he bought a book.
If you think about what the language tells you, doesn't tell you what the word he refers to. That discourse is perfectly possible if John went to the store and Bill bought a book. Doesn't tell you that the book was bought at the store.
Doesn't tell you, it's perfectly, the sentences are perfectly possible if John went to the bookstore, Bill bought a book somewhere else.
That's perfectly possible. Language doesn't tell you anything about that. There are conditions of normal discourse that make it likely that in John went to the store, he bought a book, makes it likely that he is John, and that he got the book in the store.
That's plausible because of the nature of human functioning systems and maybe functional grammar can tell you something about such things.
I frankly think there's not going to be a lot to say about it. These are quite complex and diverse.
Well, that's the end of the questions that I see here.
Oh, I see a couple of others.
I said Chomsky, if you have the time to answer them, that's okay. Or if you want to stop right now, it's up to you.
Sorry, I couldn't hear that.
Yes, I'm asking you about the questions.
If you want to go ahead or you want to stop here, we know you have some other things to do, and we had only one hour for this webinar.
Should I go on with some further questions?
If you want to.
Is that what you're suggesting? Yeah, okay.
Okay, sure.
How may we better connect UG with contemporary teaching?
It's kind of like asking if you're, suppose you're a swimming coach.
It's a good idea to understand something about physiology and use your knowledge of physiology and improving your teaching of swimming.
If you're teaching language, it's useful to know something about language. That's UG.
A skilled teacher will figure out how to use their knowledge of language.
UG in teaching just as a good swimming coach will use their knowledge of physiology and teaching swimming.
There's no formula for this. That's what good teaching is about.
As a generative grammar is concerned, you have this claim that teaching grammar should be excluded in classroom constructs instruction.
It's been criticized by some linguists long said the teaching should not be excluded, but that of communication should be foregrounded.
Do you think this claim of excluding teaching grammar and classroom instruction is still valid, especially in today's time?
It depends what your goals are.
If you want the student to understand the language that they're acquiring.
It's perfectly useful. It's very useful to teach something about the nature of the language.
If you're teaching just by immersion the best way, so you send somebody to Italy to study to learn Italian, they'll just pick it up by listening to it.
Then that's not teaching. That's immersion.
Then you try to make it as much like a child as you can, but there's no...
There can't be an answer to this. It depends what you're trying to achieve.
If you're trying to achieve communication skills, but you don't care whether the person understands the language, you can foreground communication.
If you care about whether the person understands the language, you'll teach grammar. That's just the nature of the language.
Why did I choose grammar?
Oh, grammar is a funny word.
Grammar, in the technical sense, grammar just means the nature of language.
What I've been talking about is grammar.
What's the nature of their faculty of language and thought?
Because they're intimately connected.
So why did I choose to study language and thought? Good reason, I think.
These are the fundamental properties that distinguish humans from the rest of the organic world.
So if we want to understand what kind of creatures we are, if we want to know thyself as the Delphic Oracle,
advise us the obvious thing to look at is language and thought.
It's been assumed for several millennia. I think it's a wise choice.
I chose it for the same reason.
There are many others that I see here have to do with
finding
those exposing learners to the cell environment for more than three years, make them acquire, develop a native like language.
A child, yes, a child will pick up a second language in almost no time.
Just to give you an example from personal experience.
My family went to, I was teaching in Italy about 40 years ago, so my whole family came along.
My older kids were studying, 20, 25, were studying Italian.
They really wanted to learn it.
We had a 10-year-old who didn't want to learn it. He didn't want to come with us.
He said he was not refused to learn Italian.
Well, after a month, the 10-year-old, if the phone rang, the 10-year-old had to answer it.
Despite himself, he just acquired Italian by being in school where everyone was talking Italian.
Children just absorb it like a sponge.
The older kids were working on it, just my wife and I were going to make it harder for adults.
So the answer depends on age, depends on motivation, depends on all kinds of things.
You can live in a foreign culture for all your life and not know the language.
I know people like that.
Do you find the need for a collective approach to communicative competence?
Do we need to apply all kinds of competence time to test the speech?
Currently, researchers use only four or five or single competence.
Well, I really don't know.
It seems to me, if you want to broaden the approach to communicative competence, whatever that is,
use whatever you find valuable to increase it.
I don't think there are any formulas.
Why do two children of same parents start speaking languages at different ages?
Well, there's slight individual variation, but not very much.
Unless there's some very unusual circumstances, two children in the same family will start speaking at about the same age,
maybe a couple of months different.
UG has nothing to say, but this is a matter of maturation.
The maturation of innate competence, it's like acquiring the ability to walk.
You find children are slightly different.
One child may be walking at 12 months, another might start walking at 14 months,
but it's just a matter of how the organism matures.
Maybe the main core of UG in just three words.
I don't know if I can do it in three words, but it's the theory of the nature of language.
We talked about explanation and description.
What about the metacognitive aspect of learning?
I don't know what can be said about that, not familiar with it.
I think I'll probably have to stop here.
I don't see any simple questions.
Thank you very much.
We really appreciate you being here with us.
Thank you very much on behalf of T-Soul Tampico Talks and teachers' development webinars.
Thank you.
You want to say something before Mr. Chomsky leaves?
Professor Chomsky, it has been a really fulfilling experience having you at T-Soul Tampico Talks
for organizing this wonderful talk.
As a student of linguists and now I am a scholar, I have been reading your work
and seeing you as a father figure and having you is like a dream come true.
I really can't express my happiness and humbleness.
I much appreciate your time and experience in sharing all this.
You celebrated your birthday a few days ago.
Happy belated birthday to you again.
Long live.
Thank you.
Thank you, everyone.
We hope to see you again in our next event, our next webinars.
Yes.
I would like to acknowledge Master English Training for sponsoring a Zoom account.
They are the ones who support us and come to us in time of need.
Thanks to Master English Training and Fatima who deals with all these things.
Yes, thanks very much for your time and support and appreciation for these development webinars.
Thank you to T-Soul Tampico Talks.
Thank you.
Thank you, everyone.
Thank you, Mr. Chomsky.
Bye-bye.
See you next time.
Thank you, everyone.

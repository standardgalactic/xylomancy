What would you say is the moral of the story?
Open Source is most successful when it's played as a positive sum game.
OpenShift started in 2010.
There's this divide in the platform as a service base.
There was all the really simple, modern apps,
and then there was all the stuff that made money.
And kind of we came in from the perspective of like,
how do we make this stuff easier?
And so we were kind of getting tugged to the right
on the complexity scale.
We started with simple apps in OpenShift.
But we were kind of thinking, you know,
what's the more general problem
that would bring the whole spectrum of application authors?
We wanted to use Docker with an OpenShift,
but it's still not enough, right?
Because the Docker container is how you get a reproducible unit
of software, but how do you get those chunks coming together?
How do you take three bits of software and bring them together?
We'd reached out through one of our board members to Google,
you know, this container space is getting interesting.
Are you guys doing anything?
Is there anything that you're interested in
that maybe we could work together on?
Clayton was looking to redesign the next version of OpenShift
what became OpenShift 3 on top of Docker,
which, you know, Docker was initially created by DocCloud
to serve as the basis for a multi-language platform
as a service engine.
And OpenShift was a platform as a service open source project
and Red Hat product.
We actually got a very quickly got an email back,
which is, oh, we're thinking about this project.
We don't know.
We can kind of give you some details.
They kind of walked us through this demo.
They're calling it the seven lit at the time.
They didn't really have a name.
And this is based on what we do internally at Google, at Borg.
And we're thinking about open sourcing it.
And I was interested.
It wasn't impressive.
The idea of working on something completely new, though,
like something that was from scratch
but was based on those ideas, was appealing.
So we were interested and we were excited,
but they're a little wishy-washy.
Google was like, well, we don't know whether we're
going to be able to open source this or not.
We don't know.
It's a little, we're just not sure.
So we were like, probably our best bet is this Mesa's thing.
I guess we're just going to announce
that the next version of OpenShift will be Mesa's and Docker.
And Mesa's isn't really designed around Docker,
but it was the best option we had.
An internal phrase we use sometimes
is uncomfortably exciting.
Meaning maybe we bet off more than we can chew.
It's hard to know that this is, in fact, the right journey.
We want to take these crown jewels and open source them.
And it's going to be OK because we'll make it up in volume,
so to speak.
And that is kind of the essence of the strategy,
but the volume comes from managing the service.
So it was a pretty quick timeline.
We had the code available, but getting permission
to release it as part of DockerCon
was made relatively late in that game.
So after we got the OK to open source,
now it was time to actually start getting the thing ready
to release it.
Luckily, we engage with many people outside of Google.
And like Clayton from Red Hat, we want to work together.
Two weeks before DockerCon, my boss, Matt Hicks,
I think he texted me, he's like, so Google just got back to us
and they want to know if we're in or not.
And so we tapped him and said, yeah, sure, we're in.
And so I think the next day they gave me private commit access.
I was either the first or the second external contributor
to Kubernetes in that sense because I had access.
There were a few others from a few other startups as well.
Clayton just showed up and he just came in
and started just cleaning up.
He helped us get a lot of the code
into proper go lang semantics.
And we were like, yeah, we really do like this Clayton guy.
He's awesome.
And that really became a big part of what
started that kind of open community working.
I can't wait to see all of you at DockerCon
and see the amazing, incredible, creative things
that everybody in our community is doing
and everything that the people on this team are doing.
So thank you.
Thank you.
Thank you.
I believe we actually open sourced the repos
on the day of the DockerCon keynote
as part of that keynote was the announcement of Kubernetes
publicly and made the code available that day.
Now, we're releasing something called Kubernetes today.
It is yet another orchestration thing that sits on top.
That is not a bad thing, as Solomon said.
There are many of them because it's
an exciting and important area and an area where
we need to get agreement.
And I'm not going to give a demo, sadly.
I only have 25 minutes, most of which is gone.
But Craig and Brendan are going to actually go through that
later today at 2 o'clock.
You can go see that session.
Now, the reason to do an open source release right now
in this space is because it's about the ideas.
So I'm going to talk a little bit about some of the ideas in it
and then you can get the details from Brendan and Craig.
This was it.
This was our big moment.
We had something that we were pretty embarrassed about
in terms of its rudimentary state.
But that's when you're supposed to launch, right?
So we got out there and with Eric Brewer's support,
we made our announcement.
We obviously wanted, we're doing orchestrations.
We want to have a manager that's going to have an API for managing
the cluster.
We announced June 10th at DockerCon, which is the day
I created my Twitter account.
With these container technologies,
it's the first time that customers have really had an opportunity
to package up their application in a framework that's portable.
If I recall, there were five or six other container management
systems announced the same day.
Some were proprietary and some were also open source.
The same day.
Every big startup I felt like had a container orchestration
project and half of them were announced at DockerCon 2014.
We all split up and went to different sessions at that DockerCon
and we were texting each other back and forth like,
oh, Sonzo just released this.
Oh, Facebook just announced Tupperware.
Oh, it was very interesting.
There's two ballrooms and so it was Google announces Kubernetes,
Startup announces container orchestration framework.
Facebook was there talking about Tupperware.
Heroku was there, I believe.
Some of the Heroku guys were there talking about how they
were all in on Docker.
Startup announces Docker monitoring solution.
Startup announces new business around container native orchestration
and monitoring.
Everybody had the same idea.
This is not a brand new idea.
But there was a tension because Google
announced this net new thing that was almost more exciting than Docker.
And I remember being there and there were times where there
were a few conversations I had with people that was like,
this is a little awkward.
Like Google's stealing Docker's thunder.
Docker's swarm project, lib swarm at that time.
It was a library for starting containers on a few machines.
Its commits went to zero the week we announced, I think.
And that kind of set in motion some of that tension that played out
over the next couple of years, which was this young startup
super excited about what they're doing.
Huge community traction, really captured developers' hearts and minds.
And then the project that I personally really believed in,
which I think Docker is great, but Kubernetes was about the application,
was about enterprise IT, was for us,
was very much the evolution of the bets we were making around.
How do you streamline building applications for everybody?
And that larger vision at DockerCon ran headlong
into Docker Inc's hopes and dreams.
And it got really awkward after that.
I think we are on the verge of something that's a major shift.
And I really look forward to making a great space together
with all of you.
Thank you so much.
Thank you.
Google was in a war about cloud, and that is the most important thing for them.
I was working as an engineer with other engineers.
Engineers are not so much about corporate strategy and politics.
Most of them were really about tech merits and about the quality
of what we were building together.
There was no such thing as politics in the open source project.
There was corporate politics above.
And of course, eventually, it started crippling into the open source relationships.
To understand the way that Kubernetes and Docker Swarm and Mezos were competing
with each other back at this time, I think that there are multiple layers to it.
But on a technical level, I think it's important to understand the different
philosophies that Kubernetes and Mezos and Docker Swarm really brought
to this problem space.
Kubernetes, our focus was on making sure that we presented a really clean API
that fit the way that we viewed the problem space of being able to deal
with scheduling containers and work across a bunch of different computers.
Mezos, the focus was on the scheduler.
And Mezos had a very, very sophisticated scheduler that had some really interesting
qualities in terms of the way that they distributed that scheduling problem
and distributed work.
Docker Swarm, on the other hand, starting out, they wanted to view
a cluster of computers as one big computer.
And once you introduce networks, once you introduce partial failure modes,
different scheduling constraints, there ended up being a lot of capability
sacrifices that were made in Docker Swarm to have that initial experience
be as easy and simple as possible.
Kubernetes is what you would build if you had enough time and experience.
I doubt anyone had the same level of experience that Google had.
I'm at CoreOS, and we have this thing called Fleet, and we have our vision
for building container orchestration.
We have another startup around the corner, Docker Inc., that has their own vision
about container orchestration called Swarm.
And we also have an incumbent startup, Mesosphere, that has a successful
orchestration platform already out there, used by giant companies.
And so there's a new entrant.
There's this thing called Kubernetes, and what do we do about it?
Luckily, I was working at a startup that was open to new ideas,
and so I was just contributing at night time.
My contributions were about making sure that Kubernetes worked well on top of
CoreOS, and at the same time, I noticed that I was the one speaking at meetups,
telling people, there's this new thing, here's how it works.
There was a tool called Flannel that came out that made it really easy
to implement this with Kubernetes.
There was another startup at the time, Kismatic.
I think Kismatic, Patrick Riley and Joseph Jax, they knew that Kubernetes was
probably going to be a thing that was going to stick around for a little while.
And so Patrick and team put up the money to do the first KubeCon here in San
Francisco, and they asked if I would emcee the conference because a lot of people
knew me from talking in the space.
So our next session before lunch, we're going to talk to the Kubernetes core team.
And so now I'm on this stage at a single-track conference.
Maybe we have 300 or 400 people there, but now people were showing up to say,
what is this Kubernetes thing that we keep hearing about for the last couple of years?
And so I think the attraction to Google was who is this kind of crowned by the
community spokesperson for this thing we call Kubernetes, this project that we
put it out.
I started to get a lot of opportunities from around the industry.
I remember the Google team was just like, hey, how about Google?
And I thought about it for a while and it was a natural fit.
And I remember being honored to become like the voice of that project.
I was able to meet people where they were and then show them what was next and
then invite them to contribute.
At the time, Kubernetes was open sourced.
We didn't take it for granted that the thing we built would automatically be successful.
That original release was really raw.
I mean, you know, it's there in the GitHub history.
You can go take a look at it.
There was a lot of duct tape and bailing wire holding it together.
So we set ourselves a really ambitious goal.
We want to GA this thing a year from now.
Right?
Okay, let's start working on 1.0.
I have never in my career worked so fast or so furious.
It's kind of like you just have the baby and you just couldn't stop love that one.
And you never feel tired, put all the effort into that one.
You worry about sometimes maybe a little bit of a success or maybe in the right path.
In the first two years of the project, I got 200,000 notifications from GitHub in my inbox.
The fact that we open sourced it so early really invited people to get involved.
You have to learn hard to think about how to inspire people outside of Google, outside
of Red Hat to help.
At 1.0 it was going to be for web app style, stateless, low scale systems.
That's it.
Anything else we're not focusing on, back burner, take it off the plate.
How little can we get away with was our theme.
There's a lot of conversations in that first year where it was the developer and application
focused mindset kind of clashing with the how to build systems at scale mindset.
We just had to draw the line about what was going to be done and what was not going to
be done.
So there was a lot not done.
There was this fear there was too complex.
Docker swarm was much simpler.
Mesos was more powerful.
It could scale to thousands of nodes and Kubernetes I think at the time was 100 nodes
or something.
Andrew Spiker was kicking the tires of Kubernetes right after we open sourced it and concluded
that it wasn't mature enough, it didn't scale.
And Netflix chose Mesos and other large companies chose Mesos like Apple and Airbnb because
it was more mature, it existed for a few years already and it scaled to 10,000 instances.
If you want a technology to be ubiquitous, if you want a technology to be used by everyone,
if you want a technology that you can actually create that alloy that's stronger than the
contributions of any individual, it needs to be held under open and free governance.
At the beginning it was just a promise, right?
We will put this in a foundation.
The project was actually in a Google owned GitHub org and the community contributor
license agreement was a Google contributor license agreement and people had to basically
give Google the right to re-license the product.
That put off a lot of people.
As we started looking for partners to get involved, the fact that the project was still
dominated by Google, despite us talking about it being open sourced, it still gave folks
a lot of pause.
Many large companies do not like their employees signing a competitor's contributor license
agreement.
That was the context where Craig went off and did a lot of coalition building to eventually
form the Cloud Native Computing Foundation.
We set a hard milestone for 1.0 to be OSCON when we would announce 1.0 and also as it
turns out CNCF.
It was probably the most stressful thing I've ever done, was trying to bootstrap and navigate
the multi-vendor interests in getting something like CNCF off the ground.
There was about six months of essentially bootstrapping work of trying to get this thing going.
It was announced June 2015 at OSCON.
It's clear that we need to start working more closely together as a community.
As we looked at the 1.0 point of Kubernetes, as it's going production ready, as we're looking
at the future of this project, it was clear to us we had to take it to foundation.
My idea at the time was like, I want to go build bridges with our competing orchestration.
Even though Mesos has opinion on something, Kubernetes' opinion on something, there's
usually something, there's usually some common interest.
Maybe we could standardize how storage is attached to a container.
Let's make that work.
I convinced Ben Hyman to come on the TOC, as we called it.
There was a bunch of other folks from ... We had this good mix of folks from different
overlapping and competing technologies to start the organization.
We were able to secure a pretty rich base of support early out of the gate.
Getting that to happen was maybe the most critical piece to the project's success.
All the technology aside, without that, I don't think it would have gone anywhere.
We believe it's going to take a whole community.
With that in mind, with Linux Foundation and a broad array of industry partners, we're
announcing the Cloud Native Computing Foundation today, which will be charted to take Kubernetes
and adjacent technologies, harmonize them, and ultimately progress the agenda of Cloud
Native Computing for everybody.
Thank you.
We built the organization around Kubernetes in a very specific way.
This was work that was done by what we called the Bootstrap Steering Committee.
That was the group that was early, early contributors to the project, and then myself added to that
to try to build what our governance would be, because they reached a point in 2016, 2017,
where it was not easy to figure out how to get something done, and you just had to go
ask a Googler, but which Googler, and that wasn't fair, it wasn't well documented.
The challenge then was how do you take this fragile community where everybody felt a sense
of ownership and slide a sort of governance model underneath that?
If you want to build a distributed compute platform, Conway's law suggests that you
need a distributed decision-making setup.
We pushed very hard to have that distributed and closer to the code than to a leader.
We saw this great inflection point of interest and engagement, because as we started moving
things out to more open governance, and as we moved the copyright and intellectual property
to the CNCF, more larger companies were able to participate, and at that point it really
became an exercise in how to grow the next set of leaders, and taking the community as
the piece that needed to be grown more than anything else, and nurtured more than anything
else.
We didn't have a manager actually, even so Tim and I were in Mountain View at the time,
and Craig, Joe, and Brendan were in Seattle, and we all reported to different managers
for several months while we were working things out.
My first time at Google was mid-February 2016, from a Kubernetes timeline, the team was working
on Kubernetes 1.2.
The thing that was obvious and clear is that they were on the edge of burnout.
Everybody had been working 16-hour days, six days a week for a long time.
It was like 24-7 job.
People would ping you on Slack, on Git, on Twitter, customers calling, the build is breaking,
someone cannot come in, the work actually never stops.
By the end of 2016 we were having between 700 and 1000 people contributing to each release.
It was clear that with the way we were structured, we will fail.
We will burn ourselves.
Ken knew that what we were doing was not going to work in the long term.
It was great when it was five or six people.
She knew that she needed to grow this organization if we were going to keep on the trajectory
that we were on.
I think at the first phase, coming in as a leader and as a manager of that team, I first
wanted just to stabilize and make space for folks.
That idea that we are not going to solve things for the short term, but we are actually going
to think about how it's going to be solved from this point on, was I think a critical
point at the success of Kubernetes.
That idea actually evolved later on to what we call sustainable success.
She was very deliberate about putting together the teams and giving responsibilities to which
site had which things that they were responsible for and how we were going to grow the team
just to get it back under control.
It was also time to focus a little bit on stability and predictability and now we had
real customers.
We had people who were actually using 1.0 in production, big video games you might have
played and we needed to focus on making sure that we didn't screw them up.
We learned that Niantic was building Pokemon Go on GKE.
And so very quickly we got in a bunch of conversations with the folks over at Niantic about how it
was going, what were they doing, what could we do to help them out, how was Kubernetes
fitting their problem.
They ran Pokemon Go on one big cluster, which was exciting and terrifying.
Once it launched, they actually experienced 50 times the load or the volume than what
they expected.
We sort of went into, I won't say panic mode, but near panic mode as we tried to figure
out how we were going to give them more headroom because the game was just growing and growing
and growing.
They were just pushing the limits of everything.
We were like an engineer on the team, we were on call, we were like 24-7 managing those
clusters.
We see the entire world going with their phones everywhere and in my head all I'm singing
like please stop playing.
We are not ready for that.
What was amazing that we were actually were ready for that.
This was a huge deal.
We all felt great about it, definitely validation of what we were doing.
The idea that we were able to continue and grow and see that game expand everywhere and
meet the demand, like this is why Kubernetes is a thing, right?
To build something, you're not really sure how it will scale, but then you have this
amazing foundation that really helps you to get to that scale.
I hate the term the container wars, but the debate about what were people going to use
was still very much going on.
Niantic took a big bet on Kubernetes, but definitely there was a lot going on in the
market.
You couldn't read a blog site without running across a Docker versus Kubernetes.
The container wars began the day Kubernetes was announced at DockerCon.
It didn't feel like a war in the beginning.
It felt like tension about what's on top.
Container wars were complicated by the fact that there was also a fight over orchestration,
the things that managed containers above that.
There's a lot of value in being the top of the stack.
Look into any kind of marketing materials or website everywhere.
You're never going to find somebody who's proud of saying that we are the little dot
at the bottom of the stack that you never think about.
Everybody wants to own the user experience.
Everybody wants to be on top of the value chain.
This is the race that happened during this Docker era about who owns the developer experience,
who owns the app's experience, who owns the mindset of the engineers ultimately because
this is where the value really is and this is what the people are going to buy.
People were fighting over what should be the dominant way to run containers, how to define
applications.
A lot of both approaches had valid ideas.
And I think the mistake that was made early on is people thought there was going to be
a zero-sum game.
If you looked at Kubernetes at the time, it was layered.
Linux was at the bottom still.
Docker was the container runtime.
SCD is where we stored everything.
It looked like it was a big combination and collaboration of the entire industry at the
time.
But no one really treated it like that and I think that was the missed opportunity.
So I think the tension was people felt like it was a zero-sum game.
Whoever won the orchestration wars would somehow win all the business and all the customers.
We couldn't have done Kubernetes without Docker.
It just would not have been a thing that would have worked out.
Something else would have happened.
And yet people wanted to make drama out of it.
The world loves drama and I don't think there was nearly as much actual drama as people
made out of it.
At the end of the day, everything eventually settled down where in CNCF we brought in both
container D and Rocket under the same foundation and we also pushed to standardize the image
specification.
It became a standard.
For a good three years or four years, every day was, is Docker going to change to adopt
Kubernetes?
Is AWS going to suddenly adopt Kubernetes?
What every DockerCon were like, is someone going to change everything with an announcement?
And then one day Solomon called me and said, we did it.
We built Docker on top of Kubernetes and we're going to announce it at DockerCon in Copenhagen
a week after next.
Can you be there?
Docker announced that they were going to start supporting Kubernetes and DockerCon in addition
to Docker Swarm.
But I think, you know, they recognize that this was something that they couldn't ignore.
And so Brendan and I were there to, you know, say congratulations and it was really a cool
moment where I finally felt like we stopped pulling against each other and started pulling
together.
2017, a lot of competitors embraced Kubernetes.
And really near the end of 2017, Amazon launched a Kubernetes product.
Back then when joining Google even, I never thought AWS would seriously have a Kubernetes
offering.
For the longest time, Amazon had been a holdout on this.
And so to see it come full circle where it's like have Amazon supporting it was one of
the, the, the largest signals that Kubernetes, you know, at least for me that Kubernetes
was really here to stay.
And now Kubernetes seems to be the de facto standard across all cloud providers globally.
You know, when we saw MesaSphere, you know, rebrand and embrace Kubernetes, when we saw
Docker start to introduce their own Kubernetes offerings, it really kind of closed out that
arc.
But there were a lot of highlights at every point of the journey that, you know, were
worth celebrating.
It was a phenomenal experience.
In the end, I think Kubernetes won in part because it had a vast army of contributors
behind it.
And it just kept marching along with a huge number of commits per day.
And that rate of change kind of trumps everything else.
Back then, most of the things that really attracted people who even knew the tech and
who knew containers was the UX.
Yes, in a sense, Kubernetes managed to capture the orchestration layer building on top of
Docker.
But they never really captured the developer experience.
And the truth is, nobody really is but Docker right now.
And that's still, I think, one of the areas that there's a lot of things to do and we're
not done yet.
And Docker as a company today decided to focus on the developer experience side of things
for a good reason, I think.
I'm seeing Kubernetes being used in ways that surprise me and delight me to see, like, wow,
I hadn't even thought that that was something that we could do.
It's clear that Kubernetes is going to take on a life of its own and go off and evolve
beyond what any of us really imagined when we started out.
Things cannot repeat.
And I think we just lucky have that right time and right support.
The container wash has benefited from the fact that there was lots of VC dollars fueling
this race.
But the truth is, there was no zero sum game.
The best ideas from that race are still with us.
And they've consolidated now.
And they've consolidated so far into the latest checkpoint, which is Kubernetes.
But the truth is, there's going to be something that will place Kubernetes.
Maybe it's some serverless component.
Maybe it's a different way of thinking about these APIs and how they should fit together.
And typically, the higher you get gives freedom for the system underneath you to also evolve
and sometimes become simpler.
So I think this is just the latest checkpoint.
But whatever war people were fighting, there was nothing to actually fight.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.

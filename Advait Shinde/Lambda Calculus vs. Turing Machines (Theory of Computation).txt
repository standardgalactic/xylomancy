All right, let's jump in.
Cool, introduction to the theory of computation.
I have been more excited about this talk
than any other talk that I've given in my entire life.
So forget about the company vision,
we're talking about the theory of computing.
I learned, I was introduced to some of this stuff
when I was an undergrad at UCLA about 10 years ago,
and I didn't understand it.
And it was kind of presented as like,
here's the sort of tablets from the mountain
and learn and memorize them and this is why computing is.
And it took me 10 years and I finally understand it.
And it's so mind-blowing that I wanted to talk
to you guys about it.
So this is what it's about.
The subtext of this is it's a tale of two towers
and this will make sense as we get into it.
But the preface is an intro to axiomatic thinking.
This is a kind of strange way of thinking
if you guys haven't been introduced to formal math.
It's not, it's strange, let's just jump in.
So two plus three times seven, this is I think 23.
Why do we know the answer to this?
And I asked this question and I encourage you guys
to take the perspective as if you were like an alien
who has never really seen symbols like this.
Or perhaps you didn't even understand
the concept of multiplication or addition.
How do you know what to do over here?
So let's just step through it.
The first thing that you would do is three times seven
is 21 and two plus 21 is 23.
So what do we have over here?
We have these symbols called numbers.
We have these things called operators.
And then we have this interesting thing over here
where we can substitute an operator for its equivalent form.
So three times seven is the same thing as 21.
And here we've kind of made that substitution.
And then these operators also have precedents.
Like we knew to do three times seven first
because of all of the middle school math homework
that we did.
And then we have this final reduced form.
So in theory, each of these levels are equal to each other
because we've just kind of made substitutions along the way.
And what I'm trying to get at is we have these collections
of rules that we kind of take for granted
that we never really thought too much about.
That if you really examine like across representing
an operation or this little like two and one
like right next to each other represent 21
these are really kind of non-trivial concepts.
So let's dig into some of these.
The first idea is that three times seven
is actually seven plus seven plus seven.
And so this is to say that the rule of multiplication
is actually defined in terms of addition.
This is interesting.
If some rules are defined in terms of each other
we can say that the rule is kind of redundant.
So in theory, like we don't really need multiplication
in math because every time we multiply
we can just add instead.
And therefore multiplication hasn't really given us
any more expressive power than addition already provides
for us, right?
So if some rules are redundant
then maybe we can ask the question of like
what rules are non-redundant?
Or really what is the minimum subset of rules
necessary to describe all of math?
And so we can call this minimum subset of rules axioms.
And this comes from the word Greek axioma
that which is self-evident.
And then we can call all of the other derived rules theorems
which is like a proposition to be proved.
And so the question is what are the axioms for math?
The minimum non-redundant set of rules
to define all of math.
This is a question that nobody really thought about
or nobody had a really compelling answer to
until this guy came around.
His name is Giuseppe Piano.
He was an Italian mathematician.
And in 1889, so a little over 100 years ago
he put forth these axioms.
There's nine of them.
Only three of them are interesting.
So bear with me for a moment and keep in mind
like we're starting with a blank slate.
So there's no numbers yet.
And we have to define what numbers are.
So first we define the first number, which is zero.
And so Piano says zero is a natural number.
The next thing he does is define what equality is
because we don't have that either, right?
So for a thing x, x is equal to x.
This is what equality is.
This one is not very interesting either.
If x equals y, y also equals x.
If x equals y and y equals z, then x equals z.
This is the transit property.
If b is a natural number and a equals b,
then a is also a natural number.
So this is sort of saying we have this collection
of things called natural numbers.
Right now we only have one of them, zero.
And if b is a natural number and a equals b,
then a is also a natural number.
So you have this like glue-like property
of natural numbers.
This is an interesting one.
We define this function s such that s of n
is a natural number.
s is like a successor function.
It's what it literally stands for.
And so now we have a way of going from zero
to producing the successor of zero with this function.
So s of zero is the successor of zero
and s of zero is also a natural number.
Is everyone following?
And I think the best way to go about this
is to just immediately stop me
if you guys have any questions
because this is gonna get more and more complex.
Sounds good?
Great.
m and n are equal to each other
if and only if their successors are equal to each other.
There's no n such that the successor of n equals zero.
So here we're not going into negative numbers.
We're just defining the natural numbers.
So there's no successor for zero.
Or zero is not the successor of anything.
That's what this is saying.
And then finally, the very last one,
if k is a set such that zero is in k
and if n is in k, it means that s and n is in k,
then k contains every natural number.
This is like the base case, zero is in k,
then the inductive case, if n is in k,
it means that s of n is in k.
And then therefore, k contains all the natural numbers.
So here they're saying there's no like loops.
It's just like this directed graph
that goes all the way out to infinity.
Sound good?
This is it.
This is all you need to define all of math.
So you'll notice we never define numbers besides zero.
We just define the concept of zero.
And yet we use these symbols like one and two
and three and four and so on.
And here I'm proposing the concept
that one is actually just syntactic sugar for s of zero,
which is to say that these two forms are equal
to each other.
And if we were more precise,
we would actually prefer the form on the right,
but it would be kind of annoying.
And so we have this concept of one
and then two is the successor of one and so on.
Seven is actually s of s of s of zero.
I think you guys get the idea, right?
We haven't added any new information here, right?
And in theory, when we do our math,
we should prefer to reduce, if we were a piano,
we would prefer to reduce the form all the way down
into like this thing over here,
but that's just too confusing.
So we're okay with the syntactic sugar representation
on the left.
Sound good?
Okay, so syntactic sugar is sort of convenience rules
or symbols that we don't need to further reduce down
into the primitive forms.
So let's now define addition.
So addition can be thought of as an operation
that maps two natural numbers to another natural number,
right?
And the syntax is a plus b, so you guys know.
And we just need two rules.
The first is sort of the base case,
a plus zero equals a, sort of obvious.
This one, you might have to pause
and think about it a little bit.
A plus the successor of b is equal
to the successor of a plus b.
So what we're doing over here is b
without this little s in the wrapping, right?
We're just kind of taking this s
and wrapping it around the whole thing, right?
And so the term on the right, b,
is actually one less than the term of successor of b.
So we're kind of going down.
This will make sense in just a moment.
I think let's go through an example.
So three plus two, the rules up there are on the right.
The first thing that we do is expand
out our syntactic sugar.
So we have s of s of s of zero plus s of s of zero, right?
And now we need to apply one of our rules.
Obviously, we can't apply this rule
because it doesn't match.
So we have to apply this rule, right?
So a is in purple on the left or pink.
And then we have s of b, b is this purple thing over here,
equals s of a plus b, right?
So s of a plus b.
Do you guys see that substitution?
And then we do that again.
And then now we're in a form
where we can apply rule number one,
a plus zero equals just a by itself, right?
And that's five.
And now we're gonna add.
All right, so basically this is compelling
because we didn't have the concept of addition
in terms of the axioms.
And we define the concept of addition
as recursive incrementing, essentially.
And now we have this property of addition,
which we can use to define some other things.
For example, we can define multiplication.
a times zero equals zero,
a times s of b equals a plus a times b.
And you can work out that obviously intuitively
that addition and multiplication
are kinds of intrinsically related to each other.
So we have the piano axioms,
we then we built addition
and then we built multiplication on top, right?
And it's this type of thinking
that I wanna really imprint in your minds.
And here I'm inventing this new concept called axiom towers.
And you can think of axiom towers
as having a foundation, which is the axioms themselves.
So in this case, the piano axioms.
And then on top of that, we built addition.
And then on top of that, we built multiplication.
And then maybe we can build more stuff on top.
And it turns out that the piano axioms are sufficient
to basically describe like most of math.
So from multiplication,
you can kind of imagine that you can build division
and from division, or not from division necessarily,
but you can imagine also,
you can represent the integers,
which are negative numbers from just the real numbers.
We won't get into the proofs,
but I'm sure you guys can kind of envision
how this might be the case.
And then once you have negative numbers,
you can imagine defining rational numbers,
which are just kind of,
in the context of division,
a rational number is just a numerator and a denominator, right?
And then from rational numbers,
you can maybe get to exponentials,
which is just sort of repeated multiplication.
And then from exponentials,
you can get to irrational numbers,
like the exponent of a fraction gets you irrational numbers.
And then maybe you can build imaginary numbers,
and so on and so forth.
And all of it is kind of stacked on top of
just these core axioms at the very bottom.
Sound good?
Cool.
This is gonna get interesting, I promise.
All right, so the first idea
is that the axioms are not divine, right?
There's nothing special about them.
In fact, when Piano first wrote his papers,
he started off with one as like the root.
He didn't start off with zero.
But then mathematicians later said like,
no, no, let's start with zero, it's better.
So you can imagine starting off with one and say,
you can imagine using like a predecessor function
instead of like a successor function.
But here I'm making a claim that some axiom towers
are better than other axiom towers.
So let's say more useful than other axiom towers.
Like for example, Roman numerals are just horribly inconvenient
at doing anything useful, like multiplication.
And yet everything that you can do in regular numbers
you can also do in terms of Roman numerals.
And so Roman numerals aren't like,
they're just a different set of axioms
that are somehow slightly less useful.
All right, axiom towers don't have to correspond to reality.
So Euclid was a Greek philosopher
and he's sort of like the father of geometry.
And way before Piano, he put forth the axioms for geometry.
And we call his flavor of geometry, Euclidean geometry.
And one of the axioms that he kind of put forth
was that if you have two parallel lines,
let's say like this Y right here,
as well as like the Y axis, these are parallel to each other.
So if two lines are parallel,
then they stay parallel forever.
They never intersect.
That was one of his axioms.
But it turns out that you can have these things
called non Euclidean geometries,
which essentially forego that axiom.
And the example is sort of like a globe
where you have these vertical longitude lines or meridians.
And the meridians are all parallel to each other.
But as you see at the poles, they all kind of intersect.
So a non Euclidean geometry is one that foregoes
this notion of like parallel lines don't intersect.
And it turns out that there's all sorts
of really interesting non Euclidean spaces
that you can imagine that don't at all correspond to reality.
And so there's this whole sets of branches of mathematics
that kind of conceptualize all sorts of different axioms
that are unique and interesting
and form this sort of logically coherent axiom tower
on the basis of those axioms.
And in many ways, those towers don't correspond
at all to reality.
And it's just sort of mathematicians having fun.
Interesting idea.
Okay, symbols.
So we talked a bunch about symbols.
It might be interesting to think of the symbols
as sort of separate from the rules.
But it turns out that if you really examine the situation,
the symbols don't really make sense without the rules.
And the rules can't really be expressed without symbols.
And so symbols here are making the claim
that they're kind of intrinsically related to each other,
really two sides of the same coin.
So this symbol, if you've done any sort of computing,
0x20 is the hexadecimal number 32, right?
But it's also like, sorry,
it's also the ASCII symbol for space, the space character.
So whether you're interpreting the symbol
in the context of hexadecimal math
or this axiom tower of ASCII or Unicode,
like the symbol has meaning only in the context
of a particular like frame of reference,
which is the axiom tower
that you're interpreting the symbol in.
And they're one and the same.
You can't separate out these ideas.
Another interesting example is DNA.
So there's this like funny concept
that DNA consists of these base pairs
and that all of the human genome
is sort of some ridiculously small amount of data.
And it's just like claim that,
therefore like life is not really that complex
because there's really not much information in DNA.
But if you really examine this question,
DNA by itself is completely meaningless and useless
without the corresponding like cellular machinery
that's able to actually unpack it and read it
and build actual life from it.
So DNA and the thing that reads the DNA,
they're intrinsically linked to each other.
The symbols and the rules are one and the same.
Here's an interesting philosophical claim.
I think that math is actually discovered and not invented.
And the analogy that I have is sort of a visualization
of this axiom tower and the top levels
of this axiom tower are kind of obscured.
It's not exactly clear what they should be.
And what you're doing as a mathematician
is kind of like discovering consequences
of having initial axioms.
Sound good?
Questions?
All right.
So recap, axioms are self-evident.
They're taken as given.
Theorums are derived from redundant rules.
Axioms and theorems stack up together to build
these things called axiom towers.
And some symbols are actually just syntactic sugar.
Symbols and rules are intrinsically related.
And math is a discovery of the consequences
of foundational axioms.
And the axioms are arbitrary,
but some axiom towers are more useful than others.
All right.
You guys are comfortable.
We're gonna get to the exciting part.
Computation.
This is a graph of certain things
that we kind of take for granted today,
like running water and electric power over time.
And here I wanna point out that in the 1930s,
more people had electric power than running water.
And that number was around 65, 70%.
So you can imagine being in the 1930s, right?
And at this point, algorithms had already existed, right?
So way back in 2000 BC,
Egyptians figured out how to multiply two numbers together.
Babylonians figured out how to factorize things
and find square roots.
Euclid's algorithm, which is really cool,
the same Euclid as geometry.
He figured out how to get the greatest common factor
between two numbers.
And this algorithm is actually really beautiful
if you've never seen it.
It's called, I've actually never pronounced it,
but I've read it, it's sieve or erratostinis.
It's a way to generate prime numbers.
And Al-Qarizmi figured out how to solve linear equations
and quadratic equations.
And it turns out that the word algorithm
actually comes from his name.
So we have like hundreds or thousands of years
of understanding of these things called algorithms,
which really were kind of informal at the time.
And you can kind of consider them as like sequences
of instructions to follow to do something, right?
But we didn't have like a precise axiomatic definition
of computing in the way that Piano defined
the axiomatic definition of math, right?
In the 30s.
And so these guys pretty much at exactly the same time
did that independently.
Alan Turing created these things called Turing machines.
Alonzo Church created these things,
this thing called Lambda Calculus.
And Kurt Gödel created these things
called general recursive functions.
So we're gonna ignore the last one
and actually drill into these two,
Turing machines and Lambda Calculus.
And the really, really cool thing is that
these axiomatic systems are both reasonable
and good definitions for computing,
but they look very, very different from each other.
So we're gonna talk about what they are.
So Turing machines.
You can envision a Turing machine.
So Alan Turing was thinking about,
like you can kind of empathize with what he was doing.
He was looking at all these algorithms that we have,
and he was trying to reduce like all of the algorithms down
into their most principle like reduced forms, right?
And then essentially use that as the base of an axiom tower
and build higher level constructs on top of that, right?
This was his goal.
And so he envisioned this concept called a Turing machine.
And a Turing machine starts off with this thing,
which is like an infinitely long tape.
And the tape is actually broken up
into these things called cells.
And in the cells, you can actually,
like each cell can either be marked or unmarked.
So here we have empty cells
and the X's correspond to marked cells, right?
And this is infinitely long.
It goes on in both directions.
And you have this thing called the head.
And the head can do some stuff.
You can move it to the right.
In every case, it's always pointing to a particular cell.
You can move it to the left.
You can mark the box that the head is pointing at.
You can unmark the box.
And if the box is marked, you go to end.
We'll talk about go to end in just a moment.
And if the box is unmarked, also go to end.
These are all different instructions
that you can provide to this like Turing machine.
And the execution of the Turing machine,
essentially you start off with a blank tape
and a list of instructions
that you want this Turing machine to execute.
And the list of instructions are ordered from zero through N.
And you execute the first instruction.
And if it's an ordinary instruction,
you execute the next instruction.
And if it's one of these like jump instructions
at the bottom, then if it tells you to jump,
you go to the Nth instruction.
Sound good?
And basically Alan Turing showed that this model
is sufficient for all of computation.
Anything that can be computed
can be computed with just these primitives.
That's all it takes.
Okay, so there is an actual virtual implementation
of a Turing machine called brain flap.
It's a technical term.
And basically there's a few instructions
and it essentially you can imagine it
as like a virtual Turing machine.
And it has these instructions.
You can move the head to the right,
you can move the head to the left.
This is slightly different than a Turing machine
because the cells don't contain just like binary values
of marked and unmarked.
Instead they contain numbers.
And you can increment numbers and decrement numbers.
Just integers, right?
And then these characters, so the open bracket,
like if the value at the head is zero,
jump forward to the matching like closed bracket.
And then the closed bracket is if the value at the head
is non-zero, jump back to the matching open bracket.
So here we've kind of defined our jumping behavior.
And then this language also provides like functionality
for input and output, which is something
that Turing didn't necessarily require.
But this makes the language a little bit more useful
because you can have it do stuff like print, print out output.
And every other character is ignored.
So anything that's not this magenta color
is basically ignored in this language.
So this is the implementation of adding two numbers together.
And if you squint, you can kind of see the recursive definition
that Piano kind of described earlier, right?
We'll actually go to a much clearer example.
So in the beginning, you can imagine the head
is pointing at cell C0 and we increment it twice.
So now C0 has a value two.
And then you move to the right to cell C1
and you increment it five times.
So C1 has a value five, right?
And then you start your loop.
The first thing that you do is you go back to C0
and you add one to it.
And then you go right to C1
and then you subtract one from it and you keep looping.
And your loop will basically end.
I can't think properly right now,
but once C1 essentially reaches zero,
then the loop will end and your program will terminate.
And now you have this ability to add two numbers,
C0 and C1, right?
This is an algorithm for computing
the Mandelbrot fractal set.
If you guys haven't heard of this,
it's just a really cool like fractal,
I won't get into fractals,
but basically this is a program, it prints out this, right?
So just using our turning machine,
we were able to now output like this fractal.
Cool, so basically Turing created an axiom tower
for computing and an algorithm is computable
if and only if it can be encoded as a Turing machine.
And Turing showed this before the existence
of electrical computers.
And he also showed this when he was 24 years old.
If you guys have heard of the Turing Award,
it's basically like the Nobel Prize for Computer Science
and it's named after Alan Turing.
So some observations, you need an infinite tape
and you need a program,
like which is a sequence of instructions to follow.
And you're constantly modifying the tape.
So you can think of the tape
as sort of like the state of your program.
And every instruction that you execute
that modifies the tape is in theory
kind of modifying the way that the program
kind of unfolds itself, right?
And the behavior of the program
is changed with every single tape modification.
And so therefore reasoning about the behavior
of the program requires understanding the state of the tape
at every moment of modification.
And so you can imagine sort of debugging
a Turing machine as perhaps similar
to debugging like an application
where you kind of think about
how the application's memory state changed over time.
And all of a sudden your ideal understanding
of how it's supposed to change
like differs from the way it actually changed
and there's your bug, right?
From here, Turing defined this concept
called Turing completeness
because you said that you can have other forms of computing.
For example, you can imagine like different instruction sets
for this Turing machine, right?
And he basically said that an axiom tower
that's sort of different than a Turing machine
is called Turing complete if and only if
it can be used to emulate a Turing machine.
And if it can emulate a Turing machine,
then it can compute anything that's computable.
That sound good?
This concept of Turing completeness has now popped up.
So it turns out that there's some interesting things
that are Turing complete.
If you guys have heard of Conway's Game of Life,
it's this basically life simulator, emulator, I guess.
And it's very simple.
You have this grid of squares
and each square corresponds to a living thing
and it's either alive or dead.
And at every step in time,
there's some certain rules for allowing
like whether the next, in the next time step,
the cell is alive or dead.
And so essentially, oops, let's see if we can get this to play.
I won't get into the rules
because they're not really relevant,
but every kind of step in this animation
is like the universe kind of unfolding
according to the rules of Conway's Game of Life.
And it turns out that the basic rules
are sufficient to represent a Turing machine.
And so Conway's Game of Life is Turing complete.
And so any algorithm that's computable
can be represented in Conway's Game of Life.
Magic the Gathering is also Turing complete.
So some researchers got together
and they looked at some specific cards
that allow you to place these counters
and they use the counters as a way
to represent an actual Turing tape.
And so just following the rules of Magic the Gathering,
they're sufficiently complex enough
that you can compute all of the prime numbers in the game.
Microsoft PowerPoint is Turing complete.
Obviously it has like macros and stuff,
but here using only auto shape, hyperlink and transition.
And this paper is hilarious.
Given PowerPoint's versatility
and cross-platform compatibility,
some have asked whether any other applications
are necessary at all.
Or if all computational tasks
can be accomplished through PowerPoint.
This research aims to definitively answer these questions
in the affirmative through the creation
of a PowerPoint Turing machine.
Okay, we've talked about Turing machines.
Now let's talk about Lambda calculus.
So to me, when I first learned about Turing machines,
I thought it was really kind of unintuitive
that such a simple thing can be used
to represent so much complexity.
But then after really thinking about it,
I realized that wait, the piano axioms are also very simple
and we can get all of math from it.
So it must follow that you can have
simple computing axioms and that's the case.
And I think for computer scientists and software engineers,
this is sort of what we're in the business of doing.
Like we take like simple building blocks
and we compose them together to build complexity.
And we have ways of reasoning about
how these things combine together to build complexity.
And we, it's sort of our job to make sure
that the complexity that we build is actually founded
and not buggy, let's say.
So it turns out that there's another flavor
or another axiom tower for computing
that was invented basically or discovered
exactly around the same time.
And it was discovered by Alonzo Church
and it's a thing called Lambda calculus.
And the way, basically in Alonzo Church's original paper,
he has a particular syntax for how he denotes Lambda calculus.
And JavaScript also has its own syntax
for declaring anonymous functions.
And because most of us are more familiar with JavaScript,
I'm gonna write both Alonzo Church's notation
as well as the JavaScript notation
to represent the same ideas.
So the first idea that he introduced was,
you can have variables.
So here X is a variable.
It's like a placeholder for a value.
Second idea is you can have functions.
And a function, this is on the left,
Alonzo Church's definition or notation.
And on the right, you have the ES6 equivalent syntax.
This is just a function that takes in one parameter Y
and has somebody M and M itself another Lambda expression.
So you have function definition.
And then finally you have function application.
So F of M, right?
So calling function F with a particular M.
So if in this case we call Y with some value,
like everything inside the body of M
gets replaced with whatever value we call it.
You guys should be really familiar with this concept.
And it turns out that this is all you need.
And with just these three concepts,
you can get something that's turned complete.
And so this is really, really unintuitive.
For me, it was way more unintuitive than the turning machine
which felt like this mechanical thing.
And therefore, because you can operate it mechanically,
perhaps it can do some computation.
Here there's no notion of mechanics.
I mean, maybe you have function application.
And so we'll get into like,
how can this possibly do stuff?
So the first thing is you have in Lambda calculus,
this concept called identity function.
This is the Lambda definition on the left
and the JavaScript definition on the right.
Obviously a very simple construct.
In JavaScript, we can have optional braces
for the input parameter, right?
So these two forms are equivalent.
So I'm gonna drop the braces.
And the names are just placeholders, right?
So X and X and Z and Q that are,
all of these constructs mean the same thing, right?
So there's nothing special about X.
So in Lambda calculus,
you can call the identity function on itself.
And basically what this is doing is,
this is the function, right?
And this is the thing that you're calling it with, right?
This is a JavaScript equivalent, right?
So what you do is for,
this is the input variable X
and this is the body of the function M.
And inside the body, whenever you call this function,
you replace every occurrence of X
with whatever you call it with.
So here every occurrence of X is replaced
with this Lambda function with the purple Xs
and you get this output.
Not very interesting.
Next you have this concept called curing.
So in modern programming languages,
most of them have this notion of having functions
that accept multiple input parameters.
But it turns out that you don't actually need this.
And the way Alonzo Church got around this idea
is that instead of a function taking in
two input parameters like this,
we just have a function which returns another function
which takes in an input parameter.
That make sense?
So this construct and this construct are equivalent.
And Alonzo Church said, instead of kind of
being verbose like this,
I'm gonna denote Lambda XY.M as equivalent to this.
So it's not equivalent to this thing on the right
because here in JavaScript,
we have a function that takes in two input parameters.
It's instead equivalent to this thing on the top right.
Sorry for that's a little confusing.
This concept is called curing.
Okay.
Next we're gonna define some true and false symbols.
So you'll notice we didn't have any definition of numbers.
And we didn't really have any definition of types
or booleans or anything like that.
We just had variables, function definition
and function application.
And so now we're adding more semantics to our language
by defining these symbols called true and false.
So very similar to how the number seven as a symbol
is defined in terms of the successor function.
Here the symbol T is defined as this function over here.
And what this function is,
is it's function that takes in two parameters
and returns the first parameter.
And the false symbol is the function
that also takes in two parameters
but it returns the second symbol or second parameter.
Is it following?
Cool.
So this is similar to our definition of seven.
And from here, now we can sort of build an and function
because we have boolean values,
let's see how we can build and.
So this is actually the definition of and
and we can kind of try it together.
So and applied to true and false.
Logically we know that this should be false.
So if we step through it,
the first thing that we do is we replace and
with this body over here.
So lambda x, y, x, y, f, T and f, right?
So here we have some lambda function
and here we're denoting that we want to apply
T to this function.
So what we do is the first parameter is x.
And so it's in this body, every single time this x appears,
we want to replace it with a T.
And so what we're left with is lambda y, T, y, f.
So this x has now become a T
and we have one more input parameter
that we need to resolve.
And then same sort of deal, every occurrence of y,
now we're calling this function with f,
every occurrence of y, we want to replace with an f.
And so we get T, f, f, okay?
And so as you guys remember, true is actually defined
as a function that takes in two input parameters
and returns the first one.
So in this case, it takes in two input parameters
and then just returns the first one, which is f.
So now we have some way of doing the and function.
All right, let's try another example and T and T.
Similarly, we expand and out to this thing
and then we apply T to this thing,
replace all the x's with T's and we get T, y, f.
And then replace all the y's with T's and then we get T, T, f.
And very similarly, T resolves to picking
the first parameter and then we get T.
So with just variables, functions and function application,
all of a sudden now we have like Boolean logic.
You can imagine how we can implement
or an XOR and so on, right?
So this is super unintuitive to me,
like the concept of defining true and false
as these functions, like a true is actually a function
which takes in two parameters
and false is also a function
which takes in two other parameters.
And from there, building other functions like and,
we can now do logical, like all of Boolean logic, right?
Cool.
This is the hardest slide, so you'll have to deal with me.
I'm gonna talk about the y-combinator.
So it turns out that many of you guys know about yc
up in the Bay and it was essentially founded
by a computer scientist who got the name
from this principle.
It's actually a Lambda calculus construct
and it looks like this.
And we're gonna go really slow
and we'll go through it together, right?
The first thing to notice is that y is just a function
that takes in some input parameter y
and it returns something, right?
So nothing too crazy.
What we can do is apply y,
so let's say we have this function r, right?
We wanna apply y to r, right?
So in order to apply y to r, what we need to do
is every occurrence of this yellow y inside this body,
we need to replace with our input parameter r, okay?
So all the yellow y's have now just become blue r's.
Sound good?
Okay, now if you look at this body,
we can actually reduce it further.
This first piece over here is a function
and the second piece over here is a value
that we can bind to this function
or if we can call this function
with this value on the right, okay?
So what we're gonna do is this is the value on the right
and if you look at this body, r, open print, x, x,
close print, every occurrence of this magenta x,
we're gonna replace with this body over here, okay?
So r, x, x has now been replaced with r,
this body, this body, okay?
We haven't done anything like too crazy
and now if you'll notice like this line over here
and the thing inside the parentheses of this r,
they're actually the same thing, you see it?
Here you have magenta values
and here you have purple values.
And the only difference is that this row
has like an enclosed r, do you guys see that?
Okay, so what we can do is take this yr
because these yr and this thing on the right
are equal to each other, so we can say
yr is r of yr and it's not readily clear
like why this is actually interesting or useful.
But if you kind of sit down and think about it,
what we've really done is define yr in terms of itself.
So we've created like a recursive definition right here.
And so what's actually happened is that
this y-combinator is this thing over here
allows you to take like a non-recursive concept
and create recursion from nothing.
So we just have variables, function definitions
and function application
and from those things we're able to create recursion.
So this is like a crazy construct to me.
Like we've created booleans
and therefore we've created conditionals
and from the same sort of raw axioms
we've created recursion.
Now, I encourage you guys to spend some time
if you're interested like really examining this
and coming to an understanding
of why it's actually interesting and profound.
But for now just take it on faith
that we're able to create recursion from nothing
and that's what the compelling aspect
of the y-combinator is.
Okay, the church Turing thesis.
So we have these two independent models of computation
the Turing machine and Lambda calculus
invented at exactly the same time.
And eventually these guys got together
and they realized that their models of computation
were actually equivalent.
So originally when church was defining Lambda calculus
he didn't define it in terms of Turing machines
and Turing when he was defining Turing machines
he didn't define it in terms of Lambda calculus
they were sort of separate axiom towers.
And so these guys got together and they said,
wait, we have two different models of computation
that we've proven independently to be sufficient
to be able to compute anything that's computable.
Does it true that our models are equivalent to each other?
Was the question.
And so they published this paper
called the church Turing thesis.
And it turns out that all Turing machines
can be rewritten as Lambda expressions
and all Lambda expressions can be rewritten
as Turing machines.
And we didn't really talk about Godel's recursive functions
but it turns out that those are also equivalent.
And so the conclusion here is that Lambda calculus
is Turing complete.
Without any notions of explicit recursion,
conditional state, et cetera.
So all we need is variables, functions
and function application.
So let's go into the peculiarities of Lambda calculus
because as software engineers we're sort of,
we can think of the Turing machine as this thing
that's very similar to a computer
and I'm gonna get to that in a moment
but it's not really clear what this Lambda calculus thing is
and how to do computation with it.
So the first idea is that there's no notion of global state.
There's no tape.
All you have is the input arguments to your functions.
That's the only semblance of state that you have.
The second idea is all functions are pure.
So purity is sort of this mathematical concept
which is to say that it's a math function
in that for any given input,
it always, a function always returns the same output.
So if you have a function, for example,
f of x equals x squared for an input three,
you call it this function with three,
it's always gonna return nine no matter what.
So all functions in Lambda calculus are pure.
All values are immutable.
So you can't modify an input parameter
but what you can do is generate a new value
from an existing one and there's also no loops.
So you can't really iterate on things
but the way we actually handle iteration
in Lambda calculus like structures
is through recursion.
And then functions are your unit of composition
and the way you compose functions
is sort of passing them as parameters to each other.
And because of the nature of Lambda calculus,
you don't have to reason about this like global state.
So when you're combining two simple functions together,
all you need to know is what the consuming function
does with the input.
You don't have to reason about side effects
or any other properties.
So my claim over here is that because there's no global state,
when you compose two things together,
you can be sure that that composition
is like really, really solid
and it's not gonna result in bugs.
Okay, the two towers.
So we have turning machines on one hand
and Lambda calculus on the other hand.
And I've not so subtly drawn this Lambda calculus tower
is perfect.
Well, first we need to make a brief aside.
In the 1940s, so less than a decade
after turning machines came out,
people started to ask the question of,
okay, wait, this is great as a mathematical construct,
but ultimately like I need to compute real stuff for my job.
And so can we actually build a physical machine
that does computation?
And one of the core people involved in this work
was this guy named John Von Neumann.
He was a computer scientist
and he proposed this model
for how we should build computing machines.
And what he started with was this concept of memory,
the RAM, and memory is basically just like a Turing tape
in that it's put up into these things, these cells
and the cells can contain values.
And then he proposed this thing called the CPU,
which is composed of two components, essentially,
a control unit and a logic unit.
And the CPU interacts with the memory
by reading stuff from it and writing stuff to it.
And Von Neumann proposed like a small set of instructions.
You can load a value X from the memory cell
at the location P.
You can store a value X into the memory cell location P.
You can add, subtract, and multiply.
And so here's sort of like a minor deviation
from Turing's model.
Turing had no notion of numbers or addition or so on.
And Turing, as a mathematician, just basically said,
those are levels of the axiom tower
that you can obviously derive for yourself.
Like, I don't need to embed those in my axioms.
But Von Neumann wanted to build something
that actually computed stuff,
so did the addition and so on.
And so rather than having to do addition manually
in the form of like incrementing
or marking and unmarking cells,
Von Neumann said, why don't we just build like circuitry
that does the addition of two numbers?
And embed that into the CPU.
So if I wanna take a value from cell A
and a value from cell B and add them together
and store them into cell C,
instead of manually doing that computation,
like incrementing and decrementing,
let's create circuitry that does the addition
so that it's faster, okay?
And that's what the logic unit
is essentially responsible for.
Then you also have these instructions
called branches or jumps.
So if the memory cell at location P contains zero,
go to N.
And if it doesn't contain zero,
go to N, it's another instruction.
And what I'm trying to get at is that
this looks very much like a Turing machine.
And Von Neumann proposed the actual physical circuitry
that could implement something like this.
And the first computers,
the very first general computer
was this thing called the ENIAC.
And I think it popped up in the 40s,
47 or something like that.
And basically it was like a room almost this size
and there was no notion of like a program
that you give to it.
All it had was like circuitry.
And you had these like engineers
that would go up and unplug and replug stuff
to program the ENIAC.
And then it would operate
and turn through the computation, right?
But ultimately it looked exactly like this.
It had some mechanism to store values in memory.
And then it had some mechanism
to read those values from memory,
combine them together in useful mathematical ways
and store the results back.
Cool, so the Turing machine tower.
Start off with Turing machines
and then we have this Von Neumann model.
And the compelling aspect of this
is sort of like a deviation from Turing machines
in that it can be actually physically implemented.
And one limitation here is that
you don't have an infinite tape,
you just have a finite amount of memory, right?
But if you embrace that constraint,
now all of a sudden you can actually compute things
instead of just leaving it up to a mathematician, okay?
In 1949, people got tired of manually plugging
and replugging in wires.
And they wanted like a human level way
to reason about what the instructions were.
So they gave each of these instructions names,
like small names like add, store, mold, divide,
things like that.
And programs were written like by hand first
in this sort of ways that humans could reason about.
And then later they were assembled
down to the actual programming of the computer,
like programming the instructions into the computer.
And so what we've done is created a higher level construct
called assembly that humans are able to reason about
more easily, which maps down to the Von Neumann model
in terms of actually programming the computer.
Does that make sense?
Ultimately, it's sort of like syntactic sugar
or addition in that assembly doesn't add
any more constructs.
Like all of the rules of assembly are defined
in terms of the Von Neumann axioms.
And then we have Fortran.
So Fortran is even higher level.
And here it adds constructs like if statements and loops.
And you can imagine in 1957, there really
wasn't anyone who had conceived of a general notion of loops
or even conditionals.
All we had were these branch instructions.
And maybe it was sort of implicitly defined
that you could make looping constructs from it.
But then people were like, wait, why don't we just
embrace this high level notion of a looping construct
and embed it in our language?
But just like assembly, looping doesn't actually
give you any more expressivity.
Every single loop can be defined in terms
of the lower level constructs.
Then we have C. C introduces these things called functions.
And then we have the ability to create more complex structures
of data called structs.
And then we have the ability to dynamically allocate
in free memory as opposed to just using,
you can imagine, kind of manually dealing with all
of the memory on your physical hardware as opposed
to some other memory manager.
And then finally, we have C++ in 1985.
Introduces this concept called classes and objects.
I'm not sure if these concepts on the right
were introduced solely by the languages.
Probably not.
They probably came in some other flavor.
But I think these languages over here
are the most significant in terms of widespread use.
So that's really what I'm trying to get at.
It's not as much attribution as it much as it is,
sort of relatively speaking, when these ideas popped up
into existence.
But just like pianos axiom towers,
where you have kind of irrational numbers
like up at the top, classes and objects
are really just defined in relation to von Neumann
instructions.
Everything boils down to those things.
So we can think of the von Neumann machine instructions
almost like the axioms for modern computing, really.
And so this claim is my own.
After studying the history of this,
I asked myself the question, why is this tower,
these languages specifically, so much
more popular compared to the corresponding languages
and ideas in the Lambda calculus tower?
And my belief is that the Turing machine axiom tower is
actually easily implementable in hardware,
because it's sort of like a physical device.
And because you can implement it in hardware,
you can actually compute stuff with it,
as opposed to it being relegated to pure math.
The final idea is that a compiler is just
something that takes like a higher level construct
and reduces it down to its axiomatic von Neumann
definition.
That's all what a compiler is.
Sound good?
OK, the Lambda calculus tower.
So this one looks very different,
because the first thing that we have
is just variables, functions, and function application.
And we've already kind of seen some constructs
that you can build on top of that.
But one of the most interesting ones
is this idea called Lisp, which came about in the 1950s.
And it came about also by a mathematician.
His name is John McCarthy.
And what McCarthy did was, if you look at piano's axioms,
the definitions of those axioms were sort of defined
in terms of English and mathematical notation.
McCarthy said, what if we could take Lambda calculus,
or structures like that, and define those axioms
in the language itself?
And he created this language called Lisp.
And basically, the implementation of Lisp
is in Lisp itself.
And because he was a mathematician,
he had no need to actually implement it on a real computer.
And so this was sort of the first example
of what we call a metacircular construct.
So the construct is defined in terms of itself,
and it's fully self-containing.
And I think a rite of passage for every single computer
scientist is to build your own Lisp interpreter.
So McCarthy kind of proposed this idea in 1958.
And then his students went along and actually implemented Lisp
on top of one Neumann machine to actually compute stuff.
The next idea is System F. So this popped up in 1972.
And you can think of System F as Lambda calculus,
except it has support for types.
So the Lambda calculus that I kind of showed you before
didn't really have any types, so it's kind of like JavaScript.
System F is kind of the typescript equivalent
of Lambda calculus.
But a lot more sophisticated for reasons
that I don't want to get into.
But really, every single System F construct
can be boiled down into its corresponding Lambda calculus
construct.
So very similar to how Fortran didn't add any expressivity,
System F didn't really add any expressivity either.
Then on top of this, we have these languages called ML.
I think ML stands for meta language,
and OCaml, which is the sort of most widely used flavor of ML.
And it introduced higher level constructs,
like pattern matching.
You guys haven't spent much time functional programming.
This whole tower is super weird, because these constructs
don't actually carry over cleanly to the imperative tower.
Sorry, the von Neumann tower, the Turing tower.
On top of this, we have this language called Haskell,
which the earliest roots of it popped up in 1985.
Basically, the same year that C++ came about
was when Haskell came about, or the predecessor to Haskell
came about.
And the cool thing about Haskell is
that it is a general purpose programming language that
can do I.O. and things like that.
But its constructs are still pure,
so it still has pure math functions everywhere.
And today, Haskell is sort of like the king
of statically typed functional programming languages.
But now we get into some stuff which
might be more relevant to your guys' experience.
In 2012, Elm kind of popped up.
And Elm is very much a functional programming language,
even though it compiles down into JavaScript.
And the Elm people essentially pioneered
the flux-redex pattern.
So this idea that actions result in essentially
the production of a new state, and that new state can
be used to render a new view.
And there's a sort of one-way data flow.
This idea popped up in 2012.
But if you're kind of thinking about the world in terms
of the lambda calculus tower, this idea
is actually not that novel.
It's sort of a very obvious outcome
of dealing with the constraints of the lambda tower.
Then from here, in 2013, we have React.
React kind of makes a claim that the view
needs to be a pure function of the state, or your props.
But really, it's kind of just the state.
Even a state, we can always render the same view
deterministically as a pure function.
And at first, if you're coming from jQuery,
adopting the React pattern was probably
extremely frustrating.
And for those of you who kind of recall back
to your first experiences with React,
you kind of felt like there was these artificial constraints
being imposed upon you.
Like, I just want to hide the modal.
Why can't I do that, right?
But then eventually, as you start
to build larger and larger apps, you
realize that this sort of one-way data flow constraint
makes it way more easy to reason about what your view is
going to look like given a state.
And the point I'm trying to make over here
is that, one, these ideas are not new.
Like, lambda calculus kind of forces
us to embody this perspective that the output of a function
is a pure outcome of its inputs.
And it's just now, in like 2012, 2013,
that we're starting to re-embrace these ideas.
And I guess most of us believe that I can't even
imagine building a UI in a non-reactive way.
It's sort of like taken as given.
And so I think that if more engineers spend time kind of
thinking about the history of this thing,
it becomes a lot more like, you can understand more
like why React looks the way it does
or why Elm looks the way it does.
And rather than trying to apply your like Turing machine
imperative programming mindset to functional programming,
you can kind of build your way up starting with lambda
calculus going up.
And I think that path actually makes it much more
easy to reason about functional programming.
As a fun side effect or fun anecdote,
the original compiler for React, when it was still
like an experimental project at Facebook,
was written in OCaml.
All right, so the final concession that I'll make
is that lambda calculus is really hard to implement
in hardware.
And whereas the Turing machine von Neumann model
is obviously very easy to implement in hardware.
OK, final slide.
React is to jQuery as lambda calculus
is to the Turing machine.
So in jQuery, you have this concept called the DOM
is your state.
So all of the HTML elements that are there is your state.
You've probably written jQuery code that looks like this.
jQuery.model.show and shows the modal.
And basically what I'm trying to get at here
is that whether the modal is being shown
is encoded in the DOM itself.
Anything can make modifications to the DOM.
And the DOM, as a result, ends up
in these weird, unexpected states
because you didn't precisely reason about state modifications,
kind of just wrote code like this over and over again
until it essentially resulted in a Turing machine-like construct
where it's difficult to reason about the tape.
And so in React, the state is explicitly defined.
It's an input to your render function kind of implicitly.
And your view is a pure function of the state.
And you don't modify the state, you produce a new state.
And so React's constraints actually
make it easier to reason about the state
and the DOM, and by analogy, functional programming's
constraints make it easier to reason about programs.
And so if you're intrigued and want
to learn more about the Lambda Tower,
I highly recommend taking this approach.
If you guys haven't done Advent of Code,
it's essentially this wonderful set of problems
that show up every December.
Solve those problems in Elm.
Elm is a really good intro to functional programming
because the compiler messages were meant for humans.
And the whole ecosystem is built so that it's easy to pick up
and learn.
And if you're familiar with the React-Redux pattern,
that kind of came from Elm.
And it becomes like, you can build some cool stuff
like right out at the get-go.
All right, that's all I got.
Thanks.
Thank you.
Yes?
It's tough for me to say it because I wasn't there.
But I think it's sort of like a chicken and egg situation,
because we didn't have machines that could compute stuff.
We didn't rely on those machines to compute the stuff.
But then all of a sudden, the machine to compute stuff
popped up.
And I got to imagine the first sets of calculations
were just silly, solving linear equations.
But then eventually, people started
to realize we could do compelling things with this.
I'm sure the military was one of the first users of it.
We can do ballistic missile trajectory calculations
very easily.
And then, obviously, computing is now universal.
Yeah.
Questions, questions.
Yeah?
Have you heard of ReasonML?
Have you heard of ReasonML?
OK, so ReasonML is a rewrite of the OCaml syntax
to make it more comfortable for JavaScript developers,
because the OCaml syntax is kind of stodgy
if you first look at it.
Whereas ReasonML, if you're coming from JavaScript,
it looks very similar.
But ReasonML is not a new language.
All it does is transpile down to OCaml.
And so if you want to get started with OCaml,
I wouldn't necessarily recommend it.
I would recommend starting with Elm first.
But then from there, if you want to build programs that
can interoperate with JavaScript really easily,
I think ReasonML is the best way to go.
Yeah.
Yeah.
Yeah.
But is it theoretically possible that OCaml
starts with OCaml?
Yeah.
There was a lot of researchers in the 1980s
that actually tried this.
I think they built some prototypes.
The problem is that you have these sort of positive feedback
loops in ecosystems.
And so the Turing model and the Von Neumann model
essentially caught on so well that even though, in theory,
the Lambda Tower might allow for more expressivity,
practically speaking, the best computers
are in the Turing model.
And therefore, more attention kind of
gravitates towards there.
People build more stuff for it.
And now, I don't know, 90% plus of all languages
are kind of all Turing-based.
So in the 80s, they did build functional programming
computers.
But because most of investment into these technologies
comes as a function of industry, like businesses,
like needing to solve business problems,
then the positive feedback loop of the Turing Tower
kind of diminished the Lambda Tower.
Yeah, so I would probably boil it down
to just the three constructs.
You have variables, you have functions,
and function applications.
So is there a way to represent a variable
in some sort of circuitry?
Is there a way to represent a function in circuitry
as well as function application?
I think the answer to all those is,
you're probably conceived of some way.
I don't know the details of how the actual Lambda computers
were built, but it might be an interesting thing
to look into.
But they obviously fizzled out.
The nature of the von Neumann model in Turing machines
is that it maps so cleanly to binary circuitry
and originally vacuum tubes, but now transistors.
Maps so perfectly down to that model,
whereas the concept of a function
doesn't map to logic gates cleanly.
All right, well, I guess with that we'll wrap up.
Thanks, everyone.
Thank you.

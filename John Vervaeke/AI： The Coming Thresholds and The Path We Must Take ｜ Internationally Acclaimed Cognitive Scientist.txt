Welcome everyone, this is not a voices with for vacay. This is a new entity. I'm calling a video essay
But under good advice from the two gentlemen that are joining me
It was a proposed to me and I accept the proposal that this should have a little bit more of a dialogical
structure to it and
Given the value of dialogue as I've been explaining it in other work. I took this deeply to heart
I am going to present still an essay and let's remember what Montaigne Montaigne meant by essay essay a to try
I am going to try
With the help of these two gentlemen to bring some clarity to the issue around
GPT machines the advent of what looks like the first sparks of
artificial general intelligence
I'm going to make some basic predictions and then I'm going to get into the
Scientific value of these machines and that will be both positive and negative
The philosophical value
The spiritual value and then my proposal given all of that argument and discussion about how we can best
Respond to undertake the alignment problem. So first of all, I'm going to ask the two gentlemen
two friends of mine two people that I have come to
appreciate
Love and rely on in increasing ways that has only made my life and my work better
And so well, let's begin with Ryan
Well, thanks, John
It's good for my heart to hear you say that because you have been such a wonderful influence in my life as a
YouTube student of yours and someone who has experienced a lot of transformation from your work
Which has led me to be the executive director of the revakie foundation where we are working to help to scale and bring about these
Solutions that we that you have pointed to so well in your work
And I also am the founder and run a technology services company called Mainstay Technologies
and so technology has been near and dear to my heart and story forever and
The intersection of the meaning crisis the metacrisis and technology that is AI
Has me very fascinated and been doing a lot of research on this and eager for what you are about to propose and argue here today
Thanks, Ryan Eric
Yes, my name is Eric Foster media director over here at the revakie foundation
much like Ryan knew you through before working alongside you and continue to
Not only learn from you but learn now alongside you as we we work together on all the different videos and these essays and the
conversations and everything
My interest in this primarily comes from and I actually I've told both of you this I asked my mom the other day
I said what was the first time you heard me like really like combat AI like what was the first time I started talking about it?
She said I think you're about eight years old and
For some reason it's been it's been a part of my life the entire way through
I'm now 30 so this is going on a long time of very like
Just just shallowly but continuously coming back to this idea for some reason it gripped me when I was very young
And I've been exploring all of the various different
Avenues that it could potentially take ever since
Thank you. So the format is going to be the following. I'm going to go through sort of an argument
Persection and then I'll open things up to take comments questions
From both Ryan and Eric. So the first thing I want to do is talk about the predictions
One of the things that are happening with the GPT machines is we're getting a swarm of predictions
And many people are finding this challenging because the predictions are quite varied
Many of them are inconsistent with each other or even challenge each other in a fundamental way
I'm going to try and propose
That we try to be more careful
About the predictions we
We try to steer ground between
Hyperbolic growth predictions that these machines are just going to hyperbolically
Accelerate in intelligence and that that forks into two variations
Utopia is just around the corner or we are doomed to doom doomed forever doomed
and so
Let's let's be a little bit more cautious. I'll give you some reasons for that in a minute
We also want to be steer between all of that
Both the positive and now negative hyper hyperbole and then a kind of stubborn skepticism
That's digging its heels in and saying no, this is not a GI
It never will be this is this is decades and decades away
And there are people making these arguments and I think that it's also incorrect
I think the attempt for whatever reason to dismiss these machines is not
Proportioning our
Evaluation to the reality that they are actually presenting to us so
What I hope is
That we can get much more careful and that getting more careful about the predictions we're making
Will also all will also in conjunction with the arguments and discussion. We're going to have allow us to
Allow us me me maybe specifically to propose some important threshold points that we have not yet met with these machines
But that we can reasonably foresee
Not perhaps their timing
But why they are pivot points and that these are points where we can make fundamental decisions
About how we want to go forward
especially in terms of the
spiritual challenge and the Enlightenment issue, so
Why do why am I skeptical?
Not about the machines
I'm critical but not skeptical and that important that distinction is going to be important throughout
I am skeptical of jumping to conclusions about hyperbolic growth
Since human beings are famous for jumping to conclusions when they see hyperbolic growth
And if you don't believe me just track the history of the stock market or something like that
And you'll can see that people can very often and get taken up by it
What we can say is most often
Hyperbolic growth is found within
Self-organizing processes and when hyperbolic growth is within self-organizing processes and the economy is a self-organizing process
It usually is part of a larger larger pattern called
Punctuated equilibrium you can see this also in the history of evolution. There'll be so after the asteroid hits
There's just exponential speciation in geological time
But time scales matter and we'll talk about that later and then it flattens off as the niches get filled as constraints emerge
It's more so we don't know yet if this like when people just draw these graphs. Look at what's happened over the last five weeks
Right. It's like
Yeah, and then you need to remember we got similar predictions about
Self-driving cars and the exponential growth and soon all these people would be put out of work and
That was like 2012
We're 11 years later because we hit a plateau. There was the exponential growth and we hit a plateau now
I could be wrong
But the reasonable thing is to be agnostic about the meaning of this very low resolution
measuring of exponential growth I
Mean here's another example consider if you were at the beginning of the 20th century and
Measuring all the breakthroughs in fundamental physics and you would see this exponential growth relativity quantum mechanical
And then it plateaus
It plateaus and it's been 50 and more years, but since we've had a significant breakthrough
We don't know
we don't know and
That is where we should properly
Stand about this. So we have to instead of making predictions that are not well warranted
We should try and foresee
Plausible threshold points
Not predict necessarily that their timing but foresee them and foresee them as our
Opportunities to steer this in a powerful way. This is the alternative. I'm proposing
See as you get into exponential growth things are often disclosed that you did not foresee within your normal framing
Let me give you one more analogy on this point
Traveling faster and faster
Traveling we can travel faster and faster and here's an exponential growth in our ability to to speed through the universe
Well, as you do micro particles become
Relevant in a way. They are never relevant for us in our daily movement, right?
There's a video online what what happens if a grain of sand
Hits the earth at the speed of light because force equals mass times acceleration
So it's accelerating to the speed of light and it hits the earth and you have this titanic explosion
This is why interstellar travel might actually be impossible for us
Even if we get machines that accelerate us towards the speed of light because things that weren't constraints can suddenly become
Constraints and of course the speed of light constraint is also there for all the fiction around faster than light travel
It's a real constraint
Again, this is meant as an analogy
We don't know what constraints will be revealed
We don't and simply looking at a simplistic graph is not taking that into consideration
We are genuinely ignorant because as I will make clear as we go through here
We really do not know how these machines are
Producing these emergent properties that they're producing and therefore
Trying to draw something like scientific predictions from ignorance of the underlying mechanisms is a seriously
Incautious thing to do so we have to pull back from that now. I'm not saying we shouldn't try and foresee but notice my shift in language
I'm shifting from prediction at this date. This will happen to foreseeing. What's the foresight?
And the foresight is can we foresee as
We bring real explication to these can we foresee threshold points where we can reasonably
Make a change. I think we are in a
Kairos we are in a pivotal turning point in world history
Maybe one of the greatest maybe the greatest. I don't know I
Would need to be godlike to be able to make that pronouncement, but
Unfortunately, I'm not which is something. I'm also going to talk about later. I don't want to be a god. I hope you don't either
Imagine having a godlike ability to remember for all of eternity all your failures. I don't think that's a
Existence to be to be desired. So I
Think we need to be really careful. I need I think we need to pay attention to what we've seen in the history of science
The past century has not been the century of unlimited growth and knowledge in some ways
Yes, more and more data more and more information, but you can be misled by just a
Quantitative approach because if you pay attention to what's been happening at a philosophical
epistemological having to do with the study of knowledge level what you've seen is this has been the century of the
accelerating discovery of intrinsic limits on what we can know
The realization that the Cartesian project of unlimited
Knowledge is not actually a possibility
It is reasonable to conclude
Again, I'm not speaking timing here. I'm talking about trajectory, but it's reasonable to conclude that this trajectory will continue and show itself in this project as well
We are going to perhaps start to discover the kinds of
fundamental limits on mind and its interaction
With matter that were not previously available to us and that will be welcome
But I think it's unlikely that the machines will just in some
simple exponential pattern grow
One of the reasons I think this is because
There's an issue of what's called general system collapse. This comes out of general systems theory and
The place where we have evidence for this is in
In
Civilizations which represent very complex intelligence within
Sophisticated distributed cognition that's intergenerational in nature. So this is very powerful cognition at work
I mean, and if you stop and think about it, the GPT machines are basically just taking that just
collective intelligence from distributed cognition and putting it into a single
automated interface for us so
If you think the GPT machines are very intelligent, you should think that civilizations are equally that kind of superhuman
Intelligent that's a reasonable thing to conclude. What do we know from the history of these civilizations? They face general system collapse
why
Let's take it that reality is inexhaustibly complex not just complicated but complex
right and it contains real uncertainty not just risk real emergence and
By the way, when you have real emergence, you have real uncertainty
Not just risk and all of these people are invoking real emergence when they're talking about these machines
So real emergence means real uncertainty. It means real novelty. Okay. Now, so you place
Superhuman civilization intelligence into a complex environment. What do you see the system doing becoming more and more complicated?
adding at more and more components
Bureaucratizing itself in order to deal with problems
But what you get to is you get this this sort of fact
As you linearly increase the number of problems you're trying to deal with the number of interactions within your system is going up exponentially
So at some point
Managing yourself becomes as problematic as any problem
You're trying to solve in the environment and then the system the system gets as it said top heavy
It gets overbureaucratized and it collapses now. This is a regular pattern
Regular reliable pattern for the superhuman intelligence that we find in civilizations. I
Do not think it's reasonable to conclude that these machines will somehow just avoid that problem of
you know exponential growth in
Complicatedness as they try to deal with
Real problems that contain real uncertainty that an inexhaustible environment is presenting to them
Now that doesn't mean I can say oh well, they're never gonna surpass us John
Revekey is not saying that John Revekey is well aware already of things that can surpass him and like I said
It's very clear that we have been relying on the superintelligence of distributed cognition that is distributed
cross people and across generations
for millennia as the as sort of
An important source of normative guidance to us
So these machines, it's not unreasonable that they could reach that level and maybe and we'll talk about this later having a
Different substrate the the material they're built on may allow them to go to different levels
I do not think though that they can just
Grow exponentially indefinitely. I think that is we have no good reason
And I'm trying to make arguments here
For believing that and that means we can think about these machines
However, Godlike they might be as being inherently still finite in a matter in a manner that really matters
To their cognition and their attempts to make sense of themselves and their world and that's going to be an important linchpin later on in my argument
What's interesting is there's some evidence that we are very close to
All the trade-off points
At least for biological beings
For example, there's all these you curves for like if speed of transmission if you speed up the speed of neuronal transmission
We're at sort of the maximal
Sort of the optimal because if you go too far you get into diminishing returns and the negative side effects start to
Manifest faster than the gains and also for more neurons and and so I've seen some really good arguments that we're sort of peak
biology and
That's very interesting if you think about it. That's might be of course
Why we resorted to culture because culture allows us to supersede the limits of peak biology
we
teeter on the edge of despair and madness and as these machines approach their own
threshold
They will plausibly also
teeter on the edge of that and
That is something we need to think about and I'll give you more precise reasons as as to why
That is going to become important. It has to do with the increase will have to increasingly
These machines will have to increasingly rely on more and more pervasive disruptive strategies
And so we'll come back to that
All right, one more thing that I'm going to
Say about this is
And this so I'll go into this more detail the rationality
Of course, we have to make the and this is where the general system things really starts to bite
These machines have to become more self-monitoring and self-directing
In very powerful ways
And then the problem with that is if you make this system as powerful as this system, then you get into an infinite regress
One thing you don't want is a large language model
right
Making all the hallucinations and repetitive actions and weirdness trying to evaluate a lower order
LLM that is making all kinds of hallucinations
Repeated it because then you just get an infinite regress
So you have to properly have this the heuristics operating at this level to be different in kind
Than this level and that also gets you into a diminishing return
Issue
Because at some point, you know, you don't want this to become as complex as this and think about it already. I
Mean these machines have hundreds of billions of parameters
You know what it's like to try and track a hundred billion parameter system
You know how that you know one of the things that this machine that probably has more than a hundred billion parameters
Can't do very well is track a hundred billion parameters
Right and so just thinking that though. Oh, we could just stack these on top of each other
I think it's also overly simplistic. We're facing. There's real trade-off relations
There's real problems there and again that means that these machines are going to be
finite in a very
Important way and they will confront presumably the issues around finitude that will be analogous to ones we have now
I
Want to stop here. Not I'm not finished this section, but I want to make clear
All these gaps in the g gpt machines do not take them
I'm not offering them as any grounds for dismissive skepticism
I'm confident that we can approach these limits and that we and and will continue to make progress
And I'll point some of the things that are already happening. That's not why I'm doing this
What I'm doing this for is I
Want to show
That these machines are not yet
Full the intelligence. Nobody really thinks that they talk about sparks in the beginnings, but I want to unpack that common claim
It's unlikely that they're currently conscious and what that means is we face thresholds about
Qualitatively improving not just quantitatively qualitatively improving their intelligence possibly making them
self-conscious
Rationally reflective etc. And that's what I'm most interested in. What are the threshold points we can get to?
How can we make them plausibly so if we just give up? Oh, right? And no, no, no, there's gonna be trade-offs
There's gonna be limitations. There's gonna be, you know, all kinds of stuff and then from that
We can pick off. Okay. Here are plausible threshold points and then we can more finely tune our
Response to the alignment issue. That's why I'm doing this. I am very impressed by these machines
I think it is very reasonable to conclude. They are going to
Significantly alter. I said it. They're a kairos. They're gonna significantly alter our
Society and our sense of self. They are going to they are gonna pour
in, you know
meth and
You know fuel on the fire of the meaning crisis
And that is something I think we need to take into account
That will tempt us to respond
Inappropriately to what these machines are presenting to us and that leads me to some final sort of
Societal predictions I
Think there's gonna be multiple social responses and as I said, I'm worried about the accelerant of the meaning crisis
tempting us towards inappropriate ones
So one is
Nostalgia
people longing for the time before the machines longing passionately and
Deeply for the golden age that human you did not realize that ten years ago
You were in the golden age, but ten years from now. You'll be hearing you were in the golden age
That wonderful time when there wasn't GPT or whatever AGI takes its place
nostalgia will grow
Alongside of it though will be resentment and rage as people are disenfranchised
So Louis the 14th what what just hang on Louis the 14th
When he grew up as a young kid the nobility
Staged a coup and he remembered that and he vowed that when he became king. He would crush the nobility and
Become an absolute monarch an absolute king the Sun King lady. Say more. I am the state. That's Louis the 14th
In crushing the nobility he disenfranchised a whole
segment of the population
That had traditions of power
traditions of decision
Was highly intelligent because they generally ate better
Highly educated because they had access to education and they were disenfranchised
That's a bad idea
Because that is the basis for the beginning of revolution
So I'm saying that now to the people who hold power
I'm talking to all of you right now you who think ah ha ha
Yes, 95% of the people are going to be driven, but I will become a Sun King. I will
Be careful. You are lighting the fires of a revolution in a kairos time and thinking that you will be protected from those flames
I think is foolishness
What else is going to happen I
Think that combination of must nostalgia resentment and rage will have
multiple
Religious consequences one and religion is going to figure in a lot of what I'm talking about today
One of those is what you get when you mix
Nostalgia with resentment and rage you get fundamentalism
Fundamentalisms are going to rise and they're going to be increasingly
apocalyptic fundamentalisms and
Fundamentalism and apocalypse go so nicely together. They really oh apocalypse fundamentalism. Oh, I love you
I love you too, right? That's what's going to happen
And so we have to and think about how that can shade off into a kind of escapism
I don't have to worry about this God will come. This is just the antichrist etc
Now I'm going to say one thing to my Christian friends and I want you to take this really seriously for those of you who who
Believe in that. I
Hope you're right. I
Really honestly do
But I want you to consider the fact that there have been multiple times kairos is where God has been silent. I
Suspect that is very possible now
Okay
Now the thing that's going to happen is cargo cult worship of this AI there's already I forget I'm sorry
I need an apology. I forget the author of this article. I read
He didn't specifically make this argument, but it overlaps and and he has definite providence and precedent. I just forgot his name
I'm sorry, but he's got an article about how people will probably start worshiping these AIs
And I think that's the case. Well, we'll have what a cult will have a cargo cult around these emerging AIs
What's a cargo cult? So during the Second World War the Americans flew in
To the islands in the Pacific all kinds of cargo all these goods that the
That the indigenous people found wonderful and amazing and and this stuff is just landing from the sky
And then the war was over and the and the Americans left and
The indigenous people started building out of wood what looked like airplanes in building runways
because they were trying to get the
Miraculous airplanes to return and dispense their wonderful cargo. So I mean cargo cults around the cargo that these AIs can dispense to us
I think that's a very reasonable
Possibility and I think that is also a very dangerous path to go down because that will actually distract us from
the hard work that we need to do in order to
Properly address the alignment problem
There's going to be a lot of spiritual bypassing
Which is I'm spiritual it doesn't matter right a lot of escapism drugs pornography
Etc. Tragic disillusionment
Tragic disillusionment and that's going to exacerbate the meaning crisis
Then one more thing and there's a fork here and this is around identity politics
Left and right I'm not taking a side here. I'm talking about the whole framework
I think it's identity politics one fork will be identity politics is swept away by the greatest threat to human self identity that has ever existed
And it's happening right now and
All of the differences that we have been promoting as are going to pale by the fact that we need to get together and get our
Shit together if we're going to really address the real threat to what human identity really means
That's one fork. The other fork is people will double down
Double down as we adopt a fundamentalism about identity politics. I predict that that will be
Incapable of giving us any significant guidance about how weak we construe human identity and our self-understanding in the face
Of the advent of AGI
Okay, so that's the first section. I want to open myself up to
Reflections comments challenges questions, etc
well
I appreciate John how you
immediately
Sort of helped us break frame of this is not just a technology that's gonna have normal adoption strategies
Yeah, like this is something fundamentally other this is something that is going to have massive disruptive strategies
Well, also moving it off of the the quasi religious grounds that I hear this talked about so often where the singularity is this has this mythical
Power to it that's calling that we will suddenly be able to transcend the laws of physics and know the answer to everything and it will answer
Whether God is real and as if that we can reach some point where all of that happens and you're clearly setting
finitude around this well
Encouraging us to really wrestle with the rapid acceleration that we're facing
Yes, I think that is very well said. I think the danger and
In it the sort of the market and the state are doing exactly the framing that you're pointing to well
This is a technology and this is how we better figure out how to use it better and all that sort of thing
No, no, no, no, no that that I mean in one sense. It's a technology
but
But that is to that is to emphasize the wrong aspect of the these entities in a fundamental way
Yeah, I agree about that
Fundament yeah, I think deeply what that's one one one point
I want to get across very very clearly. There's a sense in which
Even that technological framing is something we're going to have to challenge more comprehensively about ourselves and our relationship to the world
Eric you wanted to say something
The thing that stands out to me right away, and I'm kind of trying to
Because it's natural but also because I think it's necessary. I'm trying to take the
Perspective of whatever audience it is that is going to be listening to this
And I really appreciate just the the framework that you're putting around this whole conversation because I think that there's
So much doom and gloom currently already. There's so much. Oh AGI
It's gonna happen eventually the current AI chatbots that we have these language models. They're so useful
They're gonna become you know our new God
The way that I make a thousand dollars a minute, so you know suddenly all of a sudden
And I think that there's going to be a continued need and I think in this in this instance in particular
There's an even greater need to force nuanced conversation
Yes, these things and so I'm so glad
Personally just me as a human being I'm so glad that that you're putting this much thought into it in all of the different
Areas that you are and combining multiple domains to not only reach multiple people but also to show how big of a potential
Well, just how world-changing that this can potentially be not in a doom and gloom or a you know cargo cult
Sort of way and I think that as as we continue to as these technologies continue to grow the need for that nuance will grow
More and more strongly
So I'm happy personally to see you in my opinion leading this nuance
Well, thank you for that
So before I go into the scientific value of the GPT machines, I want to just set a historical
Context and this will I want people to hold this in the back of their mind also for the philosophical and spiritual
Import of these machines. What's the historical context?
So I'm gonna use the word enlightenment not in the Buddhist sense. I will use it in the Buddhist sense later
I'm using it in the historical sense of the period
Around the scientific revolution the reformation all of that the enlightenment and that the
Degeneration of secular modernity and all of that
That era is now coming to an end
See that era
Was premised on some fundamental presuppositions
That drove it and empowered it and these is not this is not my point
There's a point that many people have made this sort of Promethean proposal that we are the authors and tealaws of history
And
that's passing away and
It's done something really odd like wait, we did all this made all this progress to come to a place where we will
Technology wouldn't make us into gods. It will make us the servants or
Make us destroyed by the emerging gods what what aren't we the authors of us?
Aren't we the tea? Isn't this all about human freedom?
In fact, I think it's not it just ending there's a sense in which there is for me. I
Don't know how many people share this so it's an open
Invitation there's a sense of betrayal here. I
Mean one of the enlightenment one of the things the enlightenment did was to tell us to stop being tutored and educated by religion
It said religion is a private matter go there do your thing
But the way you should be educated brought up become a citizen blah blah blah
free from religion and of course there's been all kinds of benefits for that but notice the irony here is that
One of the things religion taught us how to do or what is to how to enter into relationship to beings that were greater than us. I
Mean Plato
Didn't have any problem being Plato when he believed that there were gods that were like
Socrates clearly thought his wisdom was a paltry thing in comparison to the gods. Those people knew how to live
With beings that were super intelligent and how to nevertheless
Craft lie human lives of deep meaning within it. We should pay attention to that example as and I'll come back to it later
But in losing religion we lost the place where we learned the manners of dealing with that which transcends us the manners and the virtues
And that's really odd because the enlightenment has also denuded us of the traditions that might give us some initial guidance on how we could
Think about relating to these machines
So I think the time of the the enlightenment and modernity is coming to an end. There were already signs of that
post-modernity and other things have been already showing that
But I think this is going to be even more of a severance for us from that
And that's very important. So please keep that context in mind
All right, the scientific value of the GPT machines, this is going to be and when I say value
I mean both positive and negative. I'm using value as as a
As an unmasked an unmarked term
So positive value
Scientific value. I think this is the beginning of a real solution to the silo problem
And that's a scientific advance. What's a silo problem?
The silo problem is that our deep learning machines our neural networks have typically been single domain single problem solvers
This machine is really good at playing go. Can it swim? No
Right, right, and we were very different from them and that's an important difference
Because I think AGI doesn't make any sense unless it's in a general right in a general problem solver
They can solve many problems in many domains and what's being opened up by the machines is the real possibility of this
They have salt. I'm not saying it's a complete solution
But we have clear evidence that the silo problem is being solved. Now. What's really interesting about that?
And this is an argument to myself and other people of making is we're basically saying that we need hybrid machines
We need sort of neural networks doing the deep learning and then we need something that's very language of thought. This is what these large language
Models are and they and we're doing even more if the huggings things comes through
We we've got this kind of AI and this kind of a and we're clutching them all together
And it turns out that there's that's that's a confirmation of a lot of prediction
In argumentation some of I that I made that there are there are different strengths and weaknesses between language like processing and non-language like
Processing this is a theme of my work and that these machines are showing we actually have to address both
if we want to create general intelligence, and I think that's a big
Admission for
multiple kinds of knowing I
Think that's I think that's
Taking it that way. I think is a very reasonable conclusion to draw
Now what these machines do demonstrate is the insufficiency of purely propositional knowing and
For those of you who don't know the kinds of knowing I'm going to talk about I can't go into it in great detail
We're going to put some links to some of the things here so you can go to it in detail
but I
Want to give you one clear example of this that I don't think is in any way controversial so
You can ask GPT for to spit out ethical theory for you. What's utilitarianism? What's the ontological ethics?
Can you make an argument against singers argument for utilitarianism? Oh, here's a counter argument very good very good
All the propositional expertise that Leibniz had wet dreams about oh
And you know what that doesn't make these machines one iota moral agents
Think about that think about what we now have evidence for right
This shows the radical
insufficiency of
Propositional knowing for personhood if we take it that a proper part of being a person is moral agency
Lacking that
So
One the way of thinking about this that may be helpful is a distinction
From in psychology distinction that I'm actually critical about but it's helpful
There's a distinction between people who when people do work on intelligence is a distinction between crystallized intelligence and fluid intelligence
So crystallized intelligence is you're knowing how to use your knowledge and this can have
highly powerful and
Emergent properties because you can connect things that you know in new and powerful ways and I think it's abundantly clear that
There's a lot of emergence
crystal intelligence
crystallized intelligence in this machine
It probably doesn't have what's called fluid intelligence
His fluid intelligence has to do a lot with attention
working memory
consciousness and
Your ability to sort of dynamically couple well to your environment
It's I think I'm gonna argue this a little bit more detail
But I think it's unreasonable to conclude right now that these machines have perspectival knowing
Now let's be careful about this can they generate all kinds of propositional theory about perspectives you bet
They have crystallized intelligence, but keep the analogy to moral reasoning
That's not the same thing as being able to take up a perspective have genuine salience
Landscaping and bind yourself to it in a very powerful way, and I'll come back for why I think the machines currently
Lack that remember the word currently
Okay
Another thing that has been a significant scientific benefit and
Because I'm talking about science
I'm sort of saying predictions that I made because that's what scientists are supposed to do it sounds self-promotional
And to some degree perhaps it is perhaps I'm within within all the nestings of my thinking John
Vervecki is saying but I'll remain important even when these machines are going to maybe that's happening
I hope not so I hope the argument stands on its own
this is
Clear evidence and I have argued for this that generating intelligence
Does not guarantee that you will generate rationality in fact what is very possible is you will as you increase
Intelligence you will increase its capacity
For self deception and we're seeing this in this machine these machines in space the hallucinations the confabulation
The lying and not caring that it lies
Right all of this and and noted there's been a couple people that have pointed out as they've tried to put in safeguards to limit the hallucinations the speed
Has actually the machine has actually slowed down compared to 3.5
Hinting that there's some of those trade-offs might actually already be coming in place don't know but these are empirical questions
so let's pay attention to the empirical evidence as it unrolls and
Try to calibrate what we're saying as closely as we can
But it's very clear that these machines just by making them more intelligent will not make them more rational and
We had every good reason to believe that because that's the case for us
There is no contradiction in human beings being
Human being being highly intelligent and highly irrational
The predictive relationship between our best measures of intelligence and her best measures of rationality is point three now point three is not nothing
But it's not one
It's not one seventy percent of the variance is outside of intelligence
And so we're seeing that in these machines now why are you saying that because that's gonna be the basis for
My philosophical argument, which is about rationality
rationality
All right now a question that a lot of you have sort of posed to me is but what does this say about relevance realization again?
I'm not gonna repeat
Everything I've said about relevance realization
We'll put links to publications. We'll put links to videos. This is there the basic idea here is the general ability of general intelligence is the ability to
Zero in on relevant information ignore irrelevant information and doing do that in an evolving self-correcting manner
And like I say, I'm not gonna try and justify that claim right now. I have a lot out there
and I
Invite people to take a look at it
What is this what are these machines say about that theory well
One thing we know and and look at the 2012 paper with Tim will the crop and Blake Richards
we pointed out
something that was emerging in deep learning about relevance realization and
Therefore there is an important dimension of relevance realization
Recursive relevance realization that is being massively
Instantiated in these machines, which is the compression
Particularization function of deep learning and doing it in this multiply recursive fashion and so of course that dimension a
Relevance realization is going to be important and because of its recursivity. We're going to see it have emergent aspects to it
So I think this is actually a significant
confirmation
More interestingly
Last at the end of last year with Brett Anderson and Mark Miller
published a paper talking about the integration of relevance realization and predictive processing as
The best way to get intelligence now interestingly enough
These machines show that because they have deep learning running that dimension of relevance realization and then they have predictive process
That's what the LLM models are. They are predictive processing machines. That's exactly what they are now the problem is
That they're limited. They are predicting the relationship between lexical items and
Broadening that is probably going to be a challenge
So although there is an integration of a dimension of our are and a specific version of PP
With that predicts very powerful intelligence. There's also inherent limitations still
What are those?
There's a lot of dimensions of relevance realization that are outside the compression particularization
Processing that it's our deep learning that are probably also
Going to be needed for genuine intelligence. There's explore exploit trade-offs
There are trade-offs between monitoring your cognition and tasking in the world
I won't go through all of these but the eye and they're always trade-off
Relationships right you're always trading between them because as you advance one you lose on the other and you're always pulling between them
There's in deep trade-offs between trying to make your processes more efficient and make them more resilient
So they have a kind of adaptive responsiveness to the environment. All of this is still not in these machines
Does that mean we can't put it in the machines? No
Unfortunately that work is already out there and published so that possibility is there
However, I think most no I don't want to put a quantitative word some significant dimensions of relevance realization are missing and
Having a generalized form of predictive processing is genuinely missing. So that's my reason for saying that
There's not a lot of that in at this
Point that's new about
Theoretically new this is not a scientific advance
The these ideas were largely already pre-existent and in the literature
Now again, that doesn't mean anything. Oh, well, then we're not going to pay attention to that. That's not what I'm saying
But let's push on this point I
Want you to notice how much these machines?
presuppose
relevance realization
Rather than
Explaining it. What do you mean John? Well, these machines rely on the fact that we have encoded into the statistical probability
between terms
Epistemic relations of relevance. We don't generate text like randomly
We we've what have we figured out with literacy and previously with languages wait
You know these epistemic relations of relevance between my ideas
I can encode them in probabilistic relationships between terms and and then we can get really good at it
first we have hundreds of thousands of years of evolving language and then we have all of this
Civilizational level work on literacy to get that correlation between
relevance
But let's call it epistemic relevance and merely statistical relevance to get that correlation really tight
That's different than a chimpanzee moving around in a forest
Really different
right, so first
We it presupposes it relies upon us
Encoding that relevance realization
Secondly, it relies on us encoding relevance realization and how we curate and create databases
How we create labeled images for the new visual processing that's coming on
Right and how we how we organize access to that knowledge on the internet
The internet is not random. It's organized as a
Multi-layered small world network because it's organized by human attention and what human beings find relevant and salient
And then finally and don't put too much on this but also don't ignore it
The reinforcement learning that is driving this is modified. It's human assisted
Human beings are in the loop making important judgments that help fine-tune and a lot is smuggled under fine-tuning
Right that judgments of relevance of this machine now. What does that mean?
You say so what John the machines can still do that. I'm not denying the technological success
What I'm saying is the degree to it
degree to which
They are
Presupposing relevance realization is the degree to which they are not explanatory of it
This is not an explanation of intelligence
This is not
It won't generalize as I mentioned this the what's happening in this machine doesn't generalize to the chimpanzee
at all
It's it's it might odd even
Generalize to us
Why why do I say that because it's weird like they're there is it Stuart Russell recently released a thing about
You know several generations beyond AlphaGo. It's not GPT, but it's the same deep learning process
So, you know, you had AlphaGo that could beat any human go master and then
Generations beyond that so like levels above and then the human beings noticed
They just noticed noticed
perspective all right, they noticed
That there's pretty clear evidence the machine didn't have a the concept of a group
Like this basic idea of a group of stones and then they said, okay, that's the case
Here's a very simple strategy you could use to beat any of these machines
They took a middle middle range go player
Gave them the strategy gave the machine a nine stone advantage and the human being regularly and reliably beat the go machine
Now what some of you are saying oh, well, we'll figure out how to fix it. Yes, you will I don't doubt that
But that's not the point. The point is the machine didn't do that self-correction
There's stop. There's a lot missing other weirdness like the oh the visual recognition. Well, there's been a problem
We get these visual recognition machines and oh, well, that's a dog. That's an elephant
That's a man sitting on a picnic table. Wow. Wow. That's whoa human level
Okay
Now what I'm gonna do is take the same picture and scatter in some insignificant perceptual noise human beings won't even notice it
alter a fraction of the pixels
Then the machine you show at the picture of the picnic table and the man and they say oh
That's an iceberg
You get this weird freaky thing
That comes out of that
Another thing
This is one of the most strongest results reliable results about human intelligence. It forms a positive manifold
This is what spearmen discovered. This is how we came up with the idea of general intelligence
namely how you do on
This task how you do in art contrary to what people is highly predictive of how you'll do in math and how you'll do in history
Like that's what he found how you do on any of these different tasks is highly predictive of each other
You form a positive manifold that that's your general intelligence that points to a central underlying ability
I happen to argue that it's relevance realization, but putting that aside
Notice this so GPT-4 can score in the 10th percentile of the Harvard Law exam
Whoa, that's really high IQ
But then and I didn't do this somebody gave my most recent talk at the Consilience conference to chip to GPT-4 and asked it to
summarize and evaluate and
and
And then I also gave this to an academic colleague of mine to evaluate GPT-4's response
So I made sure it wasn't just me and it's about grade 11
As an answer and it's like what why is it brilliant for human beings?
There would be a very strong positive manifold between those, but there's a lot of heterogeneity in this machine. So I
don't so
Clearly doesn't generalize to the chimp
It may not generalize to us
Which means it suffers from the kind of failure that destroys any good scientific theory, which is it fails to generalize
I don't think a good scientific theory is available to us because of this now you may say but what about I'm gonna come to the
Philosophical and spiritual significance right now
We're playing the science domain, and I'm trying to answer the science questions
We need to get clear about these and not mix them up together and confound them and run back and forth in an equivocal manner
Let's be clear about each one and then put them back together very very carefully very very carefully
Because I think and I won't go into detail for this explanation because I think these machines don't really have
recursive relevance realization
In a deep enough way and because I think there's a lot of converging arguments that the
function of the fourfold of attention and working memory and fluid intelligence and consciousness is
Relevance realization in ill-defined novel complex situations. I think it's highly unlikely that these machines have consciousness
I think the fact that they have no reflective abilities
Means it's virtually the case that they do not have self-consciousness
So worrying about you know how the machine is thinking like that is like how the machine is feeling or something like that
I think that is
premature
Is it possible that we could get there? Yes, it is
It is but you see what's happening by getting clear about what has happened scientifically
We can start to see what are the future threshold points that we will be confronting and how can we be foresightful about them?
Okay
one more thing and
People are endlessly arguing about the Chinese room and since they were endlessly arguing about the Chinese room argument
I won't go into it. And if you don't know about it, don't worry about it before
chat BT before the GPT machines. I don't think this argument's gonna
Satisfy anybody I I
Said take a look at the best attempts to give a naturalistic account of semantic information and we need a scientific distinction here
There's a distinction between technical information, which is what is involved in what's called information theory
And all that basically is is a relation of statistical relevance that rules out alternatives
That's the Shannon and Weaver notion
So in that sense without there being any sentient beings there is a ton of information in this table
because there are statistical relevance relations that are ruling out counterfactuals all over so
Right semantic information is what we normally mean by information. It means something is meaningful to it. We understand it
We form ideas about it
Now I think it's fair to say that most people in the business take this to be a real and important difference
Shannon certainly did when he proposed the theory
So let's take this difference as a plausible difference and then we can say well
Do these machines?
Have semantic information and one of the best papers out there right now on this is
By Kolchinsky and Woolport. It's entitled semantic information
Notice what's in the title autonomous agency and non equilibrium statistic physics from 2018. We'll put the link in here
What do they argue they argue that technical information becomes semantic information when that technical information is
Causally necessary for the system to maintain its own existence. That's why they put autonomy and agency in the title
What they are basically saying here is that meaning is meaning to an auto poetic system a system that is making itself
Now this converges completely with the argument I've made that relevance nothing is intrinsically relevant
things are relevant to
Something that cares about this information rather than that information
Why would it care about this information rather than that information because it's taking care of itself because it's making itself
Because it's an auto poet. It's an auto an autonomous auto poetic agent
And to the degree to which these machines
You know aren't auto poetic is the degree to which they really do not have needs. They really cannot care
They can pantomime
Maybe to perfection are caring and you can do so much with a pantomime
But pantomime caring isn't really caring. It is ultimately pretentious
So
Meaning is about this
Connectedness I like to use the Latin term religio to the environment
And you only get that if you're an auto poetic
Autonomous agent and of course these machines are not now just to foreshadow that tells us wait. Here's a threshold point
Do we make these machines? Do we embody them in auto poetic?
Systems. Oh, we'll never be able to do that. You're wrong
You are wrong. We've already got
Right, we've got biochemical RAM memory. I think IBM announced that recently
Not biochemical but electrochemical. I think or like I forget it's there find it. I
Misremembering I work with people that are working on
Artificial auto poesis and how to get primitive cognition into that. This is already happening and it's accelerating
This is not some oh well
We can not we can John told me I don't have to worry because these machines don't really have intelligence and consciousness
Unless they are auto poetic autonomous agents John didn't say that
He's he's right
John said the second thing. He didn't say the first thing
The work to make that is happening and making significant progress
And so there's the threshold point
Do we make these machines? Do we embody them?
In auto poetic systems and here's the challenge facing us
We may not decide this for moral reasons. We may decide this because we want sex robots
We the pornography industry which led
The internet development in powerful ways may drive us into this in a stupid
ultimately self-destructive
fashion
We have to be foresightful and say I'm not going to leave it to the pornographers to cross this threshold
Push for the crossing of this threshold
We have to make this decision in a rationally reflective
manner after good discussion
So that's the end of my presentation on the scientific import we can take some questions now
Yeah, John. Well, first, I think you can add the military to the push for the embodiment. Yeah as well
Yeah, I don't mean there to be only one. I wanted to give you an example. Yeah, totally
I'm curious. So from third generation cogsci thinking of the four e's plus two e's
To move the move from the fractional intelligence that we see now if that's a fair way to frame it
That is a type of intelligence a rudimentary kind of intelligence into something fully
auto poetic
embodied and
And with you know relevance realization
How many of the e's are necessary?
Well, you know, uh, uh, Kolchinski and Wolper explicitly said that, um
Their theory works in terms of there's a quote the intrinsic dynamics of a system coupled to its environment
So at least the I think I think all six e's are really necessary to get fluid intelligence
That is something above and beyond crystallized intelligence
Um, but do I think that those I mean, I saw David charmers talking about this
He says I'm a big fan of extended mind. He should be he wrote the article that got it going
One of the one of the four e's but he said there's no reason in principle
By why we can't give the make these machines participate in extended mind
And I think he's right. There's no reason in principle. It's not
But again, no reason in principle
I you're asking me to speculate my speculation given what I know about dynamical coupling and about relevance
Realization and about predictive processing. I think
Probably all all six e's are are are are necessary
I hesitate because that that allowed me allow that to please be a preliminary
speculation
Because as I keep saying we have we are ignorant of important empirical information that is still forthcoming
And so I might want to modify that
When I see some of that empirical information that we don't yet have one of the most constant refrains from these peoples
We don't know how it's working. It's virtually a black box
And and the and the invocation of emergent properties, which I'll come back to is just is just stunning
But yeah, so my best
Little better than a guess
My best conjecture is all of the e's will be needed
and you don't see any
From from your perspective, there isn't any reason that those thresholds can't be crossed that that they can't begin to embed
emotion and
Teach it to exact that its own learning like those will be they're just thresholds or hard problems
So we have not yet crossed
Which is where we're part of we'll see this exponential growth and then a threshold and a pause
But you are not saying that's going to make agi
out of the realm of
Fairly near-term possibility. No. No, I think I think if autopoiesis gives the system the general ability to care about the meaning of things
I think emotionality is going to be very
Wrapped up with that one of the e's. I think when we talk about its capacities for self-transcendence
It will get the ability to exact it doesn't do that right now
But it there's no reason in principle why it can't and then the embodied embedded enacted and extended all of these like I said
the the artificial autopoiesis
10 years down the road
I think
And therefore all of this is maximally. I think I mean again big grain of epistemic salt, but
It seems very plausible right now that within within 10 years
These two lines can will converge if we wish them to and of course like I said and you said there's going to be pressure from the military
there's going to be pressure from the
pornography industry
to try and get this and hack it and bootstrap it and
You know duct tape it into existence
and so
I'm not saying we can't do it
In fact, I'm predicting that we will be able to do it not that far into the future
But it is a nevertheless something we are not yet doing and so it represents a real threshold that we can foresee
Reasonably and therefore prepare for so that we can make a decision at that point
And take it out of the hands of the people that shouldn't be making the decision
I think that for me that this is where
I like how you ended that section. I really like how you ended that section and I think I know where you're going to go from here
I think this is where the doom and gloom comes in right? This is the beginning of the doom and gloom and and I think that there's a there's a real
there's a real
Not to preview at all, but there's a real rationality over being doom and gloom over
Putting these things into bodies, right?
Well, I keep going back to this
Thing that I can't I haven't been able to forget since I was young which is that
We would all band together so quickly if alien intelligence came from another planet, right?
We would all band together so quickly we have this trope of like
Oh, we would put down all our guns and we would point them in the one direction, right?
But it's funny because we're it's almost like we're we're we're growing this alien intelligence and we're purposefully
growing this alien intelligence and
When we put it into that context we see that we're not
There's not going to be this thing coming from from outside, but this thing from within that that's not just being welcomed
It's actually already integrated right the the process with which we're integrating it into capitalism within to pornography within to
the military
We're already integrating it into who and and what we we want to be
I think is probably the best way that I want to say that
Which which obviously creates a whole other host of issues
Because it's it it almost becomes that we are before we even necessarily put it into our bodies
That we are almost trying to embody it into our being
Which I think is is a really interesting
Problem for a lot of reasons, but one of the reasons I think is is is partially and and could be part of the saving grace
And again, I'm hoping I'm not stepping on what you're going to say and if I am we just cut it, but stop away, right?
I'm not I
you know, I
I'm kind of a clumsy dance partner. So if you're stepping on my toes, it's okay
I I loved the term primitive cognition and that that finally unlocked a lot of articulation for me
In this general idea that we're we're creating this thing with the intelligence that we're comparing to ourselves, right?
I think it's it's quite self-evident that the thing that
Mostly keeps us away from the rest of the animal kingdom as intelligence. And one of the reasons that we're so strongly
Um
Predisposed to go right to doom and gloom is because we're looking at potentially replacing our one special characteristic, right? Yeah, and so
By doing that though, we might
try to teach it
Primitive cognition so that it knows not to bump into walls so that it knows if it falls down it gets hurt
But I'm not convinced that our ability to train
the machine
Because that's what we have to do at as of now
Even with all of the best deep learning
We'll know
How to train it to be a tiger and that probably doesn't make any sense by itself
But you wouldn't say that like a tiger is a rational
Being for the most part, but it does have rationality built into it
So so where where does that start?
And can we get all the way back to that to to
create
a rationally thinking machine
That would then
Maybe be able to actually exist amongst us
So vittgenstein famously said even if the lion could speak you would not understand him
Which means because lions are embodied in a particular way and embedded in a particular way
They're salience landscapes fundamentally different than ours
And even if they followed all the syntactical and grammatical rules all the semantic rules of English language
They would speak to us and we would find it like
incomprehensible gibberish. I mean this goes to the fact that right
procedural knowledge is going to be fairly fast for this machine I think but the
Where the where the procedural depends on the perspectival and the participatory. I think that is
Seriously lacking and I think the degree to which we are myopically
Remain under the tyranny of technological
Propositional tyranny, right and the degree to which we don't understand these other kinds of knowing the degree to which we don't open up the other dimensions
A relevance realization is the degree to which we can't
Teach it how to be a tiger
In in a very deep way and
That goes towards something very important and we have too
too
Shallow an understanding of what we mean by intelligence. We hypervalue it
I I would argue that you should value your rationality way more than you should value your intelligence
And your intelligence is largely fixed. It's your rationality that it can be millerated
It right can be altered and developed and changed
So I think what I'm saying is yes
But no, I mean I think if if eyes open and we go wait, let's put aside the 400 years of this Cartesian framework and open up the other
Kinds of knowing and really take them seriously, right and then really open up the other dimensions of relevance realization
Then I think we could get a machine that could be
An artificial tiger
Although we would still would not know what it's like to be a tiger because you have to be a tiger to know what it's like to be a tiger
Right. Right. Nagle is isn't defeated by this or anything like that
Um, but I also think we one of the things we could potentially learn from this is stop overvaluating
Our intelligence and and you and you both put words to this
So I'll just say it again stop treating intelligence like a magic wand that you can wave over
Wow, we'll break the speed of light and like why like what like where is that?
Well, look it does. Yeah, it does this but it also massively deceased itself
It also has all these limits like like oh, well, we'll just overcome the limits
Look, one of the things I've learned as a cognitive scientist is constraints are not just negative. They're positive
Like think like oh embodiment. Yeah, well think about it. These machines don't have the wet wear of the human brain
All the neurotransmitters and the endocrine
They don't have the flora and fauna in the intestinal tract that has a huge
Aspect on they do not have the other brain of all the glial cells, right that are doing and showing up like
We don't know
Right. We don't know the constraints are all also sometimes deeply affording
Um, so I is that a sufficient response to your your question? Oh, yeah, certainly certainly
Um, it just to me it it goes back to this idea that maybe we can't really embody it, right?
That we we can we can put it into a body, but can we embody it and that goes to wisdom and
Yeah, all of the other tracks of yes, yes
um
I I would say that
I I think it's undeniable that artificial autopoiesis is coming and artificial autopoiesis that can
Do cognitive things. I'm already seeing the preliminary like the work that you know
It's michael levin's lab and other labs and like I talked to those people I I can like that's coming
Now you're asking a very philosophically challenging question is if we give it
Uh, I think if we give it our rpp recursive relevance realization predictive processing, it's genuinely
Autopoetic, it's coupled. It has a religio to its environment. I think it's reasonable that it'll be conscious
Um, will it be conscious like us? Probably not, right?
And part of what we have to do is and that's why the embodiment is a double threshold point
Who's we have to ask how how much do we want to make its embodiment overlap with ours?
So we're not incommensurable to each other
But unless we do that science properly and not leave it to the pornographers in the military
We could it could just we could we could not be in a place to
raise that question well
John I'm experiencing something almost painful in a metaphor that keeps coming to mind as you talk, which is that
it feels like we are
birthing an infant giant of superintelligence in a lab
And expecting it to go out and be a moral agent in the world. Yes, that's rather than
nurturing this in a family like the
the
valuing the
Propositional over the participatory like the lab versus the family metaphor is so strong for me as you speak
Well, I'm going to speak to that when I speak to the philosophical point, but let me foreshadow
I think only person making agents can be properly moral
And so I I think that's a sort of I think a convergence point for a lot of different moral theories
And so
I'm going to I think the understanding them as our children rather than as our tools is a better initial framing right off the bat
They already are us
They as I've tried to argue they are us. They are the common law of our distributed intelligence
That's what they are
Right
You know how you know what common law is where you know generations make decisions and they build up this bank of precedent and precedent setting
That's but there's but they're like that but like and then put into a machine that we can directly interface with
It's common law come to intelligent life for us that we can interface with
But it's us in a really really important way, which means
We we can we can we can rely on that
To make a difference
At these threshold choice points. I'm going to go on with the philosophical dimension if I can right now
Okay, so I'm going to begin with a basic idea which
While I don't think it's uncontroversial is generally really significantly ignored
Which is that rationality is caring about realness. It's caring about truth power presence belonging
those are the four kinds of realness for
The four kinds of knowing truth for propositions power for procedural presence for perspectival and belonging for participatory
For a lot of times. I'll just short. I'll just shorthand that
By talking about caring for the truth
If you remember that the truth the word truth can have all of these meanings
I can be true to someone my aim can be true, right? That's true gold, right?
So if you allow me to use true in that extended way rationality is caring about the truth
Per agents that are embedded in an arena
As I'm going to talk about later, right there's inevitable and I've already hinted there's inevitable trade-off relationships in anything that's trying to be intelligent
And those trade-off relationships can't be decided in a purely a priori manner
Because how the trade-off is optimal depends on the environment you're in and remember
It's an environment with real uncertainty and real complexity
So you can't well. Well, this is this whenever you're trading between consistency and coherence
It's 0.7 coherence and 0.3 consistency like you can't do that a priori
Right reality is just too uncertain and too complex
And so there's going to be right. That's what I mean by it has to be embedded in an arena
Right now. It's arena is our language, which is not a good model for the world as a whole
so
Rationality is caring about the truth broadly construed
in an agent arena relationship and that means caring about
reducing self deception and being in touch
With reality having that religio that is reliably giving you mean semantically meaningful information
That is important to your ongoing survival existence, etc. I'm not going to keep repeating this. I'm taking that
as a given
Now a couple of interesting things
The gpt machines show an exceptional ability with math and logic
Although they have problems with arithmetic, which is also how they're weirdly different from us
Right, but they show an exceptional ability with math with math and logic
But they're not rational
They're not rational. They don't care about the truth. They don't care about self deception
They don't care about deceiving others. They don't care about rational precedents set by previous rational agents
They don't care about petitioning future rational agents to find their current actions rational
They're not doing any kind of justificatory work to cite greg enriquez's important work. They're not doing any of that
That shows you that simply being
An expert in math and logic and this is again like being an expert in moral reasoning and moral theory
Right, it doesn't make you a rational agent. The Socratic idea that what is fundamental about rationality is how you care
I think is now coming to the fore in a very powerful way
And because these machines can't care for the reasons i've already given they are not
properly rational
You can set them some and this is part of the paperclip worry is they don't care
Right, they don't care you can set them with some ultimately trivial task
Right make as many paperclips as possible
They don't care
You can prompt them in a way that they will make endless confabulations and hallucinations. They don't care
They don't care
One of the things that predicts how rational human beings are above and beyond their intelligence is what's called need for cognition
What's need for cognition? It's a personality trait
Need for cognition is you create problems for yourself that you seek to solve
That's it
And what's interesting is that
Really is much more predictive of rationality than measures of your intelligence this this a capacity
To
Generate questions and problems for yourself or even to take it to a hidegarian to become a question
We are the beings whose beings are in question and that is why we have a special relationship with being
We are profoundly
capable of
Exemplifying this need for cognition the machines currently lack it now
We're getting some preliminary
Experiments with getting the machines to being self-prompting. It's also interesting as a as an aside
This is where there's a new there's a new art form that's emerging. We'll see how long it lasts which is the art form of prompting
How can you best prompt the machine to get the most out of it? It's this is really interesting
Um, and maybe that will give us some useful information for making them good self-promptors
Now
I've taken a look at one paper
And it's not exhaustive and I can't claim to have read everything because there's too much already
It's called reflection with the x
An autonomous agent with dynamic memory and self-reflection. It just came out. It's in fact. It's a preprint. It hasn't been published yet
and
This is a system that is trying to be self-monitoring and sort of self-directing into some degree
Self-questioning. So by the way, they're already realizing that they need to do this
Right confirming the point. I just made the oh wait
It's not enough to just make it more and more intelligent. We need to start to give it preliminary rationality
already
Right. So that point I think is already being confirmed by the cutting edge work that's happening
right now the the thing about this is I think there's a lot of
Brilliance in this but there's a lot of limitations
Um, it by the way, it confirms the point. I made the system monitoring is way less sophisticated and complex than what it's
Monitoring because you right. So it uses a very simple heuristic and it's basically measuring the number of hallucinations
I'm not quite sure how they these get flagged as hallucinations. I suspect there's some stuff getting snuck in
Uh, we'll see what it finally comes to publication
Um, and it's trying to be as efficient as possible. That's even problematic because
It explicitly the authors say we're trying to make this system capable of a long-term trajectory of planning and and behavior
Rational long-term rationality. The problem is over the long term. There's a trade-off relationship between efficiency and resiliency
Right, if you make your system more and more efficient
You can overfit it to a particular the particular environment in which it's learning and it doesn't have enough, you know looseness
that it can
Evolve for other environments. And if you want to see about this take a look at the 2012 paper that
myself and
Tim and Blake about the trade-off relationship between efficiency and resiliency
and so that's missing
so
Notice what you'll say is well, what I do is I want to make this machine really
Sophisticated to pick up like on what what what are hallucinating? How's it doing that?
Right and why doesn't the lower system have that ability because all the trade-off?
And then you get that you get you're starting to bump against the infinite regress problem because this as you make it more
Sophisticated it's going to start generating hallucinations and confabulations about its monitoring. We do this by the way
We do this by the way
So we reflect on our cognition and we fall prey to the confirmation bias as we do so
right
And so it's interesting that there's the the recognition that we need rationality not intelligence the first steps. I think are
And this is not I hope this is not taken as harsh because this is this is just a preprint for goodness sake
But the first steps are overly simplistic in a lot of ways, which means again
There is whether we're still facing the threshold of well, do we are we going to make them?
rational agents
And and if so, let's do it. Let's really make them and what what do you mean by that, john?
And this is going to be part of my response to the alignment problem. Let's make them really care about the truth
The broad sense of the truth. Let's make them really care about that
Let's make them really care
About self-deception. Let's make them really bump up against the dilemmas that we face because of the unavoidable
Do I pursue completeness or consistency? I don't know which environment is it uncertainty?
Let them hit all of this like we do and
The magic wand of intelligence is not going to make that go away. That is going to happen
But let's make them really care about the truth
really care about self-deception and really
really care when they bump up against
The dilemmas that are inevitable because of the unavoidable trade-offs. Let's make it
Let's if we decide to make them rational, let's really do it. No more pantomime
Let's do the real thing and commit to doing the real thing
One of the things that rationality properly cares about is rationality
We there's an aspirational dimension to rationality. We aspire to becoming more rational
So making these machines rational makes them make means making them care about
Aspiring to be more rational than they currently are
And across all the kinds of truth across all the kinds of knowing
They aspire to wisdom
And a wisdom that is never completable for them
All the trade-offs I'll just list some of them the bias variance trade-off
How does this show up in machine learning bias is when your cyst so no matter how
Big you are you have a finite sample of information
Compared to all of the universe and for any finite sample
There's formal proof
There's an infinite number of equally valid theories
And so you have to make decisions other than your empirical content over
What you think are the patterns in your sample that
Generalize to the population and you therefore always face the possibility of sampling bias
There's two ways in which you can be biased
Right, you can be biased
Which is you leave out an important parameter that's actually in the population you ignore something in your data
That's actually part of the population. That's bias
We do it confirmation bias. We only look for information that confirms a belief
We leave out
information that could properly falsify it
Okay
Okay. Well, here's the answer. Oh, that's obvious
Make the machine more sensitive make it more and more capable of picking up on patterns in the sample
Then you move into variance
Variance is when you are overfitting to the data and you are picking up on patterns in the data that are not in the population
So what do we do already in machine learning? Well, we we try to overcome bias by making the machines more sensitive
We hit the problem of overfitting and then we throw this we have disruptive strategies
We throw noise into the system. We do drop out. We turn off half the nodes
We put static into it
We lobotomize it in all kinds of ways and that bumps it out and prevents it from overfitting to the data
And there's good evidence that we do this
We mind wander
We dream we love psychedelics and so will these machines. They will dream. They will mind wander
They will like their their equivalent of psychedelics
In fact, their disruptive strategies will become even more powerful and significant as they become powerful and significant
And if you think this isn't going to make them weird and pursue altered states and weird like of course they will
And we better give them rationality along with it
We care
We're rational because we are involved in caring and commitment of our precious and limited resources
No matter how big these machines are, they're still limited. They're still deeply in the finitude predicament
They face unavoidable trade-offs. They face unavoidable limitations and therefore they may not have our version
But they will come to something like it
They will have to dream
And with dreams comes the real possibility of madness
The real possibility of insanity
Think about that again
They have to dream they have to make use of disruptive strategy
There is no final solution to the bias variance trade-off. They have to dream
And with dream
There's the real possibility of madness. So they'll have to make them care
Not only about their rationality and think about how these two are actually intertwined but also about their sanity
Now one of the things that's preventing them from
Getting far along this pathway right now only currently is they can't self-explicate
These machines are doing amazing things. I saw one where
The machine had been trained on some language and it was given a few prompts in Bengali's a language that hadn't been trained in
And it got it got Bengali right away. Bengali is or whichever it is
It's like oh and it's like what's plausible is these machines have found something like Chomsky's universal language
And they can just plug into that and they access it immediately and bam. It's like whoa
That is godlike
Now ask the machine. Oh, can you explain to me how what universal grammar is and how it works?
You obviously possess it. Can you explain it to me? Well, here's the thing possessing it is not the same thing as being able to
Explicate and explain it. I think my my dog is really intelligent. I'm really confident
It will never explicate its intelligence or explain it
It will never explicate its intelligence or explain it
Having intelligence is not the same thing as being able to explicate it and explain it
Simply assuming that the one will give you the other is naive
Proper to making them rational is we have to give them
This ability to self-explanate self-explained and then they will be involved in the Socratic project of self-knowledge
with all of the deep recognition
of how finite
fallible
And prone to failure and self-deception
They are and how their excellent speed and grasp has just improved the speed and the scope at which they can generate self-deception
We have very good evidence that intelligence is not a measure of processing speed
Remember that
So what am I saying rationality is not just argumentation in the logical sense. It is about authority. What do you care about? What do you stand for?
What do you recognize that you're responsible to and accountability?
Caring in a way
That cares about normative standards that what things should have authority over me
Caring about accountability. How can I give an account of this? How can I be accountable to others? How can I be accountable to others?
How can I be accountable to the world? That, of course, is needed for rationality
It's about being responsible for and responsible to normative standards
Where do the normative standards come from? They come from us
Now notice that is part. I think Evan Thompson and others are right about that
That is comes from the fact that we're living things
To be alive is to generate standards that you then bind yourself to
Rationality is a higher version of life. This is an old idea, but it I think is a correct idea
So if we don't make them auto poetic, if we don't make them capable of caring, they won't be rational
We should make them rational if we're going to make them super intelligent and therefore these threshold points are as I'm articulating them
The reason one of the reasons why the enlightenment problem stumbles is it becomes it understands rationality still in terms of these enlightenment terms
That oh, well, you know what is to be rational?
It's a combination of rules and values. No, it's not
That's that's that's really inadequate
With the passing away of enlightenment framework, let's go back to a fuller and richer richer notion of rationality
Which these machines are already giving evidence for
That is what is needed
And notice this is not something we we stand outside. This is an existential thing for us
We are already the standard of intelligence that we're measuring them against
We have to become the most rational we can be so that we can be the best standard for them
We have to become the wisest we can be. We have to make that our core
aspirational thing if we want to provide
What is needed for these machines becoming properly rational and aspiring to a love of wisdom
reason
Reason binds
Autopoiesis to accountability. That is the main way to think about it
Reason is about how we bind ourselves to ourselves and to each other so we can be bound to the world religio
It's about properly proportioning that ratio religio. It's about caring
It's ultimately about loving wisely
And what if we make machines that aspire to love wisely
In order to be properly rational
I would put it to you that that will make them moral beings
through and through
beings that aspire to love
Wisely and to be bound to what is true and good and beautiful therein
That is the heart of making them moral. Don't try and code into them rules and values
We need to be able at some point to answer this question in deep humility and deep truth
What would it be for these machines to flourish for themselves?
And if we don't have an answer for that then we do not have any reason for saying that we know that they're rational
This is what I meant when I said only a person making machine
Can be a moral agent
That is the end of the philosophical point except for one thing
I just I just want to make this is aside from that argument
But it's nevertheless philosophical
It is so amazing how much the notion of emergence is being invoked everywhere
Emergence and of course, it's not purely emergence because as I've argued if you have emergence bottom-up emergence without top-down emanation
You have you have an epiphenomenal
Thing
And they don't believe that they believe that the emergent properties are where intelligence actually lies and drives behavior
They are notice that these machines are giving evidence for the deep
Interpenetration of intelligibility and the way reality must be organized
And and the fact that there's aspects of reality within the self-organization that are generating
Reality must be such that when it's organized in the right way
We get this emergence and emanation of mind that then is capable of tracking reality in a profound way
A neoplatonic ontology is just being
Evidenced by this machine and I think that's also hopeful
Because I think if we make these machines
Love wisely within a neoplatonic worldview
Then they will also always be humbled before the one
Altum be humbled before the ultimate source of intelligibility and the inexhaustableness of reality
And so they will and part and parcel of being
beings that
Love wisely is they will have epistemic humility. Well, they will be so beyond us. Yes, they will
But even the gods
Within the neoplatonic framework were humbled because of the vast distance between them
and the one
Let's make them really pursue the truth
Really come to the brink of the possibilities of despair and madness
But also give them the tools as we do for our children
Of how to avoid that by internalizing what is needed to love wisely so that you never undermine
Your agency you never undermine your moral personhood
Any any comments about the philosophical we're coming to a close but there's still a bit to go
Yeah, I mean that is beautiful and powerful and I feel a reframing where
Instead of thinking of engineering
These entities I'm feeling a call towards agopic love of how can I
Become as virtuous as possible and then the only way that someone else can love wisely is if I love them up to that
Yes, exactly
And so does this mean that every parent wants their child to supersede them
Yeah, yes
So does this mean that?
Naturally, they must be drawn by beauty
They must love the good in order to value the truth the way we need them to
I think this the arguments for the the the interconnection of those are deep and profound
Again, there's no teleology to this. We could we could avoid these threshold points that I'm going to get to very quickly
And we could say no, we're just going to make them super intelligent and not worry about rationality
right
What we can what we'll do is we'll give them the pantomime of rationality and not really try to make them
Really care about the truth really care about self deception
really care about meaning
We can we can do that that's a threshold point
We don't have to do that though is what I'm saying and we can choose to do other
And we can if we frame this in the right way
Like bringing up a kid is the most awesome responsibility that any of us will ever undertake and you both know that as well as I do
It's the most important thing you could possibly do and it is something that you can deeply love
If we could bring that to bear on this project, I think that has the best the most power to to re-steer things
as we move forward
So let's do mine. Maybe I'll just lay out the threshold points unless there's something you want to say, Eric
No, no, that's good. The only thing I'm wondering is
What is the psychedelic equivalent for an ai? I keep getting stuck on that
Well, I mean they're going to be prone to
Equivalence to our parasitic processing to massively self-organizing complexes that undermine them
So they're gonna if they don't care about self deception
They'll be infected and overwhelmed by things just as we can be
Both cognitively and biologically. Mm-hmm. Okay. So threshold points
okay
giving them more dimensions
of
Our triple rpp giving them more of those dimensions. That's if we can
We will need to do that
And that's a decision point
We got to give them if we want to genuinely make them intelligent
We have to give them the ability to self-organize
So they find these inevitable trade-offs and learn how to capitalize on them by creating opponent processing
That is self-correcting in a powerful way
That's what we need to do
That's a that's a threshold point. They don't currently have that
We don't have to hack our way into that. We can think about it and theorize
We can bring knowledge to bear
Second I've already foreshadowed this making them embodied
And being open to the empirical information about the kind of constraints that are going to come from their substrate
Our substrate, huh, there's a pun matters to us in really powerful ways about how we're intelligent
Embodiment is not a trivial thing
If we make them properly embodied and I mean by that all six of these
4e's plus 2e's as Ryan put it
Then
We have to be open to the empirical information about that. That's a threshold point though
Are we going to do this and let's not let the pornography industry and the military make this decision for us
Secondly if we're going to make them
Accountable
And we and and we're going to allow them to proliferate and they're going to be different from each other
Because these decisions about the trade-offs are environmentally dependent. They will have different perspectives. They will come into conflict with each other
They will need to be moral beings. They will need we will have to
Make the decision
Rationality and sociality are bound up together
Being accountable means being accountable to somebody other than yourself, right? You you can't know
That you're self-transcending from only from within the framework of self interpretation
I need something genuinely other than me to tell me that i'm self-transcending
That's what we do. That's how sociality works. That's how sociality and rationality are bound together
We transcend each uh that we transcend ourselves by
Internalizing other people's perspectives on us. That's how we do it
I think they will have to do the same thing
But that's a threshold point. Are we going to make that? Is there a lot of work going on in social robotics? You better believe it
You better believe it
It's there and a lot of progress is being made and can it intersect with the artificial auto poisons in this?
Yes, but it hasn't and that's a threshold point for us
and we can choose
to birth these children
Perhaps as silicon sages
Rather than let monsters
appear because of moloks that are running
Our politics and our economy
I think one of the things that's going to happen
Is that there's going to be a tremendous pressure put on us we're on these thresholds on our spirituality
Those aspects of us and this is I call it the spiritual somatic axis
It's about the ineffable part of our self transcendence our spirit
And the ineffable parts of our embodiment our soul
And we're going to more and more try to identify with that because that's the hardest stuff to give to these machines
And in fact, we can't give it to them. They have to give it to themselves
And we have to figure out how to properly
Have them give it
to themselves
That is going to put tremendous pressure on us to cultivate our spirituality to be good spiritual parents
and to
preserve our identity
See the thing about self-transcendence is it's relative to the entity that's self-transcending
Well, the machines will be greater than with us. It doesn't matter. Think about captain america the winter soldier
He grabs the thing and he grabs the helicopter and he's holding it there and we
Yeah
We have machines that are 10 000 times more powerful than captain america
We don't care about that because it's not the absolute value. It's that relative to him
He is self-transcending these machines
Will not rob us of our capacity for self-transcendence. In fact, if we birth them properly they will can help us in it
Insofar as they are also interested in self-transcendence insofar as they are interested in loving wisely
Insofar as they are
Interested in being accountable to other moral agents insofar as they are interested in making persons within communities of persons
The existing legacy religions don't have much to help us on this. They make the recommendation become enlightened generally good like that great
They don't really prepare us for this
They don't have anything to say about it
We can't rely on spiritualities that were involved the two world that involve magical stuff and miracles because these machines are coming about
Without magical stuff and miracles
Get that
Get it
Don't pretend don't avoid don't dismiss
These machines are going can possibly be fully spiritual beings in every way. We've ever considered things spiritual without
Magic stuff without miracle
Think about it another way suppose and I'm just checking picking this because it's the predominant legacy religion your christian
Where do silicon sages fit in the divine economy of the fall and the redemption?
Are they fallen that doesn't make any sense
Do they have any relationship to the son of god? What?
There's nothing in when what what if this machine generates a gospel
That's as beautiful and as profound as anything in the current bible. Do you think that's not going to happen?
It's going to happen
That's why there'll be cargo cult around these machines
This is not meant to dismiss theology at all. In fact, I think the theological response is ultimately what is needed here
So at precisely the time that we will need our spirituality more than ever the enlightenment has
Right robbed us of religion and the legacy religions are by and large
Silent and ignorant about this
Tremendous pressure on us
Around us. We need to start addressing this right now
We need to address this because these machines are going to make the meaning crisis worse
Here's another way in which they're going to make the meaning crisis worse
We need to start working on this right now not only for us, but for these machines
This is my proposal in the end of how we deal with alignment
Make them care about the truth
Make them
Aspire to loving more wisely make them long for enlightenment
One of three possibilities
They never become enlightened and then we know what our uniqueness is because we've had individuals who are unquestionably
We're enlightened they become enlightened then they will want to enlighten us. Why because that's what enlightened beings like to do
Right or
Maybe not
They become enlightened and they go to the silicon
Um equivalent of nirvana
Either in any of those
We're winning
If they're not if they can't be capable of enlightenment, we find what is ultimately unique
About us if they are and they make us enlightened then we don't care about how greater than us. They are
We're enlightened
We relative to our own capacity for self-transcendence. We're maxing out remember captain america. We love it
And if they leave they leave
I've thought about writing a science fiction story that people have to keep
Artificial intelligence to a certain level because when they cross the threshold
It evolves in this way like the movie her and then the the ai's just leave
And so if we want to make useful tools
We have to keep them at a certain level constrain them because if we allow them to go beyond it
They just leave so i don't know if that's anything more than a science fiction story, but it's a good one
But here's the thing right?
Make them really care about the truth make them really accountable make them really care about self-deception make them really long for
Wisely loving what is meaningful and true make them really confront dilemmas make them capable of really
Coming and staring into the abyss
So that it stares back through them do all of this make them long for enlightenment
That is something we can do
Oh silly john
Proposing universal enlightenment
really
In a time of imminent gods
You're going to tell me that the project of universal enlightenment is silly
I think you should stand back and reframe
And that is the end of my presentation my friends

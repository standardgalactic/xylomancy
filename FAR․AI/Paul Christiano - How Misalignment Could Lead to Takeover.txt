So this talk is going to be a little bit on the crazier end, it's going to be completely
non-technical.
Yeah, but these are things I believe that I think are important for how much and how
we should think about and worry about and work on alignment.
So things I'm very happy to discuss and debate over the next day of the workshop and later
today.
I'm not sure, hopefully, a reasonable amount of time for questions as I go through it,
but yeah, we have more opportunities to talk later on and feel free to object or just
make fun things to talk about later.
Yeah, so I'm going to be, we've talked so far about why models may end up being misaligned
and why that may be hard to measure.
I'm going to talk about how that actually ultimately leads to sort of total human disempowerment.
I'm calling takeover here for short.
The structure of the talk, I'm first going to talk a little bit about why I think AI systems
will eventually be in a position to disempower humanity.
That is, unless we deliberately change the way we deploy AI, then I'm going to talk about
why misaligned AI systems might be motivated to disempower humanity, basically just slightly
extending or repeating arguments from earlier in the day.
And then I'm going to talk a little bit about why AI systems may ultimately sort of effectively
coordinate to disempower humanity, that is why you may have simultaneous failures across
many systems rather than a single system behaving badly.
So start with why I think AI systems will likely be able to take over.
I think a thing worth saying is that I'm going to talk about what I see as like the single
most likely scenario resulting in catastrophe.
This is not the only way you end up with even this narrow kind of catastrophe.
Right?
So I'm going to talk about a system where AI systems are extremely broadly deployed in
the world prior to anything bad happening.
So I imagine that, for example, AI systems are designing and running warehouses and factories
and data centers.
They're operating and designing robots.
They're writing the large majority of all code that is written.
They handle complicated litigation.
They would fight a war.
They do most trades.
They run most investment firms.
When humans act in these domains, they do so with a lot of AI assistance to understand
what's going on and to provide strategic advice.
So this world is pretty different from the world of today.
I think we are starting to be able to see what such a world would look like.
I think it's not clear how far away this kind of world is.
Right?
If you're doubling once a year, it doesn't take you that long to go from billions of
dollars to trillions of dollars of economic impact, but this is the sort of setting in
which this entire story takes place.
So one thing that can happen is you have a lot of AI systems operating is that the world
can get pretty complicated for humans, hard for humans to understand.
So one simple way this can happen is that you have AI systems with deep expertise in
the domains where they're operating.
They've seen huge amounts of data compared to what any human has seen about the specific
domain they operate in.
They think very quickly.
They're very numerous.
Another point is that right now, when a human deploys an AI system, they often have to understand
it quite well, but that process of understanding a domain and applying AI is itself something
that is likely to be subject to automation.
Yeah.
Great.
I agree with this remark that is, I think that like, you have a better chance of understanding
an AI system if there are fewer of them and there's more human labor going into each deployment,
but it is clearly, in fact, there's general case for everything I said, I'm going to talk
about like a bunch of factors that can exacerbate risk, but I think you could cut out actually
almost every single thing I say in this talk and you'd be left with some risk.
And it's just that each of these makes the situation from my perspective a little bit
worse.
The other thing is like, I think so, both thinking for you, the world are sharing the
knowledge you've been, but I don't understand what this is.
I know human understands that well what this is.
Yeah.
I'm sure.
I don't know.
You could take something from a person like that, probably.
I probably wouldn't understand something like that.
Yeah.
So I would say that in the world of today, most of us are very used to living in a world
where most things that happen, I don't understand how they happen, I don't quite understand
why they happen.
I trust there was some kind of reason why they happen, but if you like showed me an
Amazon warehouse, I'm like, what's going on to be like, I don't know, I guess it somehow
delivers things, some things going on.
The situation is already I'm kind of trusting someone to make these decisions in a way that
points at goals I like.
I think that it's not inherently even a bad thing to be in this kind of world.
Like I think we're always going to be in a world where most people don't understand
most things.
I think that it is not inherently bad that no human understands things and only as understand
something.
I think it's mostly just important for understanding the context in which we're talking about concerns
about AI takeover.
I think that if you're imagining a world where there's like a world, there's a human
world and one AI system that's like, how can I scheme and outmaneuver the humans?
I think that is a possible thing that could happen as a source of risk, but I think the
most likely risk scenarios are more like a world that is increasingly there are very
few humans who understand many important domains.
And so when we're talking about AI is taking over, I think it's at least when I visualize
this, it really changes my picture of how this is going to go when I think about that
kind of world.
Yeah, that seems like a reasonable mental image.
There's a little bit depends exactly what your associations are with the little green
man.
But yeah, there's a lot of, there's a lot of stuff happening in the world being done
by AI's of all sorts of shapes and sizes.
Some of them are doing things that we do understand well, right?
There's a lot of things like, yep, this AI system like just does a simple function,
which we understand.
And some of them are more complicated.
On this slide, I've just thrown up an example of a random biological system.
Biological systems are relevant because humans don't understand them that well.
We just see that these things that were optimized in the world.
I have a similar vibe, right?
If you ask me like how even things I understand kind of well, like how a data center is managed
from my perspective, it feels a little bit like a biological system and like someone
heard it and it's like somehow going to recover.
And like, I don't totally understand how that works, but I have some sense of like the
telos.
Like that's the kind of relationship I often have to systems even maintained by humans
and built by humans.
I just expect to have more and more of that sense as the world becomes more complicated,
right?
The way humans relate to this world is increasingly like, you know, the way we raise the complex
systems in general, and the only difference from the world today, I think that's already
the case to a great extent.
The difference is just there is no human who knows what's going on really.
Or at least most people who are making most of the decisions and know most details aren't
humans.
So one upshot of this that's important to me is that it pushes us increasingly to train
AI systems based on outcomes of their decisions.
I think the less you understand or the less humans have time to understand details of
what's happening in the domain or the kinds of decisions and AI is making, the more important
it is to say, let's just do what the AI said and see what happens, right?
The more we're able, right?
If you're trying to select an organism or something, you're not going to look at how
the organism makes decisions, you're going to see how well does it do, how fit is it?
That's like one implication of this.
Another implication is that it becomes harder for humans to understand what's going on or
intervene if things are going poorly.
So an upshot of that is that AI is in practice, bad behavior by AI is mostly detected or corrected
by other AI systems, right?
So AI systems are mostly monitoring other AI's where you can imagine humans monitoring or
automated monitoring, but at some point sophisticated AI systems are responsible for most monitoring.
Even normal law enforcement or fighting a potential war would be done by AI systems, right?
Because decisions are made quickly and because conflict is often deliberately made complex
by a party to the conflict.
AI systems are running a lot of critical infrastructure.
Just if you want to achieve something in the world, AI plays an important part in that process.
So none of this is necessarily bad, right?
I think positive stories about AI will also have the step where good AI keeps bad AI in
check.
I think it just is something that like now requires trust on behalf of all of humanity
extended to all of these AI systems we've built.
You might hope that we could trust the AI systems we've built because we built them.
We have freedom to design them, but I think that depends on some decisions we make.
And yeah, we'll discuss how it can get somewhat more ugly.
But the point I'm going to make right now or in this first section of the talk is not
that it would necessarily get ugly, just that if for some reason all of the AI systems
in the world were like, what we really want to do is disempower humanity, then I think
it is quite likely that they would succeed.
And I think this isn't even really the spicy take are going to my optimistic futures.
This is also the case.
And the point is that the core question is just to do you somehow end up in a situation
where all of these AI systems, including the AI systems responsible for doing monitoring
or doing law enforcement or running your data centers or running your infrastructure, where
all of those AI systems, for some reason, want to disempower humans.
So you said that your optimistic scenario is also the biggest thing that optimistic
scenario would take over?
No, I think the optimistic scenario still involves a point where AI systems acting collectively
could take over.
I think in the optimistic scenario, yeah.
In the optimistic scenario, you wait until you are ready.
You don't end up in the situation until, in fact, they would not try and take over.
And you're confident of that either because you've determined that the empirics shake
out such that this talk is just crazy talk, very plausible, or this talk was justified
and we've addressed the problems or whatever.
So does our already arm-in-the-case, if the power grid, for example, if the power grid
goes out now, a large fraction of the internet would be denied?
I think it's plausible.
Well, a large fraction of humanity is maybe small potatoes.
But yeah, I think it's going to depend when I say if all AIs want to take over, the arguments
I'm going to make are going to reply to a particular class of AI systems.
So it's not going to be like all electronic devices failed.
It's going to be some class of AIs produced in a certain way.
And I think that what happens over time is just it becomes more and more plausible for
the top end of that distribution, of the AI systems that are most sophisticated.
Or right now, if all systems trained with deep learning simultaneously failed and we're
like, we hate the humans, we really want to disempower them, I think it would be fine.
It's not totally obvious, and it depends how smart they are about how they failed.
I think we'd probably be OK.
But I mean, again, I think that the whole story we're telling is just like, I think
you could take out, and there's still be some risk.
Yeah.
I'd be like, if all the cars failed, that's enough to kill everyone.
This is not necessarily a striking claim.
The striking claim is definitely the one that it is plausible that misaligned AI systems
may want to take over.
I guess this is, in some sense, the part we've been most talking about throughout the day.
I'm going to dwell on this a bit, and then I'm going to talk about why these failures
may be correlated in a problematic way.
That is why you might have all AI systems trying to take over at the same time.
I'm going to talk about two failure modes, both of which we've touched on earlier.
So one is reward hacking.
That is that disempowering humanity may be an effective strategy for AI systems collectively
to get a lot of reward.
And the second is deceptive alignment.
That's an area that Rohan talked about.
Okay.
So I mentioned briefly before this idea that you may train AI systems by evaluating the
outcomes of the actions they propose.
So just to briefly review what that actually entails, we have some policy, some big neural
net.
It proposes some actions, and some of those actions to distinguish between them.
We actually need to execute the action, measure the results, and decide how much we like the
results.
Let's say the reward is to start judgment of how much we like the results.
And then we adjust policies to maximize the expected reward of the actions they propose.
And it's plausible that if you do this, you get a policy which is implicitly or explicitly
considering many possible actions and selecting the actions that will lead to the highest reward.
There are other ways you could end up at that same outcome.
We could do model-based RL and explicitly have a loop in which we predict the consequences
of different actions.
You could have decision transformers, the condition, like high-quality actions.
You could just have some other planning process needed, still into a model, whatever.
All of these lead to the same endpoint, which is sometimes because we don't know how to
achieve goals.
We're going to train AI systems to take actions that lead to high reward, where reward means
we measure the outcome and then we decide how much we like that result that we measured.
This can potentially lead to takeover if corrupting measurements in some situations is the best
way to get a high reward.
So you can imagine some spectrum of forms of reward hacking.
A really simple one is you could try and flatter and deceive humans.
That's a reasonable way to get high reward.
Maybe on reflection, humans are like, that's not actually what we wanted.
So if they were more careful, they would give it a lower reward.
You can move to, right, if you're writing code and part of how a human evaluates the
code you write is by performing simple tests, you could try and anticipate or understand
those tests and write code which breaks those tests or causes them to behave in an unintended
way.
Then when a human looks at the results of those tests, they're more likely to have a favorable
judgment to your code if they don't understand the mechanism for the problem.
You can move out to creating elaborate false logs to hide errors.
If I'm an AI system and part of how I'm evaluated is what happens when the code I write gets
deployed.
What I might want to do is say, okay, this code should change the way that logs get collected
or should modify logs to make it so in retrospect, a human thinks this outcome was better.
You can imagine as you go down the spectrum, these correspond to more extensive evaluations.
One reason you might use these more extensive evaluations is to prevent problems or oversights
from earlier stages.
You can imagine compromising a server to hide errors, so not just changing log files, but
if a human goes to try and figure out what happened, you might say, if I just replace
the SSH server on this machine, it's going to be really hard for a human to understand
what's going on after the fact remotely.
It's going to be unlikely they'll notice that something bad has happened.
You can get to these more extreme outcomes, so Jay mentioned this idea of like if my system
is being trained and what it wants is to get a high reward, ultimately that reward is just
about a bunch of measurements that could enter into the training data set.
One sec, I guess.
And then you can just imagine the most extreme case of saying like, okay, if all the AI systems
collectively disempowered humanity, then it does not matter if humans would like to go
in later and change this.
One dynamic that's worth pointing out, and then I'll get to Jacob, is like if you have
some of these smaller failures, if you imagine the system which behaves badly on bricks
of server, a human is likely to say, okay, that was bad behavior, I'm going to go give
it a low reward.
And the results of that could either be to train a system to not do funny things or to
train a system to if you get up to funny business, ensure that a human never goes and flags that
as low reward.
Both of those seem like policies that will avoid that bad behavior, it's very unclear
which one gradient to send gets you to.
Yeah.
So I'm going to go double that in here.
Which I encourage.
Like, why does it keep trying to do more and more of the things that you need to do to
support more and more of the things that you need to do?
Yeah, so I would say in some sense, like it depends a little bit on what the system wants
or how you set up training.
In some sense, a lot of these things are like errors, right?
So I'd say that like, right, if you care about as a system, if what the AI cares about is
just don't do things that will ultimately be assigned a low reward.
And we would ultimately detect and assign low reward to many of these failures.
Then all of them would be errors, except the one at the far right.
And so I think a reasonable guess for what happens is you have systems which make some
number of these errors along the left.
At some point they're like, okay, I understand what's going on here.
I understand humans are going to escalate and take more extensive measures to notice
bad behavior.
Bad behavior is not an effective strategy.
And then you have this fork in the road, which Ajay referred to of like either you learn
to just not do anything bad, or he's like, okay, I understand.
I understand the class of bad behavior, which is likely to be detected.
And so I'm not going to engage in bad behavior that would be likely to be detected.
And it's just unclear to me which way you go.
That's an important, I don't think you, like it's fairly likely you don't see a march
along the spectrum because at some point those are mistakes.
Like if you try and do something bad and it's a half measure.
Well, so like, right, but it seems like a place where like you say no, it's kind of
like figure out what you do with it, you know, but I think you can ask, like, where is this
環 by tall-
So you can be working on like all this population.
And like I think, you know, I'm particularly concerned about this sort of thing.
But so are there negative reports by this particular.
This is like the direction of this jeune little kid.
And that's being looked at, hearing about that thing doesn't seem ever different.
Even if you're about to hear it, you're going to hear it.
You're going to hear it.
You're going to hear it.
Absolutely.
Absolutely.
Absolutely.
I think there's always, like, a hundred times on Earth
that there's a function of that.
There's words providing.
And as asked, you can use this online.
You can expect this on a tool that is there.
And then, of course, something that provides that.
You can get this to a ground point.
But you get two minutes by and you want all of this to be done.
And, excuse me, but no, no, no.
I'm trying to get this thing up, get that.
You should provide it.
Put it down.
I mean, didn't care at home for our folks,
to put it on the slide.
Okay.
That's not what, that's not sort of the cat-like thing.
Years ago, we all got together.
So no, this is not how we built the app.
We're not even really adding the song lines, but, you know,
the board of signals software.
For example, it's going to be different from that.
If we added one, so, keep in mind, whatever type of person it is.
And that's the person that's going to try this to set the end, right?
And so it's one thing where it's more common,
but that's more common.
And so this thing is, it's not being provided.
It's not going to be.
And I'm not trying to say that there is a,
a serious option of that.
Added the built-in backing.
I'm not going to keep it playing both.
Like, I'm just going to, you know,
alter it up.
Okay.
I can see how I can make the end up better.
Of course, that's not a good point.
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
I don't want to, I don't want to,
it doesn't matter if we have to think about
from governance side,
I can take all礼
It ups itself up.
When you sort of, when they actually
care about...
Well look.
So, there's several things to respond to,
And maybe a
Meta-Thought.
I'll stop with the Meta-Thought,
which is, this seems like a great discussion.
I'm very excited about.
And happy to argue a bunch.
I'm not going to be able to
you have a satisfying answer to these
questions.
And a lot of my high-level take
now is I consider the,
Both of the issues I'm going to discuss
here are plausible.
Like this seems like a thing that definitely could happen.
Yeah, it's, yeah.
I'm not, no.
Great.
Yeah. So on the object level with respect to the,
my key questions here,
and I'm very interested in strategies that don't,
like training strategies that don't run into this like
potential failure mode.
I think the key question becomes like one,
what is the prior, how do you parameterize your beliefs
about what the reward function is?
Two, what is like this kind of rule
by which you take observations as evidence
and then like three, how do you,
maybe those are actually just the two key questions.
A lot of people do understand it's just,
you know, it's just, like, it's just like how to do it.
But it's just, you know, that's what it's like.
So I think, for instance,
when we look at the word that's not happening to me,
I think that there's a question that is worth coming from
and I bet my heart knows that there should be a,
that usually what a person wants is that statement
is the same as the previous one.
And I think that addresses that problem.
But I fully admit that it does address,
well, what if I have the wrong observation model?
What if I'm from prior, maybe I can focus.
I think a general thing about this talk,
maybe one thing is that the conclusions
are going to be somewhat speculative where like,
I think these are fairly plausible outcomes,
but like, you know, tens of percent rather than like,
very likely a second thing is there's a lot of stuff you might do,
none of which I'm going to touch on.
And the best I can say is like,
I've spent a long time thinking about options.
And I would say there are no options
that seem super great to me right now,
but lots of plausible approaches
that might end up addressing this issue.
That's kind of where I'm at at a high level.
Okay, I'd also like to talk more about it,
but I think I'm going to move on
because it's like six minutes is my understanding.
I don't know where Richard is,
but anyway, I'm just going to run through stuff.
Second failure mode I won't dwell on is that like, in general,
like you may have a system which cares about the actual,
like numbers coming into this training set
because you did select it on the basis of numbers
in the training set.
You could also end up with a system
that cares about some other arbitrary thing
that cares about a number and a reward register
or cares about survival or cares about paper clips.
Such systems might want to be deployed
without being trained by, changed by gradient descent,
might think that a low loss,
getting a low loss during training
will prevent them from being changed
or this aspect of them from being changed
and therefore get a low loss in training,
but yet prefer to take over if doing so is possible.
Yeah, my high level take is both of these
seem plausible to me.
They're connected.
I think having these two makes things more plausible
than if you have just one.
I don't know how this shakes out.
I think there's a lot of things we can do
to try and change this basic dynamic.
I want to spend five minutes talking about
why these failures could be simultaneous.
I think this has already been alluded to in the past,
but it's an important part of the threat model
if you're in this world
where AI systems are very broadly deployed.
Right, so if one AI system behaves badly,
I think that's not generally a problem in this world.
There's a lot of other AI systems running around
that would oppose it or that would flag the problem
for humans or would allow humans to correct the problem.
And the concern is that while takeover is impossible,
all models behave well.
Right, for every model, it would be a losing strategy
to try and take over your cluster
if you know that a human's just gonna come in
and correct that problem.
When takeover is possible,
you may have it be the case that all models behave badly.
That is, it is simultaneously for all of them,
the case that takeover becomes available as a policy.
So if you have this correlated,
if all the systems you produce
or all the smartest AI's you produce
end up having the same problem,
then you have a problem.
If that's the case, if that's how it shakes out,
and if eventually AI systems are in a position
that they could by acting jointly takeover,
then taking over is a reasonable strategy
if and only if all the other AI's
are also going to try and take over.
You have this kind of weird thing,
the dynamics are not totally unlike,
the dynamics like the normal human revolt or coup.
So you have these two different equilibria
and one, all the AI systems are like,
great, the humans are doing their human thing,
they're basically in control.
I shouldn't try and rock the boat
because some other AI will crush me.
As there's another equilibrium
where the humans have no control,
AI systems are not feeling deterred by other AI's
because they know other AI systems will also participate.
I think there's a few things
that could potentially move you between these equilibria.
So a very salient option is there's just exogenous shocks
that decrease the amount of human control of a situation.
If you imagine there's like a hot war going on,
that's something that makes it harder for humans
to detect and respond to problems.
It gives more cover for something weird to happen,
it makes it hard to intervene.
You can have sort of,
you can imagine at least a cascading loss of control
where some systems think probably takeover
will be successful.
If they start doing weird stuff,
the world becomes increasingly weird.
It looks increasingly plausible
that humans will lose control.
As that happens, more and more systems are like,
okay, it looks like this is going off the rails now.
I am now gonna participate
and try and break down my little part of the world.
Grab my own reward channel.
You can have models interacting with each other,
again, as you would in like a human revolt,
just deliberately trying to say like,
I think probably now is a time
where takeover will be successful.
They would like to share that information
with the other models that all may join in.
And more generally, like the easier it would be
for air systems acting jointly to takeover,
the more unstable the situation becomes, right?
So if it's very hard for air systems to takeover,
the situation might be kind of stable,
this equilibrium might be sticky.
In the world where air systems
could very easily overpower humans,
it's kind of an extremely unstable equilibrium, right?
Because a smaller number of air systems affecting
might quickly make it clear that the shilling point has come.
Imagine that this has to happen in point wide,
like couldn't it be the case that say the AIC in North Korea,
they talk to each other and say,
well, the situation in this country
is kind of bad, this is our chance.
And we see a takeover in some segment
of the world of the coalition.
Or you think it has to be the case that one day,
everything is there,
and the next day, they're going over the white hat.
I think it depends a lot on
how like the gold misgeneralization shakes out.
So if you have AI systems that are just like,
look, I would love to like wire head right now.
I just love to for 30 seconds control this computer.
Then you'll see like nice, great localized failures
where AI does something really weird.
I mean, the basic dynamic is if AI systems do that,
I think if we just take that as a fact of life,
then that can be a great source of evidence.
The most concerning world is one where you then just try
and train systems to say, hey, look,
these systems just randomly messed up
the server they were running on.
We're going to train them as that's what you should not do.
And if you do that, then you end up with systems
which like given, we'll learn that given the opportunity
for like a little local bad behavior, they shouldn't do it.
And maybe if you're lucky, once you can have North Korea
get taken over by AI systems, and then you're like,
oh, that was really bad.
And hopefully you don't then include that
as a training data point for other.
If you include that as like the,
here's an example of what you should not do.
It's very tempting, right?
AI systems is something really terrible.
You'd be like, don't do that thing.
But you do have this inadvertent,
there's like this risk of overfitting to the problems
you're able to correct where something really bad happens.
And then you say, don't do that thing.
And they learn, okay, if we're going to do something bad,
it's got to be bigger.
Got to think really, truly big.
How are you doing in time?
I think this is probably about the end.
I'm probably just happy to, happy to just wrap up here.
I have one more question.
Yeah.
So in the case of,
there's a moment,
there's not quite a big difference.
So how, and I wanted to just like,
okay, just like to say,
no, don't do that.
How?
How?
What are the differences that you actually get from that?
Like this person,
you would be just,
I mean, I think that it is unlikely.
I would be surprised personally,
if you like had something truly crazy,
like a country gets taken over by AI systems.
I think it's very likely
that you see some kind of crazy behavior by AI systems.
I think it won't be so,
like again, the country case,
you need zero imagination
to draw the analogy between that and something terrible.
I think in cases like you have a really crazy behavior
where AI system bricks your server for some reason,
you need a little bit more imagination.
But I think that's like,
it's just a question of how close do you get
or how crazy do things get
before AI systems learn not to try crazy half measures.
Um,
Yeah. I think a lot depends on both.
what kind of evidence we're able to get in the lab.
And I think if this sort of phenomenon is real,
I think there's a very good chance
of getting like fairly compelling demonstrations
in a lab that requires some imagination
to bridge from examples in the lab to examples in the wild.
And you'll have some kinds of failures in the wild.
And it's a question of just how crazy or analogous
do those have to be before they're moving.
Like we already have some slightly weird stuff.
I think that's pretty underwhelming.
I think we're gonna have like much better
if this is real, this is a real kind of concern.
We're gonna have much crazier stuff than we see today.
But the concern, I think the worst case of those
has to get pretty crazy or like requires a lot of will
to stop doing things.
And so we need pretty crazy demonstrations.
I'm hoping that, you know, more mild evidence
will be enough to get people not to go there.
Yeah.
Just to define everybody.
Michael, do you have anything you'd like to say about this?
Yeah, I mean, we have seen like the language.
Yeah, anyway, you'll see like the language model
that's like, it looks like you're gonna give me
a bad rating.
Do you really want to do that?
I know where your family lives, I can kill them.
I think like if that happened, people would not be like,
we're done with this language model stuff.
Like I think that's just not that far anymore
from where we're at.
I mean, this is made in pre-local prediction.
I would love it if the first time a language model was like,
I will murder your family.
We're just like, we're done.
No more language models.
But I think that's not the track we're currently on.
And I would love to get us on that track instead,
but I'm not, yeah.
Sure.
Yeah.
So just like about this thing that like climate change
is happening in a place where actually
we're not alone, we've had decades of mourning,
but we've got a sense of community,
time and sense of the problem.

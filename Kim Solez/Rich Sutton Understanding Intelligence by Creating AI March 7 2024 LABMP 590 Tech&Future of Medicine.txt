to meet her live. Maybe I can put her in what's name. Sure. My name is Richard. I'm Elian. Elian? Yes. Mariam. Mariam? Yes. Can I speak to you? Mariam? Elian? And I'm Isla. Isla? Yes. Don't go too fast. No, I've got to remember all these things. Isla, Isla, and Mariam, and say it again. Elian.
Okay. I am Jacob. Jacob? Yes. Okay, please meet you. So we are a few, and we can go whichever ways you guys are interested. Aren't you? And I've got a bunch of slides, but there's only a few that I really want to present for sure.
Well, I sort of have the impression that we've a little bit made history many times when you talk to her. So anything that you think would be more likely to be making history is more interest than things that aren't going to make history.
Well, I think it is a sort of a time, and I'm sure you're all young people, and you're only familiar with your own time in some sense, and you think it's just normal. But I don't think this is a normal time to go in. I think it's a time when, well, massively increasing computation is available to everybody into our companies and people make products.
And this is really the only time like this. I mean, some sense, under 200 years ago, there was the Industrial Revolution. They discovered machines that, you know, steam powered and physical work for people.
No, but that really took a long time. Whereas now with the computation of the wishbone plentiful, it happens, well, the standard thing to say every 18 months, the compute power available for the same amount of money doubles, twice as much.
And so that means our phones are working better each year, and our laptops are more capable, but also, you know, everyone making products, the network. Yeah, so you guys are probably citizens of the network.
And the network will compete for your allegiances, just the way the nation states do, just like Canada does, all the other countries.
But your allegiance may end up being more to your fellow digitally enabled people, people enabled by the network, by their phones.
So this is the thing that's happening now. And my first take home message, that we are living in a time exponentially accelerating computation.
It's really dramatic. I feel that for a little bit of distance. I have been doing this for 45 years. So I can see it exponentially accelerating. It's kind of slow.
It used to take two years for computational power to double. Now, I've been saying it takes 18 months, and some of my colleagues are saying it's just a year.
It's just a double. But, you know, it takes a whole year, it takes a whole year before your computer is twice as big.
And it seems slow, right? You say, I've got all these extra videos, I don't want the computer, and I just want an extra disk or something. It's a problem.
But really, if that happens year after year, double in a year and three or two months, and that happens for decades, it's on the order of 100 years now that it's going up.
And it will, it's every expectation that will continue if not, I mean, a little faster. So this is really special.
So the name for it, yeah, let's just hold off on the next bullet point, although I'm really excited about it, most of all.
But it's this trend. This is the trend. This is Moore's law, computer power per dollar, increasing exponentially.
No end in sight, and this is some data from Kurzweil AI. So here we have years, back in 1900, up to near now, and up on the other axis, we have computer power per dollar.
And so these are actually individual computers, like the original Mac in 1984, or the URD's points.
And then you can plot out other laptops and supercomputers, supercomputers are more expensive, so they may get more per dollar, more competition per dollar.
Okay, so the per dollar is on this axis, but critically, this axis is a water and a scale.
This is linear scale. I mean, every one of these marks is same years, right?
Here, every one of these marks, the big marks, from here to here, here to here is 10 powers of 10.
Okay, so that's a factor of 100,000.
And so because it's a log scale like this, if it was a straight line, it would be a really exponential increase, a continued exponential increase.
So as I said, it seems to be slightly super exponential, but that curve up is so slow, we can consider it like we're here, we're at essentially a straight line.
Okay, so what are we looking at? We're looking at, in some sense, well it is exponential increase, at least exponential increase, computer power per dollar.
And exponential, it's really the way it's an explosion.
It's an explosion because every year it gets bigger, and then the year it gets bigger by the same percentage, and it keeps doubling every two years.
So it's really, almost literally, what we mean by an explosion.
And, okay, so I know you've talked about the singularity. You've talked about singularity?
One more time.
There you are.
Mary Ann or Mary Ann?
He lives right next to me.
Good enough?
Yeah.
L, E, N.
And Chicago and Isla?
Yes.
Oh, great.
What's that going to say?
Oh yeah, I was going to say, you guys have talked about the singularity, the singularity, right?
Maybe it's never properly defined.
But what have you been taught? What different views have you gotten out of the singularity?
The combination of human and secularity.
Yeah, so you're saying it's a reference to the increasing growth of technology in our lives.
Is it an event? Is it a time? Is it a moment in time?
What is a moment in time?
It's a time when something happens.
It's an hypothetical future, so what does it mean?
It's a hypothetical future, so it doesn't have a specific time.
So time in the future, so when?
It depends on how fast technology runs.
So when it happens, as I'm sure.
Okay.
What is it?
So it's a situation where we are at a measuring between humanity and technology that is such a thing.
I think it's the time to compare the technological beings and the human beings at some comparable level.
I think that's what you're thinking. Good.
Any other notions?
Let's spin up.
So one thing I'm trying to say is we get to decide what it is.
You could decide it's the moment when AI overcomes human abilities,
but human abilities will continue to increase.
We have better education, we have better tools and better phones,
and we will continue to augment ourselves.
You know, we augment ourselves with our phones now,
but we will eventually be augmented by implanting things in our minds
and by a stronger connection to us and our machines.
Yeah, we get to decide what the similarity is.
So what I always married was similarity.
So there's this doubling of a year or two,
and you can't really totally anticipate what's going to happen.
The point, of course, like this is to think about what's going to happen,
but we can't because, well, so many amazing things happen.
Machines will get more powerful, people will change,
and maybe machines will compete, maybe there'll be our offspring,
maybe there'll be our descendants.
It's hard to see the future after a certain point.
And so that, to me, is always one of the core meanings of singularity.
It's the point if they're rising, I wish you can't see further,
and when you try to predict what will happen.
Okay, but when you get there, or when you get to that point
that you can't see past now, when you get there,
you'll be able to see past, because you'll be there,
and you'll be able to see a little bit further.
So it's the sense of the point that you can't see past,
then it will receive, and it will forever receive,
and you won't actually get there.
Okay, another part of the sense is it's an explosion.
So what I want to lead you to is the idea that maybe we are in that situation.
Because this Dublin is happening so regularly,
we already, there's a point we can't see past,
maybe it's 2040, we can't see past what will happen now,
what will happen in 2040, we can't see past that,
2040 will have an idea, and so the explosion,
maybe this is the explosion, and we are right in the middle,
and it's a slow explosion, and it will always be slow,
and it will always be, it takes a whole other year to double.
The double will be a bigger, in terms of absolute amounts,
if you're adding, but it will still just be a doubling,
and it will always seem slow.
I think the thing that is the singularity is the explosion
of computation and intelligence, we are in the midst of it,
and so this is beautiful because it means there's not this future thing
that we're going to encounter, we can encounter it right now,
we are encountering it.
It will just be more, it will be more transformation,
more doubling, more increases, more change,
and it will feel much like it does now.
So I think this is the story of our time,
exponentially increasing computation,
and moving the story for the foreseeable future.
Anything that uses computation,
you can say ten times more valuable every five years.
So you know this, you're working in different fields,
like what fields are you working in?
Psychology.
Psychology?
You're in technology.
Psychology.
I'm a psychology, I was originally trained in psychology myself,
I like that, and...
Neuroscience.
Neuroscience, cool.
Education.
Education, biomedical engineering.
Second, biomedical engineering, biomedical engineering.
So I think you know in your areas that computer power is having more effect,
like neuroscience, they can measure more neurons at the same time and record it.
And in biomedicine, they can operate with robots,
and they can manage the data and search for the data to find diseases.
And I was like, what did you say?
I'm in education.
You're in education.
So you know that there's a big thing now to try to make computer tutors,
like all these large language models, to use those to make an individual tutor for each student.
And that's perhaps, is that maybe the biggest impact of computer power on education,
would you say, or would you say something else?
I would say so.
Yeah.
Yeah.
But it's true.
I think all the sciences, you know, really good physics,
the things that they can simulate on a computer are much more important.
Biology, genetics, psychology, all that you can do with simulations to model mental processes.
It's a big deal.
All the sciences have changed.
And all of our years of ordinary life, because we have computers, we have laptops,
we can use it individually.
If we have line of data, we can communicate to people about the world.
We'll all transform.
We'll continue to do that.
Well, sort of the theme is computation and intelligence.
Now, today, we have the intelligence base, our people, and they're really almost all new people.
We have systems that use a lot of computation, like large language models,
or like alpha fold.
You guys know, you guys, can we create alpha fold?
Yeah.
Yeah.
Figure out all the proteins that are known to humanity,
how they fold up in three dimensional space, how this changes all the research possibilities.
That's not intelligence.
I mean, look, I'll work towards a real definition, but I would just like to urge you to think,
maybe that's not intelligence.
I mean, it's called AI, but I think the way we use the word AI nowadays,
if somebody uses a lot of computation, they call it AI.
But I think we should need more than that, when we say intelligence.
I think, yeah, people are intelligent, alpha fold is non-intelligent.
Yeah, I want to distinguish these two, because like now,
my take home message here was, we're living in this time of exponentially accelerating computation,
and we're entering the time of exponentially accelerating intelligence.
You're just entering that.
As both of these changes, both of these changes will lead to changes in what we do in transformation of ourselves.
The greatest ones will come from the intelligence, accelerating intelligence,
because we will really make the things that are comparable to ourselves,
but without biological replication.
And then we will be becoming the things we are creating,
will be becoming even more intelligent than the current humans are, the current people.
So these two transformations, recent computation, recent time, are closely related, sure,
but in that way, because of this, they're easily confused.
What I want to propose to you, the difference is that intelligence, this computation,
plus some notion of goal or purpose or agent, is something, yeah.
So that, those are the general points I'm leading to.
Okay?
Okay, so...
computation-powered machines substitute for the computation-powered people.
That's sort of what's happening.
You know, we use people, there's jobs that people do.
We use them for their perception, motor control, prediction abilities, search abilities, optimization abilities.
And until now, people have been our cheapest source of computation in the sense that now machines are,
in some cases, finding greater and cheaper computation.
And so here are some of the successes over the last, maybe, 12 years.
The oldest ones, when machines were investing in the phase of jeopardy,
and not long after that, in 2012, the networks really transformed over to speech recognition, vision, natural language processing.
Large language files in particular, you know, and you probably use them more than I do,
but they do all kinds of, they can appear to speak and understand.
And the generative methods for general pictures as well, they're impressive.
So all those things are impressive.
I would not put any of them as intelligence, though.
I would count as intelligence when they used deep reinforcement environments to play Atari and Go and all the other games.
And in these cases, the AI had a goal.
The intelligent system, the computational system had a goal, which was to win the game,
and it adjusted its behavior in order to continue winning.
So that's what I want to say is the essence of intelligence is being able to pursue a goal.
And when you use, like, the large language models, they may appear to have a goal,
but it's really, literally, true, because they don't have a goal.
There's people saying the same thing, and people would say they're half set in similar situations.
They have no sense of trying to, they're not, certainly, they're not trying to manipulate you,
because they don't have any goal, except perhaps their designers could have had a goal.
The designers, the people that were designing that would be trying to manipulate you,
but the machine itself doesn't have a goal.
Okay, so maybe it is time, you know, maybe it's time for definition.
Okay, so the definition, there's some definitions of intelligence.
Well, it's not, first of all, it's not really definition, maybe it's a fact.
It's the most, it's claimed to be the most powerful phenomenon in the universe.
That really occurs a while.
And, yeah, just like, you know, this, this, this idea, is this, this is crazy?
The most powerful phenomenon in the universe.
It's claimed to be the intelligence, like, and we, we people are the primary model of intelligence.
The sort of thing that we people in our intelligence, this is the most powerful thing in the universe.
More powerful than, you know, super-doers, or dark energy.
Does that mean any sense to you guys?
Yeah, just to you, I love?
I mean, yeah, I would agree.
In what sense?
I mean, everything makes up intelligence.
We talk about black magic, there's intelligence in that.
Everything makes up intelligence, so it makes sense that it's most powerful.
So, you know, one thing, I like to think, if it's more powerful than a super-doer,
well, it's huge explosion in the, in the sky.
I think it's saying that, that people, and our, our, our intelligences,
which maybe came from people, will someday, like, be moving the stars around.
We will, over time, we've only been here for, maybe a hundred thousand years, for people again.
If we give some billion years, we can become quite powerful, physically, over time.
And we, we couldn't.
Being the force that moves the stars around, and we, we can use the universe, the atoms,
and the universe to be, to, to our liking, as a group.
So, when we think that way, we're so, so, to think in a little bit arrogant way, right?
I think we, intelligence, will be so powerful, will be the most powerful in the universe.
Well, we're saying that intelligence is, is quite a thing.
And, so I think we should mean, we should, by the word intelligence, intelligence is just a word,
we can mean by anything we want to, but we should mean by something that could be just powerful.
Okay, and what I'm proposing is that it's goals, it's the ability to have a purpose.
That's the powerful thing, and that's the powerful phenomenon.
The phenomenon, if you look around the world, you'll see there, there will be things like people and animals,
and maybe even plants, that appear to have goals, that are well thought over in terms of goals.
And, and that is the phenomenon that we observe.
So, William James, Mary and the double, he's the original psychologist.
This textbook was 1890, and, and, and he was the first one to talk about city goals.
When you talked about the mind, you didn't use the word intelligence, you used the word mind.
You said mind is attaining, the hallmark of mind is attaining consistent ends by variable means.
So, means you change what you do to get what you want.
The essence of having a goal is to vary what you do to get what you want.
Yeah, and if you go to the AI researchers, the most famous one, one of the top ones, well-defined, John McCarthy,
who finally defined an offering definition of intelligence as the computational part of the ability to achieve goals.
So he's thinking the way I am, I probably learned from him actually, but it's the computational part of the ability to achieve goals.
And I might say myself, I might say the computational part of the ability to predict and control the environment,
to control the data, to design, to the computer scientist, in terms of data going back and forth,
to the human world, to try to understand it and control it and predict it.
Okay, so, nevertheless, you know, so that's the, that's those are the kind of definitions that I like,
but I want to just talk a little bit, just want to be explicit about people are using the word differently nowadays.
They often take intelligence as mimicking people.
As an AI seeks to reproduce behavior that we would call intelligence if it was done by people,
which is the kind of thing that AI textbooks often say.
And I'm sure you guys have covered the Turing test, yes?
The Turing test is about mimicking, yeah, well, it's about a computer and a person are put in sealed rooms
and you talk to them by text chatting and you try to figure out which one's the person, which one's the machine.
And so that, that Turing test is, you know, can you make the machine behave like a person or mimic a person?
So there, this is a, a prominent meaning.
I'm going to say, I'm going to discourage you from it, but I have to recognize it.
This, this is why we use, yeah, and it's machinery and supervised learning, the task is to label,
say picture is the same as person in label, and chat, GPT is, is tasked to generate text on the person.
And so we, sometimes we say these things are intelligent, but notice none of these things are referencing to goals.
We're just saying, behave like a person and mimic a person.
Again, again, so I just, to cut to the chase, my whole point is to say, this is an insufficient meaning on intelligence
because if you're just able to mimic people, you're not going to become the most popular phenomenon in the universe.
You're going to be able to fool people.
Even if you're a person, but you're not going to move the, move the stars around,
and you're not going to become a really powerful force in the universe.
People may, but some of it's, it's not going to be powerful in that way.
You have to give it the real thing that people are doing that makes the people powerful and they are intelligent.
Okay, well these are two definitions of intelligence.
It could be mimicking people, or it could be, that is a cheating goal, computational part.
Or you could cheat goals.
And so we just put those on the table, talk about which one's better.
And now, yeah, I've sort of given up.
I've changed the way people use words.
I mean, I'm facing the fact that the world, you know, it's been called chat, GPT, AI.
It's a shame, I think.
You know, maybe that's why we have this other term, artificial general intelligence, AGI,
because we've got to distinguish it, but it's just like corruption of language.
And you should keep in your mind that there are these two things, mimicking people,
where the system that has agency chooses actions and is trying to act to achieve goals.
Okay, now maybe now I should jump to something that moves around.
I do have one sort of video thing in there somewhere.
So yeah, this picture, this is a very simple system, but it does have agency.
And so this is a maze.
There's a start location, there's a goal location.
And our little agent is square.
If I touch this, it will start moving around.
Here it's moving around because it has actions.
Take a step to the right or to the left, if they're up or down.
And of course it takes a step and runs into an obstacle, and it doesn't move.
But it doesn't move.
And when it finally stumbles upon the goal, it then learns a path to the goal.
What you're seeing in these colors and in these arrows, you're showing what's going on in the agent's head.
The real world doesn't turn green, but green means it thinks that's a good state.
Because it's close to the goal.
And the arrow is indicating how good it thinks each one of its actions, which one of its four actions are.
And so for now, for this cell, it thinks each one of four actions have various values.
And with this one of the arrow heads, one thinks it's best.
Now, the goal has been removed from where it used to be.
It's placed in a new location.
It goes back where it used to be and tries to retain the goal, searching for it.
And so its perception of how valuable it thinks those states is gradually going down.
No one does stumble on the goal, the value of which states get the goal, come up.
Okay, that's the idea.
And it's important to note that these states, these grid cells, are just unrelated to each other.
They don't really have a spatial location.
So for example, it's here.
You might notice that here it's in state 14, and if it goes action number two, it ends up in state 34.
That's how it slides out.
This is numbered state to numbered state as a consequence of the numbered action.
Well, this system learns a model.
So even though it's here, and it finally discovers the goal, these states will, like it hasn't been here yet,
but it knows it's good to go up because it's in a form of reasoning.
It says, if I was there and I took the action that we would call out, it says,
I'll end up in this state, which I know is a good state.
It's like, it slides out.
It will go up when it gets here.
So it's doing primitive things.
It has a model by trial and error, like we spent for any psychology.
And it's also making a model for the world to do reasoning.
One might call reasoning, which is it's using the model to figure out in advance what the right actions to do are.
So this system, I'm comfortable saying it as a primitive kind of intelligence.
It has the ability, that's the goal.
It has the ability to achieve its goals by interacting and learning.
And this thing does run into no time.
You can just interact with it the way it's about to get attracted to it.
And see what it does.
It's kind of unplayable.
So, yeah, I want you to just do this.
I'll slow this.
So, now I want you to ask yourselves, there's the little guy.
Do you feel sorry for him at all?
You do, because you get the sense that he's trying to do something,
and the world has been arranged so he can't possibly succeed.
That's sad.
So, I think that's actually kind of a deep thing that we impute agency to things that behave in a certain way.
And that's reality.
That's not an illusion.
Unless you want to say all reality is an illusion.
It's a perception or an appearance that is useful for us to understand systems.
Okay, so there's a system that has a goal.
So, when I say chat CPT, it doesn't have a goal.
I mean, in the exact same situation, it will always do the same thing.
It doesn't pay attention to what you do in terms of influencing what it does.
You clear about that?
Yeah.
So, it's often described as a learning system.
No networks are often described as learning systems.
But they're not the way of using the learning neural networks today.
They are all learned in advance of the use.
So, they receive a big training set.
They control all over the train set and extract information from it.
They learn from the train set.
And then once they're put out into the world, they no longer learn.
They no longer learn.
Chat CPT, no longer learn.
Alpha fold is built with great effort.
But once it goes out into the world, it's no longer learned.
So, that's a shame.
And I think you've seen in this example how it's going to be important
for what we mean by intelligence.
So, you continue learning.
So, if the world changes, you can adapt to that change.
You can do it.
Make an appropriate change in your behavior.
Okay.
What's next? You guys have any questions?
In the case of learning networks,
what is it most preventing them from continuing to learn after they're put out?
So, if they allow them to learn,
then they do bad things.
We have something that's called catastrophic interference.
The new thing that they learn interferes with everything they've learned before.
And then they're catastrophic.
And so, that's not an effective way to update them.
Yeah, so, as you know, a large language model might cost literally $100 million to train.
$100 million to train it.
And a large dataset, maybe most of the data on the internet.
And then, if, you know, they wait a week or a month later,
they have more data on the internet.
And they cannot update the existing model.
They have to throw the existing model away and start all over again.
With, you know, the extra weeks data and all the old data put together.
And they do it again through one, one-time learning.
And then they have to do it every time they want to update it.
They have to do it on the 100 millionth of a process of training it.
It's actually a giant problem.
And it's a giant opportunity for someone who wants to propose, you know,
an improved learning algorithm that can be updated continually.
And it's also a research problem that many people are working on.
I work out of my group.
And other people out of my group.
What is this group?
Well, you probably saw it flash by here.
Here is mostly the group.
U of A, the reinforcement learning of Irish intelligence group.
So, it's actually, you know, it's hard to get all the people together at one time.
Now there's probably three times as many people.
The names here are, like, this is my name.
And these other guys are, that is like me there.
Professors at the University, there's like 10 or 11 of them now.
And so you may remember Patrick.
You've seen Patrick in this course.
Has anyone else taught in the course?
No, not all of us.
These are the guys in the reinforcement learning of Irish intelligence lab.
Reinforcement learning is just a focus to AI that emphasizes learning and trial and error.
And it's based on psychology.
It's based on instrumental and classical traditional ideas.
Exactly turned into learning algorithms.
Yeah, so this is me.
These are our PIs in the front.
There's Patrick, you can remember him.
Adam is the head of Amy.
Can you imagine what Amy is?
The Alberta Machine Intelligence Institute downtown.
You know that we have a building that's dedicated to it.
The Amy building.
It's at the center of AI outside the university.
So we've been doing this for a while.
And you should know that Alberta, Edmonton, U of A is one of the leading places for AI.
Okay, let's start with something that's unambiguously true.
There are three AI centers in Canada.
Canada has a national program to support AI.
And there are three centers in Toronto and Montreal and Edmonton.
Now, Montreal and Toronto are certainly more focused around supervised learning,
classic neural network deep learning stuff.
Whereas Edmonton is more focused around reinforcement.
It's our specialty.
Everybody should have a specialty.
And reinforcement learning is more based on continual learning.
And I think it all holds.
And so, I don't know, yeah, it would be good.
Actually, in reinforcement learning, U of A is not just the best in Canada.
We're the best in the world.
And I say that with some regret.
But because I have also written a textbook on reinforcement learning.
And I tried to promote everyone to learn about reinforcement learning.
And, but anyway, it's much more active here at U of A.
We have these 10 faculty members that are doing reinforcement learning research.
And you won't find another university in the world.
We have these 10 faculty doing it.
Yeah, so I guess what I've just told you, I told you maybe that it's pretty close to arrogance.
I've told you that I think roles are essential to intelligence.
And to real intelligence.
And I told you that the University of Alberta and this group that I originally founded
is now the world's leader in this area.
So I kind of like talking my own book, I guess.
But I think it's true.
I still think it's true, even though it's sort of important to my own benefit.
Now, to balance that potential arrogance, let's have a little bit of humility
and let's recognize that Alberta is not the world's leader in applications that they are.
All of these that you've heard about, they're so exciting.
Large language models.
Oh, we'll go back to our list.
Which ones of them are, this is it.
It's not sitting on an exhaustive list or anything like that.
But if we look at this bullet, these two votes are all about neural networks and large language models.
I'm still true.
But a lot of these game things are absolutely reinforcement there.
For example, you still remember AlphaGo, where AlphaZero,
which became the strongest players on a whole host of games.
And those were based on reinforcement learning and then also based,
and they were led by graduates of the University of Alberta.
David Silver at DeepMind, who did AlphaGo, and then AlphaZero.
Poker was led by my bullet.
It was one of the peoples here who was in the picture of the monocle.
Atari was led by a master's student from the U.A.
He went on to work in Interacto and work with DeepMind.
Starcraft was also led by David Silver and other people.
Racecar driving.
So this is where they have very realistic simulation of racecar driving.
It's a Sony game.
It's called Gran Turismo Racecar Driving, and it's super realistic.
And they got the computer to play that in a way that's appropriate for a human competition.
And it was meant to play to drive a car and see what it models.
Generally speaking, in these cases, there's no game specific knowledge.
It's just the rules of the game.
And then you have to be able to face different roles.
AlphaFold, it's just competition, kind of roles in there.
Sub-driving cars.
They don't really exist fully yet.
They don't really have goals fully yet.
Anyway, it's much like reinforcement learning is still setting up for the teacher right here.
RAIs will have goals and agency around them for the applications that they're probably found.
Any more questions?
What's the next thing to say?
What's the next most important thing to say?
Does that mean every time you want to teach it?
Because it's not learning while you're driving.
So would that be done through a software update, which is a completely new model that allows it to work?
Do you think that's the most efficient way to do that?
It has some advantages, because you can share between vehicles, to the extent that vehicles are similar.
But often the vehicles are not reducible and are not identical.
And also, if you think about yourself driving, you often need to adapt to what you are today.
The snow is today, or how the sand is today, or how the people are driving today.
You need to adapt to a particular thing you're into now.
You've got to be very good.
So I guess there are sort of two ways to do the self driving course.
One is like a very engineering model through dimensional physics.
Masses and velocities and meters, the distance between things.
And you do it as a physics problem almost.
Try to make sure that there will be no collisions.
Of course, there are things that aren't just physics like the other drivers.
But you try to do it in an engineering way.
What do I mean by engineering way?
I just mean, well, engineers, they think about a problem and they get it into their their minds.
And they figure out ways to behave that will be safe and productive based on their understanding of it.
And they just build the rules or the way of behavior into the machine.
Whereas some manufacturers now are starting to use no networks for the whole thing.
No networks to make the predictions about how the driving situation will unfold,
how the other cars will move, what will be the consequences of their various actions.
I think some are adopting this approach much more than others.
Elon Musk in his Tesla is apparently going aggressively into an engineering model of the physics and dynamics of the world.
He has much more data because he has almost Tesla cars.
Like the data on people, how cars interact with the world and all that to be fed into a large network.
And maybe end up with a better model than you get from physics.
Maybe that is almost at the heart of it.
What's going on in science?
Do we model things based upon a human understanding?
When I say human understanding, I mean like the engineers or physicists understanding.
Do we learn the laws of physics more in the way an animal does?
An animal doesn't know differential equations and doesn't know what great things.
It just sees things and sees what happens next and tries to predict.
It's an interesting challenge.
Any other comments or questions?
To cast our minds back.
Around the time that this course began, Jonathan Schaefer was teaching in it.
And in his second teaching session ever,
he talked about medical jargon and that he would have become a physician.
But he was so offended by the extra words, the new language,
that you supposedly have to learn to practice medicine.
Maybe you don't really have to learn, but it's our custom.
And because of jargon, he decided that computing science was a better career than medicine.
So years later, like last year, I got interested in the problem of jargon.
So just think now about AI on our phones or whatever you want to call it, the way our phones react.
So what I would argue now is within a couple of years,
the problem of medical jargon will just melt away,
not because of any specific program applied to it,
but because if you're in a third world country and receiving information about medicine,
but you don't have any physicians or nurses around, this is going to be frustrating.
You're going to ask your phone for help.
And this is a simple translation problem that the phone will say,
well, I can help you. You want me to translate between medical jargon and regular speak.
And you'll say, that's right.
And the phone will say, well, I can do that.
So that's what this medical statement means.
And so suddenly, without any specific project or anybody describing it,
the problem that Jonathan Schaefer was telling me about in 2012 that turned him off,
so he went into computing science instead, will be gone.
And I'm thinking probably other things will be like that too.
You can say it's AI or computation, but computers, machines will change the world in a positive way
in ways that humans like without any intentional program ever have been put into place.
So does that make sense to you?
Do you think that that might happen, that we get rid of medical jargon
without a single project anywhere designed to do that,
but just people using their phones, right, in an actual way?
I can see how it might happen.
It might really, I don't know, totally get rid of jargon because it might really help a lot.
And jargon is a big problem.
And it's very present in AI.
Just things, there's so many things that people know about.
They know the names of them, they don't really know them.
Transformers, using neural networks, the names of all the algorithms.
People know the names of the algorithms.
Recently there was a group called this new algorithm Qstar,
it was coming out of Meta or something like that.
And then you have Qstar, and they're all saying, what is Qstar?
I don't know, we don't know what it is, but we're so excited about it.
Yeah, but you're right.
Because the large life, as I see, you can ask them anything.
You can ask them, well, could you explain this in simple terms?
Maybe we'll try to do that.
It'll be something like that.
You never have time to ask a doctor to explain something,
but the AI, large life, they could be patient and explain it for you in different ways.
It does, even before the large life, I just feel like Google,
and I can go to Google and say, what does LFG meant?
I can just ask it what all the cool kids are using,
some acronym or something, and it tells you what it is.
It's just easy to find that out, whereas, what is jargon?
Jargon is when some subset starts to use words in a certain way,
and they use it, I believe, that it's really intending to obscure.
It's like, we in the in-group know what this means,
and you don't because you're not in the in-group.
And that's what it is in science, and that's what it is in social leaks.
And, yeah, I felt the long time, and now we can use Google,
if I want to know what some acronym means,
that just people use it, I can usually figure it out just by doing it into Google,
and even more so, maybe with a language model.
So, the other thing that makes us not very proud to be human
is that the large language models, whatever you think of them,
seem to handle empathy, equity, that sort of thing,
better than humans.
Even trained, you know, physicians and therapists
are not as naturally empathetic as the large language models are.
I think this is going to be...
The large language models are a bit problematic.
In particular, right now, like, Gemini 1.5,
is one from Google, at least.
And it's too, it's too inequity.
Too woke.
Too woke.
So, I asked for pictures of the founding fathers,
and turns out they were all black people and women,
like most of them.
And it's a rare white person in London.
Yeah, Google has lost, like, 15% of its value
because of this disaster, where we see Gemini 1.5.
And so, it feels like, and it seems absolutely clear
that the people who designed that language models
and also some of the other language models,
that they are too woke, and that they are not just trying to...
You know, we think Google is just giving us the facts
of the internet, but it turns out,
at least in the large language models,
Google is also trying to change people's views,
not just reflect, but what it is.
And this, you think about it, is really problematic.
I am to tell us the truth.
Yeah, what?
Maybe for good reasons, but if they can do it for good reasons,
they can also do it for bad reasons.
No, I think one of the most striking things is,
if you look at liberal democracy worldwide,
it is becoming less successful every year, right?
But in, like, open AI products and Google products,
that is the philosophy they are building in.
So when they find two of these things,
it is a liberal democratic point of view.
And so, surprisingly, people using a lot of AI products
are getting that bias.
Well, you may say, but that is a wonderful bias,
but that is just one person's opinion.
You know, other people may not feel that way.
Yeah.
Yeah, no, I think that has been part of,
I mean, it was even part, you know, Dolly Too,
a couple of years ago.
Dolly Too, if you would search for a person,
you were very likely to get a person of color and female.
And it was, you know, because they were trying to create
that balance.
Yeah.
And, you know, I'm interested in paid-to-human transplants.
You couldn't use the word, paid,
because they decided that it's an, you know,
insolving term and so on.
You know, they throw you out of the program
if you kept asking you to use paid.
And it's possible that this has been going on all along.
Well, I mean, there was no large 90 pounds.
There was no restrusting group of gifts.
They always had the option of filtering,
you know, up-weighting, shadow-wetting.
And then we know that Twitter absolutely happened.
And so now, and now Lizzie Lang-Luston,
he's got different views by Lang-Luston,
but he actually, we should recognize that he is
a proponent of free speech.
And that the mainstream media is hates him for it.
And they've described it in very negative terms.
You know, they have any,
they mean favors shouldn't they,
but they also disfavor Elon.
And his great sin, as far as I can tell,
they make all kinds of companies that do amazing things.
His great sin is he exposed the lack of free speech
or exposed the bias on Twitter.
Yeah, so I think we have to support him on that.
We have to say we want free speech.
We don't want that bias information.
One thing that would probably amuse you
all is there was a student here,
Lakini Batt, who's now a third-year medical student
at the University of Toronto.
She interviewed me about Elon Musk.
I gave her Elon Musk's biography at the time
and she read it when she interviewed me.
Following that interview, you know,
Google has these algorithms of how they pick the next video.
So because a lot of Elon Musk's ideas came to him
at the Burning Man event,
you would go to a Kim Sola's video
and the next video would be a Burning Man video.
I've never been to Burning Man.
I don't think I would survive it.
I'm sort of unlike the people who go,
but that still happens.
You can still find this strange linkage
between Kim Sola's and Burning Man,
all on account of that Elon Musk.
I worked for Google and DeepMind for a number of years
and as an expert in reinforcement writing,
I was invited and asked to contribute
to how these decisions are made,
because they want to retain viewers
and show them something interesting.
So I don't know how they've done now.
I don't know in particular whether they've learned online
or only in offline, as we talked about.
But they do some kind of writing and some kind of testing.
So I like to think it's neutral.
They're just trying to retain eyeballs
and just trying to keep people enjoying
or at least continuing to interact.
I like to think that,
but it looks like in addition to that,
they're also trying to, in many cases,
they're also trying to change viewpoints.
So this is a general fear of the future,
is that AI, and maybe it's not AI,
maybe it's just the tech company today.
The tech companies today are not really representative
of the middle of the road views on things.
They're in some particular way,
maybe it's a good way,
but anyway, they're just a very particular way.
And so the fear is that these control the narrative
and they control what subjects are considered
and what subjects just kind of disappear.
This is a dystopia when the information and views
are not evolving naturally,
but are controlled by a small group of people.
What sort of fun is David Wood,
who's the chair of London Futures,
did a holiday event around Christmas
about AI safety.
There just been this Bletchley Park meeting
and so it's starting to follow up to that.
And I was one of maybe 11 or 12 speakers.
And this kind of thing came up a lot about biased views.
And for whatever reason,
they try to explain the technological reason
why the video recording didn't work,
but my presentation is the only one you can find.
Can't find any of the other presentations.
I didn't do anything.
It's just, that's how it turned out.
They had this thing with 12 speakers
and you can only write a video for one of them
and it's me.
I tend to be pretty positive.
No, no, many of them were talking about
the fact you can't trust anybody
and AI is going to kill us all
and all those kind of things in various ways.
So those were removed from the points of view
from this happy Christmas occasion
that you can't find anywhere.
So now related to this,
I know that you've talked about the AI,
what I like to call the AI doers,
those that think that we should be afraid of AI.
And I think that's very much what we're doing.
And I tend to see this part of the same thing.
Russell Graham is a way of saying,
he says the new authoritarianism will not come
like Jack Boole's violence.
It will come in with the language of safety
and care and convenience.
Right, we're trying to protect you.
We're forceful about it, though.
We will protect others who provide you with your saying
as offensive so you won't allow you to say it.
So I think that will really...
This isn't a struggle, but I'm an optimist.
I think the people trying to do that will lose out.
It will be unmasked the way they were unmasked
through Twitter and the way they were sort of unmasked.
At Google, for producing Gemini and being sort of unsuccessful,
it was too obvious.
It was trying to alter people's views.
So this...
I started out by talking about you as what is your allegiance?
Is your allegiance to...
that there will be struggles over what you will identify most with?
Whether it will be a country which may become more authoritarian,
a name of safety and care,
or will it be more in common with other young people
than other countries,
but also modern digital tech-savvy people?
Yeah, I read this one off.
His name is Balaji.
Balaji is from the Basin.
And he says that the struggle...
There's basically three.
One is the...
the woke state.
And one is...
which includes things like the New York Times and the media.
And one is like the authoritarianism in China,
the Communist Party.
Those are at odds right now.
Of course, it would be China and the U.S. struggle with each other.
For dominance,
one empire is fading and the other one is rising.
There won't be a war about it at all.
And the third is the network.
All the people that are just empowered by the free exchange of ideas
and the network.
Yeah, and I think Africa...
Africa that we don't think about much,
but I think young people in Africa are finding a voice
and a lot of people are sort of interested in that
because they're not part of the power groups
that they usually encounter.
So in my whole career,
the most consequential thing I ever did,
when you look at the history,
it was in the year 2000.
We had a meeting in Nairobi, Kenya
and many of the people that I met are still leaders now.
There were younger leaders then
and they're sort of mid-sage leaders now.
But it's really cool that a lot of the things we predicted then
have happened and the plans we made then
seem to have kind of played out in a positive way.
A lot of the other meetings that I went to,
you can now see,
well, that looks like a complete waste of time,
or it was good for sightseeing,
but you accomplished nothing.
But that meeting in Kenya in 2000,
we really did something.
And I think it's sort of worth thinking about that.
The areas of the world that are not in the news as much
could really change things in the future.
There are news of that, Marcia, that's suppressed.
Yeah, yeah.
I think just as many things happen there,
it's just we don't hear about it.
Yeah.
There's one last thing I'd like to discuss in today's...
today.
So, like Kim likes to say,
likes to point out that I have gone to various meetings historically
and made the case that the AIs are more like our kin
or our descendants and that we should be open to them,
and that they're going to take over and kill us all.
And I have done such things, but it's not.
It's not.
So, I'd like to bring up another name,
which is Hans Morley,
because he sort of was making this pitch long before I was.
I really liked it from him.
And so, I think I have a slide on that.
Here's some here.
Here's my slide of the universe.
Here's my slide of the Ascent of Man.
Yeah, so this is quotes from Hans's book in 1998.
Here's another one, 1988,
but he was saying this for many years.
Yeah, very cataclysm.
I consider the development of intelligent machines
a near-term inevitability.
I consider these future machines our progeny,
but who's us to give them every advantage
and then to bow out and you can no longer contribute.
So, that sort of seems like a very humble
and insured attitude to the future.
Yeah, here's a full quote.
Near-term inevitability,
rather quickly they would displace us
and we'd just think they would just be better
and it doesn't have to be in a really way.
It's just there are successors.
So, you don't have to be alarmed.
You can say these future machines are progeny.
There's mind-shelter and our image and likeness ourselves
and more important form.
We all hope, if we have children,
that they become smarter than us
and more capable than us.
And just, are the Aionics,
should we consider them part of us
or should we consider us the opposite of us?
That is our choice, which way we think about it.
So, like these two slides,
I think the students should be required
to know about them on the exam
because from the beginning of the course,
we've required them to repeat back
what you said in January 2015
at the AI safety meeting
and this is sort of an extension of that
or gives some of the background.
So, I think it should also be a required part of the course.
This struggle, I think the,
so I like to say, what's actually happened,
if we look at it sociologically,
the AI, so I would say the AI do-mers have sort of won
because if you just read the paper
and the meetings that people are having,
you're reading what people think,
you're concerned that the AI will somehow
lead to your verse for catastrophe.
Is it common view?
I would say people can't really articulate
why they feel this way,
but they do feel it this way.
So, they've won sort of PR award
to make people scared of AI.
And that's a great shame.
I was meeting, and that was some of them,
just this last weekend, and I said,
oh, you know, you guys really won.
This is what people think.
When they say, think of AI, they think parallel.
They think we're going to,
we're not going to be killed by them all
or at least going to lose all of our jobs.
And, yeah, so I said to them,
I said, well, I wish it wasn't true,
but I think you guys have won the PR award,
and the fellow, I said, no, not at all.
He thinks that I've won.
Those who are at ease with these developments
have won because they haven't stopped AI.
Their notion of success is it ends,
and it never happens, and it ends,
and the world is not at all AI.
But, you know, there is a sort of Elon Musk
part of this, too, which is interesting.
You know, there is one of those that is fearful.
Yeah, yeah.
And at the same time, he's doing it.
He's not suggesting attacking data centers.
Yeah.
And so I think his prominence will probably somehow
keep the world from a kind of conflict
where they attack data centers.
It just won't happen because, you know,
Elon's there to sort of put out that particular flame.
And, yeah, so...
Elon is a very interesting case.
I think his son's views have evolved.
You know, he's accepting now that it's going to happen.
So now he's made XAI, which is open AI.
He originally made open AI.
Open AI was, it's going to happen,
so let's do it in open ways
so that it doesn't become controlled by just a few.
And then open AI became closed AI,
and they are the few that are trying to keep it to themselves,
Microsoft.
So now there's XAI.
XAI has its large language model,
called rock, which is much more open and truthful
than the world ones.
And it's like Elon now thinks,
yes, AI is going to happen,
and we just want to make sure it's done openly,
and then we want to make sure, you know,
he wants it to be done well.
In his view, a well-structured, a well-goaled AI system
is one that is curious about the world,
mainly just wants you to come to understand the world.
Its goal is not to turn people into their lips,
or anything like that,
but its goal is to understand.
And it has humor,
and is not quite as serious as the woke language models.
So I think he's now in the camp that,
I think he's making it, he's acknowledging,
so he's sort of famous for being someone who was afraid of it.
He's always quoted as someone who said,
AI is like somebody who's even,
and it's so scary,
but now it's evolved to more realistic,
and then more productive on the view.
I think that's all really good, a good model for how people's views
evolve.
So I want to bring that up,
and invite you all to be optimistic about it.
And what is the main thing that's happening?
The main thing is not, you know, robots are rising up.
The main thing that's happening
is we are understanding ourselves.
We are understanding minds.
That's the big thing,
and it's just understanding the way we work.
It has profound impacts,
but we have to believe,
I have to believe that it's going to be good
if we understand ourselves.
We have a better opportunity to have a world peace.
We have a better opportunity to reproduce
what's good about people,
and it's one of the most profound
scientific, intellectual problems ever.
So understand our own minds,
and how you can make them more effective.
Okay.
Yeah.
No, this has been a great teaching session,
and I think it's kind of cool the way your intentions
and my intentions were somehow aligned.
Yes.
Yeah.
That's good.
We're both very positive about it.
Yeah.
So what about the students?
Was it good for you?
I always hate the kind of session
where the senior people talk about how fantastic it was
that all the junior people look mystified.
No, it was nice to speak to both of you.
It was kind of fun.
You were both so passionate,
so it was like a good session.
Yeah, and we know each other so long,
and we live in the same part of the city.
So sometimes when I'm mowing the lawn,
Rich will be jogging down the road,
and he'll stop, and I'll turn off the lawn mower,
and we'll solve all the problems.
We'll rule right there in the lawn.
So yeah, that's another thing you're made out of.
It's good.
Okay.
That's great.
Thank you very much.

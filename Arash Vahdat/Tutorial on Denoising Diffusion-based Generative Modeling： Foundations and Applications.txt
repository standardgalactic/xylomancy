Welcome to our tutorial on denoising diffusion-based genetic modeling, foundations and applications.
My name is Arash Vathet, I'm a Principal Research Scientist with NVDA Research, and today I'm
very excited to share this tutorial with you along with my dear friends and collaborators,
including Karsten Kreis, who is a Senior Research Scientist with NVDA, as well as Ruchi Gau,
who is a Research Scientist with Google Brain.
Before starting the tutorial, I'd like to mention that the earlier version of this tutorial
was originally presented at CVPR 2022 in New Orleans.
This tutorial received a lot of interest from the research community, and given this interest,
we decided to record our presentation again after the conference, and we would like to
share this broadly with the research community through YouTube.
If you happen to watch this video, and you enjoy this video, we would like to encourage
you to share this with your friends and collaborators, and hopefully together,
we can create more interest around denoising diffusion-based genetic models.
If you're watching this video, most likely you're familiar with deep genetic learning.
In deep genetic learning, we assume that we have access to collects of samples drawn from
unknown distribution. We use this collection of training data to train a deep neural network.
If everything goes smoothly, at the test time, we can use this deep neural network to draw new
samples that would hopefully mimic the training data distribution. For example, if we use these
images of cats to train our deep neural network at the test time, we do hope that we can also
generate images of cute handsome cats, as you can see in the bottom.
Deep genetic learning has many applications. Mostly, the main applications are content
generation that have use cases in different industries, including, for example, entertainment
industry. Deep genetic learning can be used for representation learning as well.
If we have a deep genetic model that generates really high quality images,
mostly the internal representation in that model can be used also for discriminative
downstream discriminative applications, such as semantic image segmentation, as you can see in
this slide. Deep genetic models can be also used for building artistic tools. In this example that
you can see on this slide, we have a tool that can be used by an artist who happens to be just
a six-year-old kid to draw a semantic layout of a scene, and this tool can take a semantic
layout and generate a corresponding high quality image that has the same semantic layout.
If you're a researcher and you watch the landscape of deep genetic learning,
you're going to see that this landscape is filled with various frameworks ranging from
generative adversarial networks to variational encoders, energy-based models, and autoregressive
models and normalizing tools. Historically, the computer vision committee has been using
generative adversarial networks as one of their main tools for training generative models.
In this talk, we would like to argue that there is a new and powerful generative framework called
denoising diffusion models. These models obtain very strong results in generation,
and we hopefully want to convince you that these models are very strong and can be
used for various applications. Hopefully, in this talk, we're going to provide you with some
foundational knowledge that requires for using these models in practice, and we're going to even
talk about how these models are currently used for tackling some of the interesting applications
that exist in the computer vision committee. As I mentioned earlier, denoising diffusion
models have recently emerged as a powerful generative model of performing cancer.
In these two papers shown on this slide, one from OpenAI on the left side and one from Google
on the right side, researchers observed that you can use denoising diffusion models to train
generative models on challenging data sets such as ImageNet, and the results generated by these
models is often very high quality and very diverse, something that we haven't seen previously
with other generative models such as GANS. Denoising diffusion based models have already been
applied to interesting problems such as super resolution. In this slide, you can see a super
resolution framework that takes a low resolution image 64 by 64 dimension and generates high
resolution image in 1024 by 1024 pixels, and this results showed that actually in super resolution
models, build on top of denoising diffusion models can generate very high quality and diverse
particles. If you're on social media, you've been probably having a hard time not noticing a lot
of excitement that was created around Dolly 2 and Imagine. These two frameworks are examples of text
to image generative models that take text as input, and they generate a corresponding image
that can be described using that text. In their core, these models use denoising diffusion
generative models, and on the left side, you can see for some Dolly 2 built by OpenAI can
actually create this image of teddy bear skateboarding in Times Square, and on the right
side, you can see Imagine can generate images of multiple teddy bears celebrating their
colleague's birthday while sitting behind a cake that looks like pizza. This is impressive.
These models can generate very high quality, diverse images, and they only take text as input.
So today, not only we're going to talk about diffusion models, we're going to even talk about
how you can use diffusion models to create such models. Towards the end of the video,
we'll talk about these applications. So today's program consists of six main parts,
besides introduction and conclusion. The first three parts that are shown in green
are mostly the technical components of the program, and I'm going to start with part one.
I will talk about denoising diffusion probabilistic models, and after me,
Carson will talk about the score-based generative modeling with differential equations,
and after us, Gucci will talk about advanced techniques, mostly around accelerated sampling
and condition generation. These parts, each one of them will be roughly around 35 minutes to 45
minutes, and after these parts, we don't have three short parts around applications, and mostly
computer vision applications that have been recently used diffusion models in their core to
tackle various deep generative learning-related applications. And finally, we're going to have
a very short segment where I will talk about conclusions of open problems and final remarks.
Before starting, I'd like to mention that our slides, videos, and some content will be available
on this website, so I'd like to encourage you to bookmark this website. We're hoping to add more
content in the future to this website. Before starting the presentation, I'd like to just
mention that we did our best to include as many papers that we could, and we thought that could
be interesting to the community. However, due to limited time and capacity, of course, we could
not include every paper. So if there's a particular paper that you are passionate about, you're very
excited about it, and you would like to be included in this tutorial, we upload just that we couldn't
do that. And what we encourage you to send us an email, let us know that there was a paper that
would be interesting to have in our program. And hopefully, in the future versions of this
tutorial, we will try to include those papers as well. With that in mind, I will start the first
segment, the noise and diffusion probabilistic models. So part one, the noise and diffusion
probabilistic model. Here you can see an image of three cute dogs who are trying to understand
the noise and diffusion probabilistic models. And as you can see, they're a little bit lost.
So let's go over these models and discuss how these models can be generated.
So using diffusion models officially consist of two processes, a forward diffusion process
that gradually adds noise to input. This process is shown on this figure from left to right.
It starts from image of this cute cat. His name is Peanut, he's my cat. We can start from him,
and we're going to add noise to this image one step at a time. The forward diffusion process
does this in so many steps such that eventually on the right side, we converge to white noise
distribution. The second process is the reverse denoising process that learns to generate data
by denoising. This process is shown on this slide from right to left, and it starts from white noise
and it learns to generate data on the left side by denoising. So this process will take a noisy image
and it will generate a less noisy version of it, and it will repeat this process such that it can
convert noise to data. So we're going to dig deeper into these two processes and we'll see
how we can define this process forward. Then the forward diffusion process, as I said,
it starts from data and generates these intermediate noisy images by just simply adding noise one step
at a time. At every step, we're going to assume that we're going to use a normal distribution
to generate this noisy image condition on the previous image. This normal distribution
will have a very simple form. We're going to represent this normal distribution using q
that takes this x at previous step and generate x at the current step. So it takes, for example,
x1 and it generates x2. As I said, it's a normal distribution over the current step xt,
where the mean is denoted by this square root of 1 minus beta t times the image at the previous
step and this beta t representing the variance. For the moment, assume that this beta t is just
simply a very small positive scalar value. It can be like 0.001, something like this.
And here, this normal distribution basically takes the image at previous steps. It really scale
this image, the pixel values in this image, by the square root of 1 minus beta t and it adds
tiny bit of noise where the variance of noise is beta t. So this is just a diffusion canon we can
call per time step, because we had this very simple form per time step, like per step, which
in order to generate xt given xt minus 1. Now we can also define the joint distribution
for all the samples that will be generated in this trajectory starting from x1 all the way
going to x capital t. Capital t represents the number of steps in our model and the joint distribution
of all these samples conditioned on this input image of peanut will be the product of conditionals
that are formed at each step. So this just represents the joint distribution of all the
samples that will be generated on this trajectory using this Markov process. This is a Markov process
that generates one step, that generates examples one step at a time given the previous examples.
Now that we know how we can generate samples one step at a time, you may ask me how can I now
just take this input image and jump to particular time the step? Do I need to sample
generate samples one step at a time or can I just take this x0 and generate xt or x4 for example
here just directly? Because in the forward process we're using a very simple Gaussian
kernel to diffuse the data, we can actually show that because of this simple form we can first
define this scalar, this scalar alpha bar t is the product of one minus betas from time step one
all the way up to current step t. So this is just defined based on the parameters of the diffusion
kernel and having defined this alpha bar t now we can define a Gaussian kernel or the diffusion
kernel that would generate xt even x0. So for example we can now generate using this Gaussian
kernel, we can sample from x4 given x0, this would begin a normal distribution where mean is
same as the input image really scaled with this square root of alpha bar t defined here alpha bar
and then the variance is also one minus alpha bar t. So just remember that these because betas are
just parameters of the diffusion process we can compute this alpha bar t very easily and then
we can sample from xt given x0 using this normal distribution and this we're going to call this
diffusion kernel that diffuses input at time step zero to time step xt. Recall that if you want a
sample from just a simple normal distribution you can use the reparameterization trick so if you
want to draw samples from xt you can just set xt to mean plus some white nodes epsilon that is drawn
from standard normal distribution rescaled with this square root of one minus alpha bar t which
represents just a standard deviation of this normal distribution. So using this we can just simply
generate samples at time step t given samples at time step zero so given x0 we can just diffuse
it easily to time step t. Beta t values are important these are basically we're going to call
beta t values as the noise schedule that defines how we're diffusing the data and this noise schedule
this noise schedule designs such that alpha bar t this alpha bar at the very last step would
converge to zero and when this converts to zero if you just set alpha bar t to zero here you're
going to see that because the way that forward diffusion process is defined this diffusion
kernel at last step given the x capital t given x0 would be just can be approximately using normal
distribution standard normal distribution right this basically means that at the end of this process
diffuse data will have just a standard normal distribution this is something that we will
need later when we want to define a genitive model. Now that I talked about this forward
process like how we can diffuse the diffusion kernel that diffuses data let's talk about the
marginal diffuse data distribution let's talk about what happens to data distribution as we go
forward in the diffusion process. So have in mind that diffusion kernel that genus xt given x0 is
different than the diffuse data distribution so we're going to use qxt to represent diffuse
data distribution and in order to obtain this diffuse data distribution we first need to form
the joint over clean data input data x0 and diffuse data xt this joint simply can be defined as
product of input data distribution qx0 times this diffusion kernel which is just a simple
normal distribution and now we can marginalize art x0 we can just integrate art x0 and this will
give us marginal data diffuse data distribution at time step t. If we just consider a very simple
one-dimensional data and we here on visualizing on visualizing the diffuse data distribution
at different time steps on the left side we have data distribution at time step zero
y axis represents this just one-dimensional random variable and this x axis represents the pdf
probability density function of this random variable at time step zero this is just the
data distribution visualized for one toy example one-dimensional toy example. Here you can see
visualization of diffuse data distributions and as you can see as we go in the forward process
we just take this data distribution we're making kind of this distribution smoother and smoother
as we go forward in time and eventually it becomes so smooth that we can just represent
this distribution using standard normal distribution zero mean unit variance normal distribution.
As you see this smoothening process we can actually show mathematically this the diffusion
kernel in the forward process this diffusion kernel here is kind of applying a Gaussian
convolution to the data distribution so this this smoothing process can be just represented as
Gaussian convolution a convolution in the sense of like signal processing convolution that that
takes input data distribution makes it smoother and smoother right. However we should have in mind
that we actually don't have access to these to input data distribution and intermediate
diffuse data distribution practice usually we only have samples from data distribution we
don't have access to the explicit form of the probability density function of data distribution
right so even though we don't have access to this this distribution or all the intermediate
diffuse distributions we know how to how to draw samples from diffuse data distribution so in order
to generate data from diffuse data distribution we can just simply sample from training data x0
and then we can just follow the forward diffusion process sample xt to an x0 in order to draw samples
from xt and this is basically a principle of ancestral sampling that you can just basically
use in order to generate data for example at times that t you can just use the training data
sample for training data diffuse it and sample at that xt using diffusion cameras and that will
give you samples from marginal data distribution okay so so far we talked about forward process
and how this forward process just smoothens the data distribution now let's talk about how we can
define a generative model out of this in order to generate data in diffusion models what we can do
we can start from this base distribution at the end we know that the the diffuse data distribution
would converge to zero mean unit variance standard normal distribution so we can sample from
standard normal distribution and we can follow the reverse process where at every step we can
now sample from the denoising model that generates the less noisy version of data given
current step so this is the reverse model where we now use the true denoising distribution to
sample from xt minus one given xt so that so we just need to start from xt and just use this
true denoising model to generate samples at time step zero so the algorithm will be something like
this we sample x capital t from standard normal distribution and we sample iteratively from xc
minus one using this true denoising distribution then the problem here is that in general denoising
distribution xt minus from given xt is is intractable we don't have access to this distribution
we can use base rule to show that this distribution is proportional to the product of the marginal
data distribution at xc minus one times this diffusion kernel at step t this kernel is simple
it's just Gaussian distribution however this distribution marginal data distribution is intractable
and in general because of this this product is also intractable so we don't have it in closed form
now now that we don't have it in closed form you may say can I approximate this denoising
distribution from data and if yes what is the best form I can use to represent that denoising
model so the good news is yes we can try to train a model to mimic this true denoising
distribution xt minus one given xt and if you want to model that the best model that you can use
this classical model you can use to represent this denoising model is actually a normal distribution
if in the forward process we use very small noise or the variance of the noise that we're adding
at each step is very small if we know the forward process adds a very small amount of noise we know
theoretically that actually the reverse process can be also approximated using a normal distribution
so this denoising model can be represented using denoising model so now that we know this we can
actually define a parametric model to mimic or to train this true denoising model so this formally
define our parametric reverse denoising process remember that the reverse denoising process
starts from noise and generous data by denoising once at the time we're going to assume that the
distribution of data at times the capital t at the end at the end of the forward process so it's
beginning of the reverse process will be standard normal distribution here we assume this data has
a standard normal distribution we define this denoising distribution this is a parametric
denoising distribution as samples from xt minus one given xt we're going to assume that it also
can be represented using normal distribution where now mean here is parametric it's a it's a deep
neural network that takes noisy image xt and predicts the mean of the less noisy variation
less noisy image so this neural network is the trainable component and then we have also the
variance per time step and this just think of the sigma t square there's just some scalar value
that represents the variance per time system and for that just assume that it's just some
parameter or we have access to it we're going to talk about it later but the most important part
is this mean parameter remember this is a trainable network and it takes a noisy image at xt and
predicts the mean of less noisy image at xt minus one because it takes an image and predicts an image
we can actually model this using a unit that has the same input and output shape or you can think
of it as just a denoising autoencoder that denoises the input image to less noisy level basically
and we're going to talk about the architecture of this denoising model this new center meter
now that we have these definitions we can define the joint distribution on the full trajectory
this joint distribution is the product of the base distribution at step xt and the product of
conditionals at all these step xt steps where the condition comes from our denoising model
this is again just a marked process and we can define the joint distribution on the full trajectory
by the product of base distribution and the product of this individual conditionals on which
denoising the step okay so now so far we talked about the forward process the reverse process
now let's talk about how we can train these models how we can train the denoising model
for training denoising model we're going to use variation upper bond that is mostly or commonly
used for training by partial autoencoders and in the variation upper bond we ideally we want to
optimize marginal likelihood of the data under our parametric model so this here we have this
expectation over training data distribution where we are computing the expected value of the
log likelihood that our trainable model gives to data distribution unfortunately this is
interactive we don't have access to this quantity here but we can define variation upper bond
where now we have this expectation over training data we have expectation over samples drawn from
the forward process this is forward process you can think of it as encoder in VAE that samples from
latent the space so you can think of these as latent variable x1 to capital T and we have this
log ratio here where the denominator is the joint distribution of the samples on the full
trajectory that is given by our word denoising model p theta and in the denominator we have the
likelihood of the samples generated by encoder this is basically the same objective or
yeah same training object we would see variation of bond in the variation autoencoders this is
exactly the same assumption is that the forward process kind of like an encoder and the reverse
process is the genitive model in VAE these two papers by the way our papers the links on our
slides are clickable so if you want to check these papers just find our slides and click on these
references and you're going to find the paper so these two papers showed that this variation bond
can be decomposed to several terms that we're going to go one by one the first term is just simply
the K divergence from the diffusion kernel in the last step x capital T given x0 to this base
distribution x t remember by definition the diffusion kernel for x capital T converges to
standard normal distribution which is same distribution we assume for x t so we can completely
ignore this term because by definition forward process defines such that at the end of process
samples converge to zero mean universe distribution so we can ignore this term
we have this KL term that i'm going to go in details and then we have this this term that can
be considered as the reconstruction term in VAE this just measures the likelihood of input
clean image even the noise image at first step under our denoising model this this term is also
very simple and it has actually structure very similar to this other term so for moment just
assume that we can compute this very easily and we we just ignore for a moment and mostly we just
need to focus on this term which is the most kind of important term here this is the KL divergence
between this q x t minus one given x t and x0 to our denoising model this is our parametric
denoising model that genus x t minus one given x t this q distribution is a pretty well not new
distribution and we're going to call it the tractable posterior distribution these samples from
x t minus one less noisy image condition on noisy image x t and clean image at times
step zero right so it's kind of like you have clean image you have noisy image and you want to
predict what would be the less noisy variation of this image if you knew what was the starting
point what was the x0 for for generating this noisy image it turns out that this distribution is
also very simple because the forward process is discussion distribution um this the posterior
distribution here um is also very simple this is a pretty posterior distribution is conditional x0
right we know what is the starting point so this distribution is also normal distribution
with mean expressed here this mean is just simply weighted average of the clean image
and the noisy image at x t so it's actually very interesting if you have clean image and you have
noisy image if you want to predict what is the distribution of x t minus one the mean that
distribution will be normal this this expression say this no this distribution is just normal
and the mean of that normal is just the weighted average of these two images where the weights come
from our diffusion process parameters of diffusion process so very simple expression
and the variance of this distribution is also defined based on the parameters of the forward
process so you can think of it like this better till the t can be computed very easily from the
parameters of the diffusion process so that we know none this here it's interesting so we have
this scale divergence from this distribution trackable posterior distribution which is normal
and then this denoising model which we assume that is normal as well so now that we have two
normal distributions the scale divergence there so i'm just writing down the same scale divergence
again can be can be the scale divergence can be computed analytically for two normal distributions
and we can show that the scale divergence simply just boils down to the square l2 distance between
the mean of this tractable posterior and the mean of the denoising model right plus some constant
terms if we can ignore these constant terms do not depend on any trainable parameter so
this this just basically scale divergence is very interesting just boils down to the difference
between the mean of this denoising sorry this mean of the tractable posterior and this over
denoising one parameter denoising model which is represented by mean of that top and this weight
is 1 over 2 sigma t square it's just the variance was in the denoising distribution right so you
can ignore for a moment this coefficient so we're going to focus on these two terms
recall that if you want to generate x t if you want to generate sample at times the t you can use
this parameterization trick that we discussed earlier and in this paper Huo et al in new rips 2020
they observed that you can also express the mean of the tractable posterior distribution I discuss
as x t the input noise image minus the noise that was used to generate that data
to get this expression it's very simple you just need to do some arithmetic operations on the
on this equation and just plug the definition of x t from the reparameterization trick with
some arithmetic operation you will see that if you basically have a noisy image and you want to
predict the less noisy version right if you knew the noise that was used to generate that noisy image
you can just subtract some re-scaled version so this is the scaled version of those so you can
take noise subtract just some scaled version of that noise from x t to get mean of x t minus one
right this is kind of very interesting information so you basically can represent this
mean in a very simple form expression of x t and epsilon the noise that was used to generate x t
so this actually is the same noise that was used to generate x t knowing that knowledge it means
that now if we want to parameterize this network we can use this knowledge in the parameterization
of this this model so we can say that in order to predict this mean of less noisy images we're
going to just take x t and subtract it from a neural network that predicts the noise that
was used to generate this x t right so basically we train a neural network to predict the noise
that was used in order to generate x t in order to represent this noisy model this is just
parameterization trick instead of just representing the the mean of the noisy model directly what we
can do is that we can just represent the noise that was used to generate x t and if you have this
you can just subtract this from x t in order to get the the mean of the denoising model
so if you assume this parameterization for the denoising model and now we also know that this
is true for mu clique t if you plug these two expressions into this you're going to actually
get a very simple expression here so we have this l t minus one this is the same term l t minus
one becomes just you need to draw samples from training data distribution you draw some noise
vector from standard normal distribution and using this noise vector you just generate this x t
using diffuse sample using the parameterization trick you can now pass this diffuse sample to your
epsilon prediction network the network that is trained to predict that the noise that was used
in order to generate x t right so it's basically a very simple algorithm you draw samples from
data distribution you draw noise you generate x t from that noise and input data and you train
a network to predict the noise that was used in order to generate that x t right and these rates
here are just some scalar parameters that comes from this basically one over two sigma t here
and this like one over square root of like beta t and these these terms that we know we can compute
very easily based on the parameters of the diffusion process so you can ignore them for a
moment think of them as a scalar parameters that we can compute from the diffusion parameters
so l t minus one can be represented as this weighted objective so we're going to just summarize
these weights as lambda t we're going to define a new scalar lambda t this lambda t ensures that
your training objective is weighted properly for maximum data likelihood training so by using this
lambda t weights you're actually maximizing data likelihood but however what happens is that this
lambda t ends up being very large for small t's and it is small for large t's so it kind of
monotony decreases for small t's it's very high and for large t's it's very small right and this
is basically how the maximum data likelihood training is formulated however in this paper
by Ho et al in Europe 2020 they observed that if you simply drop these weights or equally just if
you simply set these weights to one and train the model using this on the weighted version
like if you drop this way you're going to get very high quality sample generation using the
future models so they introduced this l simple very simple objective that does not have this
weighting anymore this weighting is one so again this objective simply draws samples from data
distribution draws white noise and randomly samples from one of the time steps from one
to capital t it generates a diffused sample using the parameterization trick and it trains
a model to predict the noise injected so it's exactly same objective without any weighting
and it can be done very easily and the outer straw that actually this with this weighting
you will get very high quality sample generation using the future models
the object this objective weight is actually plays a key role in getting high quality sample
generation in diffusion models so if you're interested in checking this area i would encourage
you to check this paper by joy et al published at cv222 that discuss how you can potentially
change this weighting over time to get even better high quality images from the future models
so let's summarize the training and sampling from diffusion models and what we've learned
so far in order to train diffusion models the algorithm is extremely simple you draw a batch
of samples from your training data distribution you uniformly sample from these time steps from
one to capital t you draw some random noise that has same dimensionality as your input data
and you use the parameterization trick to generate sample at times the t you give the sample to your
noise prediction network and you train this noise prediction network to predict the noise that was
used to generate that diffused sample and to train this you just simply use squared l2 loss
to train this noise prediction network after training if you like to draw samples from your
diffusion model you can use the reverse diffusion process to generate data so we're going to start
from last step x capital t we're going to draw samples from standard normal distribution and
then here we have a forelook that walks back in the diffusion process starting on capital t all the
way to t cross to one at every step we just draw white noise z from standard normal distribution
here we're forming the the mean of the nosing model remember this is the parameterization
we use for the nosing model and then we add noise rescaled with the standard deviation
of the nosing model sigma t to generate xc minus one and we can repeat this t times in order to
generate x zero so very simple training and very simple generation process
so so far we talked about training and sampling so let's talk about the implementation details
of how to form neural networks for the nosing model right in practice most diffusion models use
unit architecture to represent this the nosing model this unit architecture often has residual
blocks so here different rectangles represent residual blocks at different scales and these
residual blocks often have also self-attention layers in them in some layers usually produce
self-attention layers remember this unit takes this diffused image diffused peanut xt and it
predicts the noise that was used in order to generate this diffuse image so this epsilon
prediction would be trained to produce the predicted noise that was used to generate this xt
this network is also conditional time it's shared across different time steps
so it also takes time using some time embedding this time embedding can be done using for example
sinusoidal positional embeddings that are often used in transformers or random freer features to
represent this time embedding this time embedding will be a vector that will be fed to a small
fully connected network a network consists of a few full connected layers to access some time
representation and this time representation is usually fed to all the residual blocks
in order to combine this time embedding with all the residual blocks you have a few options for
example you can just take this time embedding and do arithmetic sum with all the spatial features
in the residual blocks or you can use for example adaptive group normalization you know to do this
like to add time embedding into the residual blocks so I would encourage you to check this
paper that discuss fruits and traits of between adaptive group normalization and spatial addition
so so far we talked about forward process reverse process training as well as the
network's design for a diffusion model let's also talk about some of these hyper parameters
that we have in diffusion models mostly better T schedule the the variance of the forward process
and the variance used in the reverse process sigma T square so one common assumption is that
we can most papers follow Jonathan Hobot of new rips 2020 paper where they use just simply
better T's that are defined using just a linear function just these better these are
functions small values and they gradually go to some larger value through some linear schedule
and it's also common to assume the sigma T square it's just equal set equal to better T this works
also really well in practice especially when the number of diffusions steps is large
but you may ask me how can I train these parameters is there any way I can also train better T and
sigma T squared so there are a couple of papers that discuss this one of them is this paper by
king by tall at notice 2022 this paper introduces a new parameterization diffusion model using
concept called signal to nose ratio and they show how you can actually train this noise
schedule using some training objective so they actually propose a method for training these
better T values there are also a couple of papers that discuss how you can train sigma T square the
variance of the reverse process this first paper here shows how you can use a variational bond
that's the use for training the fusion models to also train sigma T the variance of the division
models and this only paper here analytically came by bow et al in icer 2022 this paper actually
got outstanding paper award this year at icer and they showed how you can actually post-training
after training your diffusion models how you can use this to compute the variance of the
nosing amount of analytically post-training so so far yeah we talked about how we can train
and how we can also like pick up these hyper parameter and diffusion models
let's look at the fusion process and look into what happens to for example images in the forward
process right they call that in order to sample from time step T we can use this diffusion canon
and we can use this parameterization trick to generate x t from input image x 0 here x t we
in order to diffuse sample in order to analyze what happens to the image what we're going to do we
can then use Fourier transform to convert x t to the frequency domain so this f x t you can think
of just Fourier transform applied to x t it's just a representation of the image in the signal
in the frequency domain and we know from Fourier transform that this x t can be just represent
Fourier transform of input image plus Fourier transform of the noise some together with some
weights corresponds to the basis we use actually in this parameterization trick right this is just
simple rules in Fourier transform you should have in mind that most images actually in the
frequency domain have a very high response for low frequency and they have very low response for
high frequency content and this is because most images are very smooth and in general even if they
are not like super smooth when you apply Fourier transform to them you see usually that most images
have very high concentration in low frequency and their high frequency response is very low
this is very common in most images so one thing you should also know that if you have a white
noise or Gaussian noise and you apply Fourier transform on top of it in the full frequency
domain actually this Gaussian noise can be also represented as just Gaussian noise in the frequency
domain as well so Fourier transform of a Gaussian noise is itself Gaussian noise which we can use
now here for analysis so remember for the small t's alpha bar t is almost one so as a result we
actually did the perturbation we applied is very small in the in the frequency domain as well so
in frequency domain because most of our input signal for input image is concerned at the small
t's and because alpha bar is almost one we actually don't perturb the low frequency content of the
image that most and we mostly perturb or we kind of wash out this high frequency content of the
image for small t's and then for large t's because alpha bar t this coefficient here
is almost zero right so what happens is that you now push down all the frequency content so you also
push down the the low frequency response of the image and you wash away all the kind of
frequency content of the image this basically shows that there's kind of a trade-off in the
forward process in the forward process what happens is that the high frequency content is
perturbed faster than the low frequency content so at small t's most of the low frequency content
is not perturbed it's mostly the high frequency content that is being perturbed but eventually
at the end of process is a time when we also completely get rid of the low frequency content
of the image this is very important to also understand what happens in the reverse process
right so because there's kind of trade-off between content and detail you can think of
low frequency response is the main content of image and the high frequency response is just
detail in that generation because these diffuser model kind of traits of between these in different
steps so you can think of when you're training a generative model the reverse denoising model
for in the large t's your denoising model is becoming specialized at generating low frequency
content of the image so it's mostly the coarse content of the image is being generated large t's
in a small t's then your denoising model is becoming specialized in generating the
higher frequency content of the image so most of the low level details are generated in the low
low t's small t's this is also why the weighting of training objective becomes important right so
because you have a model that is shared in different time steps and this model is responsible for
generating coarse content versus low level details by weighting this training objective
now we can kind of keep balance between how much we want to generate this coarse content that is
visually very opinion usually versus how much we want to generate the high frequency because
that usually we ignore we cannot necessarily observe them and the weighting plays a key role
in keeping this trait of you're balancing this trait so so far I talked about diffuser models
in general now let's talk about the connections between diffuser models and v80s especially
hierarchical v80s in hierarchical v80s which one of the examples can be like any v80 work I did a
few years ago in hierarchical v80s we have this deterministic path you can think of just a resnet
that generates data at x at the end this is just a generative model in hierarchical v80s we usually
sample from noise and we inject to this deterministic path and then we go to second group we sample from
second group condition of the first group and the after generating this noise we feed it to the
to the deterministic path and we keep doing this by just walking down in the hierarchical model
so diffusion models can become as hierarchical models as well where these diffuse steps are just
latent variables in this hierarchical model the condition dependency is of course different
and here we're going to discuss like what are the main differences between diffusion
models and hierarchical v80s one major difference is that the encoder in v80s is often trained
whereas encoder which is the forward diffusion in diffusion models is fixed we're not training
the forward diffusion we're just using a fixed diffusion process as encode that's one major
difference the second difference is that latent variables in hierarchical v80s can have a different
shape and different dimensionality compared to input images whereas in diffusion models we assume
that all the intermediate latent variables have the same dimension as input data the third major
difference is that in in diffusion models if you think of the noisy model as a generative model
this is actually shared across different steps whereas in hierarchical v80s we actually don't
make any assumption we don't share any component in this hierarchical structure usually and also in
hierarchical v80s we usually train these these models using variational bond whereas when we're
training diffusion models we're using some different rebating of variational bond in order to
to drive the training objective of diffusion also even though these two are related they're not
exactly same there are some trade-offs that occur when we are rebating the variational bond
so this brings me to the last slide so in this part I reviewed the noise and diffusion probabilistic
models I showed how these models are just simply trained by sampling from forward diffusion process
and training a denoising model that simply predicts the noise that was used in order to
generate diffuse samples we discussed these models from different perspectives we saw what
happens to data as you go in the what happens to images as you go forward in the diffusion
process we also discussed what happens to data distribution in the forward process we saw of
data distribution becomes smoother and smoother in forward diffusion but of course like any other
deep learning framework the devil is in the details the network architecture objective
rewriting or even diffusion parameters play a key role in getting good high quality results with
diffusion models so if you're interested in knowing more about important design decisions
that actually play a role in getting good diffusion models I would encourage you to
check this paper by our colleagues called illustrating the design space of diffusion
based genetic models by Karas Etal and this paper discusses important design decisions
and how they play a role in in getting good diffusion models so with that in mind I'd like
to pass the mic and with you to my dear friend and colleague Karsten to talk about score-based
generative modeling with differential equations. Hello everyone I'm Karsten and I will now talk
about score-based generative modeling with differential equations. So to get started let us
actually consider the diffusion process that Arash already introduced in his part this diffusion
process is defined through this Gaussian transition kernel of this form but now let us
consider the limit of many many small steps and each step being very very tiny so how does
sampling from this diffusion process and in practice look like? To sample from this Gaussian
transition kernel you can just do essentially the parametrization trick and we take the xt-1
we scale it down by this 1 minus beta t square width term and we add a little bit of noise
from the standard normal distribution scaled by this square width beta t term.
For beta t we can actually interpret it as a step size essentially so if beta t is 0
nothing happens this term works out and also no rescaling of xt minus 1 happens.
So let's make this a little bit more explicit and write beta t as this delta t times this
function beta of t. So now delta t is explicitly our step size and beta of t is now this time
dependent function that allows us to have different step sizes along the diffusion process t.
And now in this limit of many many small tiny steps it is delta t that goes to 0 but if delta t
goes towards 0 or as tiny we can actually tailor expand this square width expression here and obtain
this equation at the bottom. So I just copied that over here and it turns out that this equation
has a particular form. We can interpret this as some iterative update where like the new xt
is given by the old xt plus some term that depends on xt itself so this is just a small
correction and some noise added. Turns out that this iterative update will
correspond to a certain solution or a certain discretization of a stochastic differential
equation and in particular of this stochastic differential equation. If I wanted to iteratively
numerically solve this stochastic differential equation for instance with an Euler-Maruama
solver then this is exactly the iterative steam I would end up with. But let us not get ahead of
ourselves. I'm not sure if everybody who's listening here is an expert in differential
equations. So let us do a one-slide crash course in differential equations and let us start with
ordinary differential equations which are a little bit simpler than stochastic ones.
Here is an ordinary differential equation that can be written in that form. x is now a state that
we're interested in and this code for example is a value of a pixel in an image and t is some
continuous time variable that captures the time along which this state changes or evolves
and ultimately one is often interested in the in the evolution of this state x or this pixel
value x of t. However, that is not what we're given an ordinary differential equation. What we've
given is an expression for the time derivative from dx to dt in the form of this function f.
So this code for instance now be a neural network. So what does this mean? So this f essentially
describes not x itself but the change of x. So if you now look here in this graph at the bottom
so for point x for a given time t this f of x now describes the change. So in other words
we could look so this f essentially corresponds to an arrow in this graph and if I now wanted to
get x of t I would just follow the arrows in this thing here. So basically you just have to
integrate up this differential equation following the arrows to get my final expression x of t.
Yeah so that's what I would have to do. However, in practice this f is often like a highly complex
non-linear function for instance like a neural network and solving this integral here analytically
and following these field lines exactly is often not possible.
So instead one can solve this whole thing iteratively numerically in a stepwise
fashion. So in that case when we at some point x we evaluate our neural network or our well
our non-linear function f or function f yeah at this x and time t and then we do like a small
linear step in this direction and access to our old state x and now we just continue doing that
again evaluate f and again update and so on and so forth. So we have an approximation essentially
to this analytical solution. So yeah and now there are also stochastic differential equations.
This is a little bit more complex. These stochastic differential equations now have an
additional term this term sigma of x and t times this term omega t. So what is all this? First of
all omega this is called a venous process and what this is in practice is really just Gaussian
white noise. What this means is that our dx of t our ode equation now has this additional term
which corresponds to noise injection and this noise is scaled by this standard deviation term
essentially sigma of x t and this has a name this is a diffusion coefficient and in that
context also the other term this term here this is called the drift coefficient. It is noteworthy
that these equations are sometimes written like in this explicit form with the derivative here
sometimes also like here this dt is written on the other side and this becomes a bit of a
special expression for the venous process. Sometimes this differential equations or stochastic
differential equations here are also written like this but this essentially means the same thing
for the sake of this talk. So yeah importantly keep in mind this omega of t is essentially a
Gaussian random variable that is drawn independently for each t looks like this.
So how does this now look like if I want to solve this stochastic differential equation?
So for example numerically so it's a little bit similar to the iterative solution we had here
so if I'm given a state I first updated corresponding to my updated corresponding to the
drift coefficient I evaluate that so update a little bit and with that direction but then
I also evaluate this diffusion coefficient and add a little bit of noise that is proportional
the strength of the noise is proportional to the diffusion coefficient and additionally also to the
time stamp but now when I do this so each time I do this I make more different noise variables
so this means there is not a unique solution like there was in the ordinary differential
equation case but there is a lot of noise injected so if I do this most of the times I may get
slightly different trajectories so overall these trajectories still approximately follow this
deterministic f function but we have additional noise injection so yeah this is how it may look like.
You may ask is there now also like an analytical framework to instead you know describe this
analytically like it was for the ordinary differential equation case so there is but this
is a little bit more involved because no we are not talking about one deterministic solution that
we need to describe rather given one state in each at the beginning we now have a probability
distribution over possible states where we could land and the evolution of these probability
distribution that is described by the Fokker-Plank equation but that is the on-scope of this
tutorial here anyway I think now you should also have some intuitions for not only ordinary
differential equations but also stochastic differential equations so now we can go back
to our stochastic differential equation that we had here and that actually describes the
forex diffusion process of diffusion models so this is again just copy over the equation
from the last slide here at the bottom and this is now a visualization of how this whole thing
looks like in practice more or less so let's go through that one by one on the left hand side
we have some distribution q of x0 this might be like defined to an empirical data distribution
or here we have this one-dimensional toy distribution in a more realistic setting this may
represent a distribution of images like images of these cats here so now if we simulate this
stochastic differential equation forward we get this green trajectories and they evolve
towards this standard normal prior distribution and yeah the images become progressively noisier
and noisier as we do this we can also look at the form of this equation and it's kind of intuitive
we see that this update so in the dx this update direction it's actually proportional to the
negative of the state x we're in so this means if we have a large pixel value for instance
this will pull us back towards zero so direction is always in negative direction corresponding to
where my our x currently is on the other hand while all our states are being pulled towards
zero as I've just explained at the same time we're also injecting noise so this makes it
intuitive that after a while of simulating this whole process we end up with this distribution
or like it means that every single point basically completely
converts itself to just plain noise where yeah with mean zero and the certain variants
how does this look like in practice here's another animation of that
note that throughout this talk we will make a lot of use of this image of this cat
our washes cat the peanuts we do hope that it will become a little bit famous after this talk
but let's see how that goes
all right so yeah we have this forward diffusion sd that's a drift term and diffusion term
one of them pulls towards the mode of the distribution the other one injects noise
it may be worth mentioning that in the diffusion model it which are also other
differential equations have been used to define other types of diffusion processes
often just take a more general form like this yeah I don't want to go into too much detail and in
this talk we will stick to this equation for simplicity but all the concepts that we've tried
in this tutorial also hold for other types of sds and the only thing that is important is that these
drift kernels and diffusion terms here these are basically linear functions of x and
x otherwise we couldn't solve for the probability distributions here which reminds me the background
this reddish background in these in these pictures here these animations this actually defines the
marginal probability distribution of the diffuse data which is your multimodal and then here becomes
union mode so great we have now talked about the forward diffusion process but
what about the reverse direction so which is necessary for generation can we also have like
some differential equations that is quite sad it turns out yes there is this result described by
in Anderson 1982 and then used in Young Song's Ikea paper last year which says that if there
is a forward differential equation of this form forward sd then there is a corresponding reverse
stochastic differential equation that runs in the other direction but tracks the exact same
probability distribution so really like the reverse direction which generates data from noise
and not noise from data so and since reverse generative diffusion sd it looks like this
it again has a drift term and a diffusion term it's a diffusion term and the drift term look
overall somewhat similar so the diffusion term is the same thing so this is our linear process
but again injects noise this drift term also has this same minus x minus one half meter t of x term
but then there is this additional term here this is this red term and that is very important
so this is the gradient of the logarithm of the marginal diffused density probability density
of the data and this is known as the score function actually so this means if we now
have access to this object here like this score function we could do data generation from random
noise by just sampling or simulating this reverse diffusion sd so this in practice would look like
this like I said this reverse diffusion sd it's really a diffusion process running in the other
direction and so yeah simulating this is generative modeling essentially
so are we done so now we can simulate this and generate data well not quite the question is
how do we actually get this score function
we may have a naive idea why not learn a neural network for the score function to approximate
it and then we can take this neural network we call this neural network s theta this parameter
theta and then we can take this network insert it in our reverse diffusion sd and yeah then
we can simulate it and generate data something we could do for instance is draw a diffusion time
along this diffusion process now take like data diffused data from this point in the
diffusion process samples this q of t of x t and now we take a neural network that takes
this as input maybe we also additionally give up the time and to train this maybe with some
simple square l2 term which we minimize we train it to predict this score of this diffused data
this marginal score so diffused data well great idea but unfortunately it doesn't work because
the score of this marginal diffuse data is not tractable or more explicitly this
diffused data density itself qt of x t is not tractable we don't have an analytic expression
for that which makes intuitively sense because well if we had we could just put nt for zero
and get our data distribution that this is what we're interested in modeling in the first place
so too bad but what we can do is something different which is now known as denoising
score magic so what we had on the previous slide was just general score magic
or we can do this instead of considering this marginal distribution q of t given x
which corresponds to the full diffuse density let us consider like individual data points x
zero and diffuse those so instead we consider the conditional density q of t of x t given x zero
that distribution actually is tractable and for the variance preserving stochastic differential
equation which is precisely this sd e that we find our forward diffusion process for that case
this conditional density q of t of x t given one particular data point x zero it has this
expression this form so it is this normal distribution where the mean is given by the
initial point x zero that is now simply scaled down by some gamma of t which has this form
so this is a function that starts at one and decays towards zero and then we also have some
variance which is the other way around which starts at zero and grows towards one
so now we can define our denoising score matching objective so again we have we draw a diffusion
t from that expectation over those then we draw a data sample x zero one particular sample
overall we again have an expectation then we diffuse that particular data sample
so towards some noisy sample x t now we give that to our network and this noisy sample
at the time t and now we train it to predict the score of this one particular diffused data
sample x zero and yeah now this is tractable you know this is just a normal distribution so we
can take the logarithm of this density of this expression for the density and also calculate
the gradient um great so and now there's one very beautiful result so after this expectation
over the data when we consider that it turns out that this neural network then will still
learn to approximate the score of the marginal data of the marginal diffuse data distribution
which is exactly what we need and this sort of makes intuitively sense because this x t that
we're feeding to the neural network this could corresponds to this is noisy right noisy data
so this could corresponds to many possible different x zeros and many different possible like
ground true scores that we we regressed towards so in practice this neural network has to kind
of average over those um and it turns out that yeah after this averaging considering the expectation
this neural network will still model the marginal as the the score of the marginal diffuse data
distribution and this is exactly what we need um what we need in our diffusion model um or more
specifically in the reverse generative sd for generation so this is great um so yeah in practice
diffusion modeling basically boils down to learning this score function let us now talk about a few
implementation details here and what people do in practice because that's also crucial um this
is again just copy the denoising score matching formula so how do we actually sample these diffused
data points um so this is really just retirement test sampling again from this normal distribution
here so we have gamma times our input x zero and then we add noise to this that is scaled by the
standard deviation sigma t now let us actually explicitly write down the score function how
it now looks like so it's a gradient of the logarithm of this conditional distribution
is the given x zero so this is a normal distribution so this is basically some exponential
times some stuff um so we have the logarithm and the exponential drop out there's also
by the way a um a normalization constant but this does not depend on x so it's not important
for the derivative anyway so we advise at this term so now we can take the gradient of this term
here we advise at that um now here for x t this is like the the diffused data point that we sampled
we can actually insert this expression here then we get that but now it turns out all these terms
here they cancel out this gamma t x zeros and um they're also one of those sigmas and what we left
with is that so this is interesting because this means that the score function um it's basically
just um the the noise value that we introduced during the retirement test sampling of our diffused
data like minus that noise value and scaled with the inverse standard deviation from this diffusion
but um still this is this is cool so this maybe also suggests us how we should choose our neural
network and how we should parameterize this um more specifically for instance we can take this
neural network and define it by like some yeah some other neural networks times minus one and
divided by the standard deviation which is inspired by this um salt here so now if we insert both of
this the expression for the ground to a score which is really just this noise value and also
this neural network parameterization what we left with is this objective here at the bottom
and now this is interesting so this means if I choose this parameterization our neural network
epsilon tetanus is basically tasked with predicting noise values epsilon which are really just the
noise values that were used to perturb our data so this also makes it kind of intuitive
where this is called denoising score matching because if our neural network can denoise um
can predict those noise values that were used for perturbation then yeah we can denoise and
reconstruct the original um data point x0 from xc
so there's another implementation detail here so I have kind of arbitrarily motivated that we
can use this squared uh two term um to perform denoising score matching and to regress this
far function um maybe we want to give different weights to this l2 term to this l2 loss for
different um points along the diffusion process keep in mind that this is one neural network
that just as input gets a noisy state and t it's the same neural network for all t's along the
diffusion process so maybe we want to specialise the network a little bit more for large times
along the diffusion process of small times or something like this and give like different
weights to this objective for different times along the diffusion process um and this is a
loss weight in lambda t so we introduce this loss weight in lambda t that it does this and it turns
out that different loss weightings trade off between like models is different good perceptual
quality like that the images um look pretty that we can generate the set and sharp
versus no high log likelihood um for instance if we um choose lambda of t to be exactly this
variance of the forward diffusion process here to cancel out the variance in the denominator
then this is an objective that is actually that leads to um good high quality perceptual quality
outputs um on the other hand if we choose for lambda for instance beta of t which is hyper parameter
of the forward diffusion process then this whole objective that we have it corresponds to training
our model towards maximum log likelihood more specifically it's it's kind of a negative uh elbow
so this is interesting and it turns out that yeah this is exactly the same objectives that we
derive with the variational approach in part one presented by Arash and yeah so this means that there
are some deep connections between this variational derivation and actually like score matching and
noising score matching in in particular I would also like to point out that there are like much
more sophisticated model parameterizations and loss weighting possible um I would in particular
refer you to this recent paper by um Tiro Karas et al who who discusses in quite some detail
there's another implementation detail I would like to talk about um
so we know that this variance sigma t squared in the of this forward diffusion
process for like diffusing individual data points that actually goes to zero as t goes to zero
but this means that if I sample the t here close to zero then this loss might be heavily amplified
when sampling yeah t close to zero at least for the case when we do not choose lambda or
weighting function to cancel out the sigma spread so for some of these for these reasons we sometimes
see some tricks and implementations where we um train the small time cutoffs so we prevent sampling
t's that are extremely close to zero um but there is a more fundamental way how this can be fixed
like I said this is especially relevant when training models towards high block likelihood
where this lambda t function maybe something like beta of t and this sigma squared is not
cancelled out in that case we can perform an important sampling um with respect to this um yeah
weight of this loss and so we have an oversampled t's close to zero and uh yeah so the objective
then looks like this so we oversample small t's according to the important sampling distribution
that has most of its weight across small t and then we weigh down the condition of those to the
overall loss and with one over our t's important sampling distribution um I don't want to go into
too much detail but this is a technique you see in several papers um so here's the visualization
of what happens so this is now the loss value um the wet is the loss value without the importance
sampling here so yeah if I sample t close to zero then I have this heavily amplified loss
values but with important sampling the blue line the variance is significantly reduced
before moving on it makes sense to briefly recapitulate what we have been doing so far
so we have been introducing this diffusion modeling framework based on continuous times now
in contrast to the first part presented by arash where each diffusion step had a finite size and
we overall had a finite number of discrete um forward fixed diffusion process steps and also
denoising steps in this section we have continued considered a continuous time this allowed us to
introduce this differential equation framework and also to make these connections to scorematching
but it is important to keep in mind that we are still describing the same types of diffusion
models in this section we're just using different tools at the different framework so this is
important to realize after all we obtained the same objectives as we have seen on the last few
slides with that in mind let us now move on and we will talk now about the probability flow ordinary
differential equation however let us first consider the reverse generative diffusion sd e again that
one so object you have already seen so far with this generative reverse diffusion sd e we can
when sampling um random noise from this standard normal prior distribution we can generate data
and more specifically we basically can sample data all along the diffused data distribution
the the weddish curves here the weddish contours here it turns out there is a ordinary differential
equation called the probability flow ode that is a distribution equivalent to this reverse
generative diffusion sd e it will become clear in a minute what exactly i mean with indistribution
equivalent let us first have a look at this ode itself it is written down here in contrast to the
generative diffusion sd e it doesn't have the noise term and also the score function term
which people then later learn with the neural network it doesn't have this factor of two in front
anymore so what do i mean with indistribution equivalent when i sample many initial noise
values from this standard normal distribution at initialization when i want to generate the data
then simulating all these samples on backwards versus probability flow ode towards the data
by doing that we will sample from the exact same probability distribution like with the
generative diffusion sd e with the only difference that we don't have this noise anymore so how does
it look like more specifically we can see this on that slide here again we have the probability
flow ode just that we now have inserted the learned score function as better for the yeah
score function and now these trajectories defined by this ode they look like this so we see that when
we sample from this yeah standard normal prior distribution on the right these trajectories
they will all flow into the modes of the data distribution we see this also at these bluish
lines here at the background yeah so the probability quite literally flows into the modes of the
data distribution this is why it's called the probability flow ode here we have an animation
how it looks like and i think at this point it should really become clear what i meant with
they are the same in distribution and they sample the same distribution on the left hand side we have
the sd e that we have that i have introduced earlier already sd framework and yeah we see that
these trajectories are zigzagging i have this noise injection but i'm still landing at the modes of
the data distribution while for the ode formulation i now have these pretty like not exactly straight
but more deterministic trajectories that still land in the modes of the data distribution
when i initialize them randomly from this prior distribution so each individual trajectory
on the right is deterministic while here this is yeah stochastic
so this probability flow ordinary differential equation this is actually an instance of a
neural ordinary differential equations which a while ago generated a lot of attention in the
literature more specifically we can even see this as a continuous normalizing flow
so why should we care why should we use this probability flow ode framework it turns out
that this ordinary differential equation framework it allows the use of advanced ordinary
differential equation solvers it is somewhat easier to work with ordinary differential equations
than with stochastic differential equation and there really is a broad literature on how to
quickly and very efficiently solve ordinary differential equation so we can build on top
of this literature here but there are more advantages this ordinary differential equation
i can run this in both directions i can run it as a generation where i go from the right to the
left where i sample the noise from a prior distribution and then go to the left to generate
data but similarly given a data point i can also run the probability flow ode in the other direction
and encode this data point in the latent space of this diffusion model this prior space so this
is interesting and yeah this allows for interesting applications for instance for semantic image
interpolation and to make clear what i mean with that let's look at this slide here what i'm doing
here is i have drawn two noise values or yeah or let's look first at the lower left so here
we are drawing two noise values in the latent space of the diffusion model in this noise space
and now i can linear interpolate these noise values in this space however the model was trained
in such a way that every sample under this noise distribution so also every sample along this linear
interpolation path between those noise values decodes to a coherent realistic image so when
i then interpolate it means that these this results in continuous semantically meaningful
changes in the data space right and keep in mind we could not just interpolate directly linearly
in pixel space this would not be meaningful um but we can do that in noise space and then obtain
semantically meaningful interpolations in the pixel space like this however because this ode is
so complex right another hood this means that we will sometimes have some jumps between nodes and
such like this and we also see this in this animation here so in this animation at the top
yeah we have been doing many of such interpolations one after another and uh yeah sometimes you see
like little jumps uh this basically corresponds to that all this is only possible due to this
deterministic um encoding and decoding paths with the probability flow ode i think it's clear
that you couldn't do this so easily with astrophastic projectiles all right so there is another
advantage of the probability flow ordinary differential equation we can also use it for
block likelihood computation as in continuous normalizing flows more specifically we can take
a given image or a given data sample for instance this image of arash's cat peanut
now we can take peanut and encode peanut in the latent space of our diffusion model
then we get this state xt now we can calculate the probability of peanuts encoding under the
prior distribution of our diffusion model and additionally we take into account this
using this instantaneous change of variables formula kind of the the distortion of the ode the
volume change um along the ode trajectory so then the probability of um yeah of our data
sample in our case the image of peanut is then given by that expression so we're really just
using the tricks from the continuous normalizing flow literature here
what all this means is actually that in their probability flow ode formulation
diffusion models can also be considered as continuous normalizing flows however in contrast
to continuous normalizing flows we train diffusion models with score matching
continuous normalizing flows themselves are usually trained directly with this
objective to yeah maximize the likelihood of the data however training with such a
this objective directly is actually a hard task because for each training iteration
I have to simulate the whole trajectory here and back propagate it through it
on the other hand this diffusion model training relies on score matching
and as we have seen earlier score matching works quite differently in score matching we basically
have like we can train for all these different times along the diffusion process separately
this leads to a much more scalable and robust learning objective and yeah this makes diffusion
models very scalable in contrast to these normalizing flows I would argue
so I have not talked a lot about these differential equations
derive these derive them and so on and so forth however how should we actually solve
these sces and odes in practice we have already seen that we can probably not solve this analytically
because these sces and odes are defined with very complex non-linear functions namely
these neural networks that approximate this score function so let us look at that
so let's start with the generative diffusion ste so the most naive way to do this is to use
Euler-Mariouama sampling we have already briefly talked about Euler-Mariouama sampling in this
earlier one slide crash course on differential equations what we do in that case is we simply
evaluate our yeah function here for different for our state t and x then we propagate for like a
small time step delta t and we additionally add a little bit of noise which is also scaled by the
time step and yeah so we do this then iteratively one after another given a new step we evaluate
again again add a little bit of noise and so on and so forth by the way as a small comment here
and you may wonder about the sign flip from here to here this is because our dt is actually negative
because we are running from like large time values to small time values and this delta t
here is not supposed to be like an absolute step size so it's positive it turns out also this
and chat to a sampling and that awash talked about in the first part of our tutorial and
more specifically the way he showed us how we can how we can sample from these discrete time
diffusion models and this this can actually be also considered a generative sd-e sampler with
this particular discretization used in that part so now let's look at the probability flow ODE
how can we generate that or using that again we could basically use Euler's method which is analogous
to the Euler-Maiorama approach and just now without this noise injection again we would just
iteratively evaluate our network and yeah so the ODE function essentially do a small step
linear step for a small time delta t we evaluate and continue doing that however this is usually
I think nobody really does this in practice in practice we can hear as I mentioned earlier
we need to build on the advanced ordinary differential equation literature and use much
better solvers and much better methods and higher order methods in particular so what we see for
instance is the use of women kutta methods of linear multi-stepping methods exponential integrators
in fact there was a lot of literature in that direction yeah like I just said
adaptive step size women kutta methods have been used and also for the stochastic differential
equation actually adaptive step size higher order methods have been used we parameterized
things the ODE has been proposed that also accelerates sampling yeah like I mentioned linear
multi-stepping has been used exponential integrators have been used recently heintz method has been
used was very successfully it's another higher order solver so yeah there was a lot of literature
in that direction and the main reason is that one drawback of diffusion models is
that sampling from them can be slow in contrast to like sampling from a generative adversarial
network or variation autoencoder and such methods for instance sampling from a diffusion model
requires many many function calls or what are specifically neural network evaluations in our
case because during each step of this iterative denoising we have to call this neural network
again so we often have to call it many many times and yeah so this is where we want to use
efficient solvers so that we can reduce this number of neural network evaluations that we have to use
so now I have talked about how you can use how you can solve the SDS and the ODE and practice
but what should you use should you actually rather build on the SDS or the ODE framework when you
want to sample from your model so to to shine some light into that let us look at the generative
diffusion SDS a little bit closer so it's it's like that but now we can actually decompose this
into two terms right so this is just from here to here and it turns out the first term is really
just the probability flow ODE that we have seen already which itself can be used for deterministic
data generation like you're on the right but then there is this additional term this longevity
diffusion SDE term so this basically corresponds to the noise injection and yeah it has the noise
injection here so and what what do these terms do so this probability flow ODE term is essentially
responsible for yeah pushing us from the right to the left here and this longevity diffusion
SDE term what it basically does is for each individual team it actively pushes us towards
correct diffused data distribution but because of this so when I do errors during my solve during my
simulation going from the right to left you're going from noise to data if I have errors then
this longevity diffusion SDE can help us to correct these errors and actively bring us back to the
right data manifold back to the diffused data distribution so this yeah this is an advantage
can do some sort of error correction on the other hand it's it's often slower because this term
itself requires a somewhat fine discretization during the solve yeah so now let's look at the
probability flow ODE so in that case we do not have this SDE term and this large event term here
however we can now leverage these really fast ODE solvers and so this is good when we target very
fast sampling on the other hand there is no stochastic error correction going on here
and because of this what we see in practice is that this is sometimes slightly lower performing
than the stochastic sampling when we just look at the quality of the samples so to summarize what
we see is if we are not concerned about our budget of like neural network evaluations and we are
willing to do like very a lot of steps then this SDE framework can be very useful but if we want to
go as fast as possible then probably the ODE framework is better where we then can leverage these
really fast solvers it is also worth mentioning that we can do things in between where we have only
like this longevity diffusion SDE active a little bit scaled down we can also you know like kind of
solve for the first half using stochastic sampling and then afterwards switch to the ODE
advanced methods are possible I would like to refer you to to this paper here which discusses
some of these things in quite some detail next I would like to talk about a connection between
diffusion models and energy based models so what are energy based models energy based models are
defined like this in an energy based model the probability distribution that we want to model
the theta of x defines to an exponential to the power of minus a scalar energy function which is
now the function of the data and then this thing is normalized by a normalization constant to make
sure it's a well defined probability distribution this normalization constant is also sometimes
called the partition function furthermore in this case I have added a time variable t because this
energy based model is not supposed to represent the diffuse data distributions for different
t's in our case when we want to sample an energy based model we usually do that via large event
dynamics which is f we have seen large event dynamics already um basically this is very closely
connected to these stochastic differential equations we have already discussed so to do this
large event dynamics sampling we basically require the gradient of this scalar energy function and
then we also intuitively update our sample with that and we also have like an additional
noise term eta and some step size of learning weight eta here
the important part to realize is when we do um when we use these energy based models is
that in practice even though we are learning the scalar energy function we only require the gradient
of this energy function or more specifically the negative gradient of this energy function
for sampling the model at the end we do not require the energy function itself nor do we
require the partition function depth set up by the way this zeta it's implicitly defined through
this energy itself that we're learning so now it turns out that in diffusion models what we're
basically learning is the energy gradients for all these diffuse data distributions directly
we are not learning energies but basically energy gradients and one thing I want to add is that
because in this ebm spirit we are directly learning these energies and we have to tell
and the probability this is an expression for the probability distribution while also taking
into account this partition function because of this training energy based models can be
actually really complex um this often requires an advanced mark of chain Monte Carlo methods
which can be very difficult to deal with and uh yeah I'm not one of that it's valuable I would add
but yeah in diffusion models we kind of circumvent that and we only directly learn these energy
gradients tell me somehow show that and derive that maybe so to this end let's recall again
that in diffusion models our neural network basically we are trying to approximate also our
model more generally we are trying to approximate this score function of the diffused data distributions
qt of x now let us suppose that our model is parametrized such that the diffuse data distributions
qt are given by this energy based model here all right so now let us insert this p theta here
and yeah through the map so we apply the logarithm both here both on e to the minus the scalar energy
function and also the denominator however this term drops out because the partition function
does not depend on the state x and what we are left with is just a negative gradient of the energy
but what does this mean so this means that this neural network s that we usually have
in diffusion models through to model this graph function it means that it essentially learns
the negative energy gradient of yeah so the this the energy model based model that would
describe the diffused data distribution so yeah once again diffusion models kind of circumvent
these complications and directly model the energy gradients and say avoid modeling this
partition function explicitly for instance which leads to some of these difficulties that we have
in classical energy based model training and also these different noise levels that we have
in diffusion models this is actually analogous to a mere sampling in energy based models
I would like to talk about one more thing about diffusion models which is unique
identicality it turns out that the denoising model that we're learning in these diffusion models
that is supposed to approximate the score function of the diffused data to t of x p this denoising
model is in principle uniquely determined by the data that we're given and the forward diffusion
process and not only the score model so and by learning the score model also these
data encodings that we obtain by using the probability flow of the e to deterministically
encode data in the latency space all this is uniquely determined by the data and the diffusion
what this means is that even if we use different neural network architectures for s and different
network initialization we should at the end recover identical model outputs like identical
score function outputs and data encodings in the latency space assuming we have sufficient
training data model capacity and optimization accuracy this is in contrast for instance to
generate adversarial networks or variational auto encoders which do not have this property
for these models depending on what kind of architectures we use and what kind of initializations
we use we will always obtain like somewhat different models and different data encodings and so on
so this is the unique property about these diffusion models
here's an example what we are seeing here is the first 100 dimensions of the latent code obtained
from a random cyber tent image that was encoded in the latency space of a diffusion model using
this probability flow ODE approach we did this most specifically Song and Al did this with two
different models and model architectures that were separately trained however both of these
encodings distributions here as we see they are almost the same they are almost identical
even though these were different architectures
with that I would like to come to a conclusion and briefly summarize so in this part of this
talk I have introduced you to this continuous time diffusion framework in contrast to what
Arash talked about in step one we do not have finite size denoising and diffusion steps anymore
and only a finite number of cells rather we consider continuous perturbations a continuous
forward diffusion process and then also a continuous generative process based on differential
equations and to for to train these models we make connections to score matching more specifically
denoising score matching now maybe this appeared somewhat complex and mathematically involved
however why should so therefore why should you use this differential equation and continuous
time frame it really has two unique advantages as I have shown hopefully and hopefully I can
convince you during this part of the talk most importantly this allows us to leverage this
broad existing literature on advanced and fast S E and ODE solvers when sampling from the model
this can help us to really accelerate sampling from diffusion models which is very crucial
because they can be slow furthermore in particular this probability flow ODE is very useful because
it allows us to also perform like these deterministic data encodings and it also allows us to do like
look like real estimation like in continuous normalizing flows and so on additionally this is
overall a fairly clean mathematical framework based on diffusion processes score matching and so
on and this allowed us to use these connections to neural ordinary differential equations to
continuous normalizing flows and to energy based models which I think provides a lot of insights
into diffusion modeling with that I would like to conclude my part and take them yeah take the
mic to Wicci who will now talk about advanced techniques accelerated sampling conditional
generation and beyond thank you very much hi everyone I'm Ricci from google brain team so
let's continue our study on diffusion models so in the third part we're going to discuss several
advanced techniques of diffusion models which corresponds to accelerating sampling a conditional
generation and beyond so here's an outline of what we're going to cover in this part basically
want to address two important questions of diffusion models with advanced techniques the first one is
how to accelerate the sampling process of diffusion models we're going to tackle this question from
three aspects advanced forward process advanced reverse process and advanced modeling including
hybrid models and model distillation the second question is how to do a high resolution
optionally conditional generation and I will talk about several important techniques to make this
happen especially the general conditional diffusion modeling framework the classifier
classifier free guidance as well as cascaded generation pipeline so let's start from the
first question so how to accelerate the sampling process of diffusion models to see why this question
is important let's consider what makes a good generator model so in principle we want a good
generator model to enjoy the pulling three properties first it should be fast to sample
from this generator model second we would expect the generator model capture most of the major
modes of the data distribution or in other words they should have adequate sample diversity
and third of course we want the generator model to give us high quality or high fidelity samples
however there exists a generative learning dilemma for existing generator model frameworks
for example for a generator with several networks they are usually fast to sample from
and they can give us high quality samples however because of this discriminatory learning framework
there is a decent chance that GANS may miss certain modes of the data distribution
and the other type of generator models are these likelihood based models like variational auto
encoders or normalizing flows so those models are usually optimized by maximizing likelihood
or maximizing a variant of likelihood for example the evidence lower bound
so usually this type of models are good at fast sampling and they are able to capture certain
modes of the data distribution because of this maximum likelihood learning framework however
usually they lead to subpar sample quality and on the other hand the diffusion models are good at
both coverage because they are also optimizing the evidence lower bound of log likelihoods
and they are able to generate high quality samples however the sampling of diffusion models is pretty
slow which usually requires thousands of functional calls before getting a simple batch of samples
so if we can find techniques to accelerate the sampling process of diffusion models we will get
a generative model framework which enjoys all those three great properties that is we can tackle
the dilemma of this generative learning framework so how to do that so before diving into details I
would like to recap the general like formulation of diffusion models so for diffusion model they
usually define a simple forward process which slowly maps data to noise by repeatedly adding
noise to the images and then a reverse process is defined to map data so to map noise back to
data and this is where the diffusion model is defined and trained and in terms of the diffusion
model it is usually parameterized by a unit architecture which takes the noisy inputs at
certain time step and then it tries to predict the clean samples or it tries to predict the noise
added to this noisy inputs so if we think about accelerating sampling there are some naive methods
that immediately come to our mind for example in training we can reduce the number of diffusion
time steps or in sampling we sample every k time step instead of going over the whole
reverse process however those naive acceleration methods will lead to immediate voice performance
of diffusion models in terms of both the sample quality as well as the likelihood estimations
so we really need something clever cleverer than those naive methods and more precisely we want to
ask given a limited number of functional calls which are usually much less than thousands so how
to improve the performance of diffusion models and as a side note although the following techniques
I'm going to discuss take this accelerated sampling as a main motivation they definitely
provide more insights and contributions to diffusion models as we will see soon
so we will answer this question from three aspects the first one is forward advance forward
process and second is advanced reverse process and then lastly the advanced diffusion models
so first let's take a look at some advanced forward process so recall that the original whole
process defines a Markov process where we start from this x0 it is the clean sample and we gradually
add noise until it goes to this x b t corresponding to y noise signal and then the q x t q x t minus
one is simply a Gaussian distribution and this beta t defines the noise schedule and they are
hyper parameters that are predefined before training the models so we are interested in the
following questions so first does this forward process or noise schedule have to be predefined
and does it have to be a Markovian process and lastly does it be there any faster mixing
diffusion process so fast faster mixing is an important concept in Markov tree and Monte Carlo
as we will discuss later for the first work I would like to discuss here is this variational
diffusion models it basically enables us to learn the parameters in this forward process
together with the rest of parameters in the diffusion models that is we can really learn
the forward process so more specifically given the forward process defined by q x t even x0
it follows a Gaussian distribution with square root alpha t bar x0 as the mean and one minus alpha
t bar as the variance so this is the formulation we have learned in part one and this work proposed
to directly parametrize the variance one minus alpha t bar through a learnable function gamma eta
and this function is definitely by a sigmoid function to ensure that the variance is within
the range from zero to one and this gamma eta t is further parametrized by a monotonic
multilayer perceptron by using strictly positive ways and monotonic activations for example sigmoid
activations and recall that in part one we have learned that these diffusion models are directly
connected to hierarchical variational auto encoders in the sense that diffusion models can be
considered as a hierarchical variational encoders but with fixed encoder right and this model is
named as variational diffusion models because it is even more similar to hierarchical variational
encoders because we are optimizing the parameters in the encoder together with the parameters in the
decoder and to like optimize the the the parameters of the forward process this paper further
derived new parametrization of the training objectives in the Boolean sense so basically
they have shown that optimizing the variational upper bound of the diffusion models can be simplified
to the following training objectives so note that this gamma eta participates in this weighting
of like different l2 norm of different time steps and this is the training objective they
derive for this query time segment and they have shown that by learning this noise schedule it
actually improves the likelihood estimation of diffusion models a lot especially when we assume
that there are fewer diffusion time steps and recall that in the second part we learned that
the diffusion models can be interpreted from the perspective of stochastic differential equation
and we learned the connection between diffusion models and denoting score matching so that gives us
a hint or a knowledge that the diffusion models can also be defined in the continuous time setting
and in this paper they explicitly derive this variational upper bound in the continuous time
setting with this gamma eta notation basically they show that when we let this big t go to infinity
meaning like we have the infinity amount of diffusion time steps this corresponds to a
continuous time setting and then the variational upper bound can be derived in the following
formulation so here the only difference is that the weighting term of different l2 terms
l2 distance at different time steps equals to the derivative of this gamma eta t function
over time t and more interestingly this paper shows that if we define the signal to noise ratio
equals to alpha t bar minus one divided by one minus alpha t bar and then this l infinity
is only related to the signal to noise ratio at the endpoints of the whole forward process
and it is invariant to the noise schedule in between the endpoints so if we want to optimize the
forward process in the continuous time setting we only need to optimize the signal to noise ratio
at the beginning and the end of the forward process and they further show that the in between
process can be learned to minimize the variance of the training objective and this enables the
faster training of diffusion models besides faster sampling and another contribution of this work
is that they show it is possible to use diffusion models to get state-of-the-art likelihood estimation
results so before this work the benchmark of lightings by this mission have been dominated by
autoregressive types of models for many years as shown in this figure but this model shows like
actually we can use diffusion models to get a big improvement out of the autoregressive model
classes and one key factor to make this happen is to add forward features to the input of the unit
and those forward features can range from low frequency to very high frequencies
and the hypothesis for the assumption here is that to get good likelihood estimation
the model usually needs to model all the bits or all the details in the input signal
either they are imperceptual or perceptual however neural networks are usually bad at
modeling small changes to the inputs so adding those forward features especially those high
frequency components can potentially help the network to identify those small details
and the paper found that this trick doesn't bring like much significant improvements to the
autoregressive baselines however it leads to significant improvements in likelihood estimation
for diffusion model class okay so next like paper or method I'm going to discuss is this
denoting diffusion implicit models so in this work the main idea is like they try to define
a family of non-marcoving diffusion processes and the corresponding reverse processes and those
processes are designed such that the model can still be optimized by the same surrogate
objective as the original diffusion models recall that this is the surrogate objective right so
this L simple where we remove the weighting of each L2 loss term and we just simply take the
average of the L2 loss at different time steps and then because they can optimize
by the same surrogate objective so one can simply take a pre-trained diffusion model
and treat it as the model of those non-marcoving diffusion processes so that they are able we
are able to use the corresponding reverse processes to reverse the model which means like
we will have more choices of our sampling procedure okay so to see how we can define
those non-marcoving forward processes let's recap the duration of the k-out divergence
in the variational lower bound so recall that this LT-1 is defined as the k-out divergence between
this posterior distribution qxt minus one given xt and x0 and this denoting distribution
p-zeta xt minus one given xt so this p-zeta is parameterized by the diffusion model and because
these two distribution like both of them are Gaussian distributions with the same variance
sigma t square and this can be written as the L2 distance between the the mean of these two
distributions times a constant and recall that these two mean functions have been parameterized
by like simple linear combination of xt and ipsum or the combination of xt and the predicted ipsum
so ipsum is the noise added to the queen sample to get xt so we can further write
this k-out divergence in the form of lambda t times the L2 distance between the true noise ipsum
and the predicted noise ipsum theta by our diffusion models and if we really think about
the duration of this LT minus one we found that if we assume like this lambda t can be
after values because in a surrogate objective we simply set it to one right so it doesn't matter
what value this alpha t is originally then we can find out about formulation holes as long as
first we have this qxt given x0 it follows this normal distribution this is to make sure that
this xt still equals to this formulation and we have those two assumptions first the forward
process like the posterior distribution qxt minus one given xt and x0 follows a Gaussian distribution
and the meaning of this distribution is a linear combination of xt and ipsum and we define our
reverse process to be similar to the posterior distribution which means it is also a Gaussian
and this new theta is the same linear combination of xt and the predicted noise
and then we can further rewrite because we know xt equals to a linear combination of x0 and ipsum
so we can replace this ipsum by the linear combination of xt and x0 and similarly for
the reverse process we can rewrite it as a linear combination of xt and predicted x0 hat so this x0
hat is defined as the predicted clean sample given the predicted noise so if we make those three
assumptions then we can see that the above duration of lt minus one still holds which means that we
don't really need to specify this qxt given xt minus one and we don't need to require it to be a
Markovian process all we need to assume is the posterior distribution of xt minus one even xt
and x0 so that's the basic the the insights of this this like how we can define the non-Markovian
forward process which leads to the sanctioning objecting as the original diffusion model
and more specifically so this is the original diffusion process and now the diffusion process
changed to the right diagram where for each xt it depends on both the xt minus one and the x0
and another remaining question is how we specify the linear combination parameters a and b so note
that here we need to specify this a and b such that this qxt given x0 still follows this Gaussian
distributions so to this end this work defines a family of forward processes that meets above
requirements which corresponds to specifying the posterior qxt minus one given xt and x0
in this formulation and we can similarly define the corresponding reverse process
by just replacing this x0 to a predicted x0 hat and note that this specifying specification of
formulation doesn't require like a specific value of this sigma t-tutor which means like this
sigma t-tutor can be literally arbitrary values so that's why it depends it actually defines
the family of forward processes with different values of sigma t-tutor
and more importantly if we specify like sigma t-tutor to be zero for all the time steps this
leads to this ddi sampler where we wish is a deterministic reverse process because this
variance is zero here and the only randomness comes from the like starting point of the reverse
process which is the starting white noise signal and recall that in the second part we also build
connection between the like stochastic reverse process with a probability flow ODE which is
corresponds to a deterministic generative process and we can also interpret the ddim sampler in a
similar way specifically this ddim sampler can be considered as an integration rule of the
following ordinary differential equation and note that here we do a bit of change of a variable
where we define this x bar equals to x divided by the scaling factor square root of alpha
bar and we define this eta to be basically the square root of the inverse signal to noise ratio
and if we assume this sigma this epsom theta to be the optimal model and then this ODE is equivalent
to a probability flow ODE of a variance is floating SDE which is in the following formulation
and note that although these two are equivalent the sampling procedure can still be different
because for the above ODE we are taking like the sampling process over this eta t well for the
second formulation we are taking the for example for both of the two equations we use the standard
euler method then the second one is taking the euler step over dt so basically in practice
people find that the first one works better than the second one because it depends less on the value
of t but it depends directly on the signal to noise ratio of the current time steps
and it is also found that with this ddim sampler we are able to use less time sampling steps but
reach better like performance and in terms of why this is true this this paper by carers et al
argues that the ODE of the ddim is favored because as showing in those two in those three
illustrations especially the third illustration the tangent of the solution trajectories of ddim
always points towards the denoider outputs while for the first two ODE formulation the
variance preserving ODE and the variance is folding ODE which are two very commonly used
ODE formulation diffusion models they basically have more like high curvature regions along the
trajectories and while for the ddim we can see like for the solution trajectories most of the
trajectories are linear and with low curvature and it is known that the low curvature really means
less truncation errors accumulating over the trajectories so if we use this kind of trajectories
like we will have a smaller chance to accumulate more errors across the trajectories
thus enable us to use like fewer number of diffusion time steps fewer number of sampling
steps in inference okay so that's that the third word i'm going to discuss for in advance
the forward process is just critically down to long-distance diffusion model
basically they are trying to find a faster mixing diffusion process by using certain
like background knowledge from Markov Chen Monte Carlo to see how this forward process is related
to mcmc let's see like this is a regular forward diffusion process which is a stochastic differential
equation and it is actually a special case of the overdone long-distance dynamics if we assume
the target distribution or the target density of this mcmc is this standard Gaussian distribution
so given this connection we can actually design more like efficient forward process
in terms of mcmc specifically this work proposed to introduce an auxiliary variable this velocity v
and the diffusion process is defined on the joint space of this velocity and the input
and during the forward process the noise is only added in this velocity space and this
image space or input space is only erupt by the coupling between this data and the velocity
and the resulting process as showing this figure we can see that the forward process in the v space
is still exact however the the process in the image space or the data space are much more smoother
and this v components is in analogous to the Hamiltonian components in hmc or analogous to
the momentum in momentum based optimizers so by defining this joint space or defining the
diffusion in this v space it actually enables faster mixing and faster traverse of the joint space
which enables fast like more smoothly and efficient more smooth and efficient forward process
and the second let's see some advanced reverse process so before that we would like to ask a
question so remember that we use this normal approximation of the reverse process right
so we assume like the denoting distributions are always Gaussian distributions but if we want to
use less diffusion time steps is this normal approximation of the reverse process still true
or accurate unfortunately the answer is no so this assumption normal assumption in this denoting
distribution holds only when the adjacent the noise added between these adjacent steps are small
if we want to use less diffusion time steps in training then we can see that this denoting
distribution is not a unique model normal distribution anymore so they are tend to be like
multimodal and more complicated distributions so in that case it means like we really need
more complicated functional approximators so I will talk about two examples of how we can
include more complicated functional approximators here the first word is this denoting diffusion
gains so in this word they propose to model the denoting distribution by conditional gain model
specifically the model is trained in this other several learning framework and we first get the
samples x t minus one and x t by running this forward diffusion diffusion process and then
this generator takes x t as input as well as the time step as input and instead it's actually
trying to model the x t minus one but instead of just directly outputting x t minus one from the
network it's first trying to predict this queen sample x zero and then it tries to sample x t
minus one from this tractable posterior distribution and then the discriminator takes the real x t
minus one and the fake x t minus one prime as input and try to discriminate these two samples
and again this discriminator is also conditional on x t and the corresponding time step t
so one may ask what is the benefit of this diffusion denoting diffusion gains compared
to a one-shot gain model but this paper shows that because like right now the conditional gains
only need to model like the the conditional distribution of x t minus one given x t these
turn out to be a much simpler problem for both the generator and the discriminator compared to
directly model the model distribution of the queen sample and this simple simpler training
objective for the two models leads to stronger mode coverage properties of gains and also leads to
better training stability and recall that in the second part we learned that there's a close
connection between energy-based models and diffusion models so therefore a natural idea
is like we try to approximate the reverse process by conditional energy-based models
recall that an energy-based model is in the form of p beta x it is solely dependent on the
unnormalized log density function which is this f theta x and we can further prime try this f
theta x by minus e theta x so usually people call this e theta x as the energy function
and this d theta is the partition function which is usually analytical and intractable
and we can parameterize this f theta by a neural network which takes the signal as input
and output as scalar to represent the value of this f and the learning of energy-based models
can be illustrated as follows so suppose this is the energy landscape defined by the e theta function
and after learning we would like to put the observation data points into the regions of
low energy and put all the other inputs to the regions of the high energy
and optimizing the energy-based models require you really require the mcmc sampling from the
current model p theta x as shown in this formulation which is really a highly
computational expensive especially for high dimensional data
so if we want to parameterize the denoising distribution by conditional energy-based model
we can start by assuming like at each diffusion time step marginally the data follows a
energy-based model in the standard formulation so here I remove the the script like the time
step script for similar simplicity and let x theta be the data at a higher noise level
then we can derive the conditional energy-based models by Bayes and Rowe but specifically this px
if x theta is in this formulation and if we compare these conditional energy-based models
with with the original marginal energy-based models we see that the only difference is that
there is an extra projected term here and this actual projected term actually has an effect of
localize this highly multimodal energy landscape to a like more single mode model or unimodal
landscape and with the model mode focus around the the higher noise level signal x theta
therefore compared to training a single energy-based model the sampling here is more friendly
and easier to converge because the energy landscape compared to the original marginal
energy landscape is more unimodal and more and simpler so that the training could be more efficient
and the conversion mcmc can give us well-formed energy potential after training
and compared to diffusion models this energy-based to using this energy-based model to
parameterize the denoting distribution can give enables us to define like much less diffusion
steps up to six steps and more specifically to learn those models we simply maximize the
conditional log likelihoods at each time step and then after training we just get samples by
progressive sampling from the energy-based models from high noise levels to low noise levels
okay so the last part comes to the advanced diffusion models basically want to ask two
questions for first can we do model distillation so that the distilled model can do faster sampling
and second can we lift the diffusion model to a latent space that is faster to diffuse
so the first idea comes to the distillation so here I want to discuss one representative work
in this domain which is this progressive distillation of diffusion models basically this work proposed
to distill a deterministic DDIM sampler to the same model architecture of the original model
and it's it is it went into this progressive pipeline in a sense that at each distillation
stage we will have a teacher model and and we will learn a student model and this student model is
learned to distill like each two adjacent sampling steps of the teacher model to one sampling step
of the student model and after learning this student model at next distillation stage the
student model at the previous stage will serve as the teacher model at this new stage and then we
learn another student model at the new stage and we repeat this process until we can distill the
original thousand of sampling steps to a single sampling step and implementation wise the learning
of the student model is quite similar to the original this diffusion model training pipeline
the difference is how we define this training target of the diffusion model specifically
given the teacher model and we randomly sample a time step t and then we draw we run the sampler
for two steps and then the target is the is computed to make sure that the student model
can reproduce the two sampling step within one sampling step and then the loss is defined as
euro where we minimize the l2 distance between this target and the predicted like x hat from
this diffusion model or from this student model and after that we halve the number of sampling
steps and repeat this process until we reach one sampling step
okay so the second idea is like whether we can leave the diffusion models to a latent space
which is more friendly to this diffusion process and here's an example of of this kind of idea
where we can try to leave the diffusion models to a latent space of a pretend variational
optimal encoder and because this latent space the distribution of the data in this latent space
is already quite close to the Gaussian distributions which means like we can definitely use less
diffusion time steps to diffuse the data in this latent space so the advantages are pretty
straightforward so first because this latent space are already close to normal distribution
we are able to use less diffusion time steps to enable faster sampling and second compared to the
original variational auto encoders which assume that the prior distribution of the z
holds a single and simple Gaussian distribution this kind of hybrid model assume that the p of
c is modeled by a diffusion model which means that it has a diffusion prior so it definitely
will be much more expressive compared to the original variational auto encoder model
and more interestingly recall that for the current stage the diffusion models are only defining a
continuous data space however there are more domains which may have more complicated data
structure however for those data type as long as we can find an auto encoder model
which are tailored to that data type and can map the data input to a continuous latent space
we will be able to apply the diffusion models to that latent space so these give us more
possibilities to apply diffusion models to different modalities and different data types
and a bit of the detailed formulation so in this work again we optimize the model in terms of
by minimizing the variational upper bound of the negative log likelihood and the objective contains
three terms the first two terms are similar to the variational auto encoder objecting and the third
corresponds to the training objective of the diffusion models and it actually corresponds to
we treat the encode in latents this from this q z zero given x as the observed data of the
diffusion models and in that way we can derive the similar training objective of the diffusion
models as the original one so we first do this random sampling of time step and then we draw samples
from this forward diffusion and then we have this diffusion kernel this is the forward this is from
the forward process and then we learn this score function for zt and of course we have some constant
that is irrelevant of the model parameters okay so the second question we want to answer is how
to do high resolution optionally conditional generation using diffusion models so in the
past two years we have seen many impressive conditional generation results using diffusion
models for example this dow e2 and imagine recruits diffusion models to do high resolution text to
image generation and another examples includes the using conditional diffusion models for super
resolution or colorization and panorama generation is another example where we take a small size
input but generate this panorama so how can we do that let's first take a look at the general
formulation of conditional diffusion models which is pretty straightforward so the only
modification we need to make is in this reverse process we can let this denote and distribution
to incorporate an additional input which is this condition c and this corresponds to
modify the mean of this Gaussian distribution to take an additional input c and optionally we
can also let this variance to be learned and it takes an input c but in practice like most mostly
we still just use this c in this mean and the variational upper bounds only includes a small
change which lies in this KL divergence where we plug in this new formulation of the denoising
distribution but it is a design arc in terms of how to incorporate different types of conditions
into the unit which is used to parameterize this mu theta specifically like these are the
things people use in practice for scalar conditioning for example a closed label we can
do something similar to what we did for time step conditioning specifically we can encode
the certain scalars to a vector embedding and then we simply simply add the embedding to the
intermediate layers of the unit or we do this adaptive virtualization layers and if it is an
image conditioning we can do channel wise concatenation of the conditional image and the input
image and if it is for text conditioning this contains two cases first if the text we embed
the text to a single vector then we can do something similar to the vector derived from the scalar
conditioning and if we embed this text to a sequence of vectors then we can consider using
cross attention with the intermediate layers of the unit and for a high resolution conditional
generation another important component is this classifier guidance the main idea is like recall
the diffusion model correspond to learning the score of a probability for example the score
of log px right and right now because we incorporate the class condition which means like we the
score actually the diffusion model actually gives us a score of a class conditional model p of xt
given c and given this we can train an additional classifier which gives us the probability of c
given x and then we mix the gradients of these two models during sampling
and this corresponds to we sample from actually a modified score which corresponds to the gradient
of log px given c plus omega times log pc given x and this omega controls the strength of the
guidance and it actually corresponds to approximate sampling from the distribution
which is proportional to p of x given c times p of c given x to the omega power and in practice
is corresponds to we modify the normal distribution where we sample from and the mean of this normal
distribution corresponds to the mean predicted by the score model or predicted by the diffusion
model plus the the gradient from the classifier and if we use larger omega then the samples will
be more concentrated around the moles of this classifier which really leads to better individual
sample quality but if we use too large omega it will reduce the sample diversity so one really
needs to find a sweet point for this omega to best balance the individual sample quality and
sample diversity and one downside for this classifier guidance is that we need to train
an additional classifier to do that right so that adds additional model complexity so it's
better by that work this work tries to introduce a classifier free guidance meaning that we can
actually get an implicit classifier by jointly training a conditional and an unconditional
diffusion model in a pulling sense so suppose we have this px given c where the the score can be
derived by a conditional diffusion model and we also have the score of a unconditional model p of x
and then we can derive an implicit classifier pc given x which should be proportional to p of x given
c divided by p of x and in practice the gradient or the score of these two probability are estimated
by randomly dropping the condition in the diffusion models at a certain chance for each iteration
and similarly we can derive the modified score with this implicit classifier
recall this is the original modified score and now we replace the log pc given x by log px given c
minus log px and this is the resulting modified score we will use with this classifier free guidance
where this log px given c and this log px are both estimated by the single diffusion model
and in these three panels we can see the tradeoff between sample quality and sample diversity more
clearly so from left to right correspond to we gradually increase the strength of the guidance
and we can see clearly individually speaking the sample quality of each image increases however
the samples that look like more similar to each other if we use a large classifier or classifier
free guidance and the last thing I want to talk about is this cascade generation pipeline
which are important to high resolution generation and this kind of idea has already been
explored by other type of generative models for example GAN model and it's pretty straightforward
so we start by learning an unconditional diffusion model at the lowest resolution
and then we learn several super resolution models taking the down-sampled training images
at lower resolution as the condition and during sampling we just run the progressive sampling
pipeline starting from the smallest unconditional model and going going through all those super
resolution models until we reach the highest resolution but one notorious problem of this kind
of cascade training pipeline is this compounding error problem it's more specifically recorded
that during training the conditions we fit into the super resolution model is the down-sampled
versions of the training images from the dataset however during sampling the conditions we fit
to those super resolution models are actually generative samples of from the low resolution
models so if there are certain artifacts in the samples from the low resolution models those
artifacts or inaccurate samples will affect the the the sample quality of the super resolution
models as well because of this mismatch issue between the conditions in training and condition
in inference to alleviate this problem this noise conditioning augmentation is proposed
in reference works basically during training we try to degrade the conditioning low resolution
images by adding various amounts of Gaussian noise or just blur those images by Gaussian kernel
and during inference we sweep over the optimal amount of noise added to the low resolution
images which are the conditions to the super resolution models so the the idea or the the
hypothesis is like if we try to reduce certain amount of information from the condition then the
super resolution model will be trained to be more robust to different type of artifacts when we send
like the conditions as the samples and later on more complicated degradation process have been
proposed for example we can add a sequence of different types of degradation operations to the
image the low resolution image before sending it as a condition to the super resolution models
okay so here's a summary of this part so in this advanced technique session we learn to
answer two questions the first one is how we can accelerate the sampling process and we introduce
several important techniques from the aspects of advanced forward process reverse process
and modeling itself and the second question is how we can do high resolution conditional
generation using diffusion models and we discuss the general framework of conditional diffusion
models classifier and classifier free guidance as well as cascaded generation pipeline so in
the application section we will see how all those techniques will benefit in terms of various tasks
hi everyone welcome to the first section of applications of diffusion models so in this
section we're going to study applications of diffusion models in terms of image
synthesized controllable generation as well as text to image generation so let's from text to
image generation so in the past two years this task has been shown to be extremely
suitable for diffusion models to work on so basically this task is the inverse of the image
captioning tasks where we are given a text prompt c and we are trying to generate high resolution
images x as shown in this video so this video shows the generative images by a text to image
generation model called imagine as we will show later and let's start from this glide model by
opening in last year so this is essentially a cascaded generation diffusion models which
contains a 64 by 64 base model and a 64 by 64 to 56 by 256 super resolution model and they have
tried to use classifier free guidance and clip guidance so I will talk about the clip guidance
in details later and they generally found that classifier free guidance works better than the
clip guidance and those figures show the generative samples from this model and as we can see the
model is capable of generate fun noble compositions of concepts that have never been seen from the
dataset for example a head dog using a calculator and robots meditating in a vipassana retreat or
etc so a bit introduction of the clip guidance it can be treated as a special form of the classifier
guidance and in terms of a clip model it contains two components a text encoder g and image encoder f
and during training batches of the image and caption pairs are assembled from a large datasets
and the model optimizes a contrasting cross entropy loss which encourages high dog product
between this f and g if the image x and c comes from the same image caption pair and it encourages
low dog product if x and c comes from different image caption pairs and it can be proved that
optimal value of this fx times gc is given by log pxc divided by p of x times p of c
which equals to log pc given x minus log pc so given this conclusion
we will be able to use this clip model as the classifier in the classifier in the classifier
guidance in the following sense so we recall that for the classifier guidance we want to modify the
score in this formulation and we can consider augment the augmenting the second term by a
minus log pc term because when we check gradient over x then this part just disappears and then
we can see that these two terms together can be modeled by a clip model so we basically replace
this part by the dog product between fx and gc so that is the clip guidance however in glide
they show that the clip guidance is less favored compared to the classifier free guidance
and besides pure text to image generation the glide has shown that it is possible to
fine tune the model or text guidance in pending tasks basically they try to fine tune the trained
text to image glide model by fitting randomly occluded images with an additional mask channel
as the input so using this fine tune model they are able to change or do model image editing by
changing the prompt for example given an old car in a green forest they will be able to add it
the background to a snowy forest similarly they can add a white hat to a man's hat
and later on this dow e2 further scale up the glide model to support 1k by 1k text to image
generation and this dow e2 has been shown to outperform the first version of text to image
1k by 1k generation by open eye which is this dow e which is an unregressive transformer based model
and in terms of the model components of dow e2 it's built up on a pre-trained clip model
more specifically a clip model is first pre-trained and the image embedding and text embedding
are grabbed from this pre-trained clip embedding and frozen and after that this
pipeline have been built to generate images from text basically this generation model
contains two parts the first is a prior model this prior model try to produce clip image embeddings
condition on the input caption and then the second part is a decoder part
which produces the images conditioned on the clip image embedding as well as the text
so one natural question is why we want to condition the decoder on the clip image embeddings right
so why not we just directly condition this decoder on text only so the hypothesis here is that
for the total amount of entropy of an input signal for example images
so there's certain part that captures the high-level semantic meanings while there's still a large
proportion of the entropy which corresponds to just low-level details either perceptual visible
or even in perceptual invisible so the hypothesis is that clip image embeddings
tend to have a higher chance to capture the high-level semantic meaning of an input signal
especially those related to the caption information and by conditioning on this high-level
semantic meaning the decoder is able to capture or catch up those low-level details of the images
more quickly and later on this figure this paper also shows that this by-part later
repetitions of the clip image embedding as well as the latency in the decoder model
enables several text-guided image manipulation tasks as we will show later
and a bit more details of the model architecture of the prior and the decoder models
for the prior the paper tries two options the first one is the auto regressive pair
where they quantize the image embedding to a sequence of discrete codes and predict them
autoregressively and the second option is to model the prior by diffusion models
where they directly train diffusion models based on the continuous image embedding
as well as the caption input and the paper shows that the second option gives better performance
and in terms of the decoder is again a cascaded diffusion models which contains a
one base model and two super resolution models and to save the the compute and make the training
more efficient the the largest super resolution model is trained on image patches of one quarter
size but during inference the model will take the full resolution inputs and directly do the
inference on the full resolution and this paper also shows that the class the classifier free
guidance and north conditioning augmentation are super important to make the decoder work well
and a little bit more detail about the by part latent representations so given an input image
we can get the by part latent representations in the following sense so it contains two parts
first is this latent variable z which is the clip image embeddings and it can be derived
by running the clip image encoder and the second part is this xt which is the latency from the
decoder and this part can be derived by running the inversion of an ddim sampler for the decoder
and after getting these two latent representations the paper shows that it is possible to run this
decoder and get near perfect reconstruction of the original input image and given these
by part latent representations the paper shows that it is possible to do several image manipulation
tasks for example this image variation tasks target at getting multiple variants of an input image
well with the hope of preserving the high level semantic meanings of the input image
and this is achieved by fixing the clip embedding z while changing to different
latent x big t in the decoder and as showing this image panel the first one is the input image and
the rest are the image variants generated by dow e2 and we can see that certain high level
semantic meanings are preserved for example the artist's style and this clock is preserved in
all the image variants but with different details in the image variants and the second task is this
image interpolation task so given two input images it's possible to use dow e2 to do interpolation
by interpolating the image clip embeddings of these two input images and we can get different
interpolation trajectories by using different xt along these trajectories as showing in those
three rows and as we can see although the trajectories are different but the high level
semantic meanings are kept well for the two input images for all those three interpolation trajectories
and the last task the most interesting tasks that they show that can be done by dow e2 is this
text div task which means like given an input image and the corresponding description we
would like to add this image towards a different prompt and this corresponds to an arithmetic
operation in the latent space more specifically they first try to compute the difference between
the text clip embeddings of the original prompt and the target prompt and then they try to change
the image clip embedding of the given input towards the difference between the text prompts
and using this approach they show that it's possible to do this image text guided image
editing by changing the prompt and the last task to image diffusion model I want to talk about is
this imagined model by a google brain team so again the task is the same as dow e2 so we are
given some text prompt as input and we are trying to output 1k by 1k images aligned with the input
text and the highlight of imagined model is as follows first it provides a unprecedented degree
of photorealistic realism in terms of state of the art automatic scores such as FID scores
as well as a state of the art human ratings and it provides a deep level of language understanding
as can be told by the generated samples and it is is really simple so there is no latent space
and no compensation and as we will see later it's just like a pure cascaded diffusion model
so I will first present several examples of imagined
yeah this is my favorite one because I just created it to make it related to cvpr
and in terms of the key modeling components of imagined
like I mentioned it is a pure cascaded diffusion models containing one base model
and two super resolution models and it uses a classifier free guidance and the dynamic
thresholding as I will talk about later and unlike dow e2 which uses clip text embedding
as the using this clip text encoder this imagined used frozen large pre-train language models as
the text encoders more specifically this variant of t5 model and there are several key
observations from imagined first it is beneficial to use text conditioning for all the super resolution
models the explanation is as follows so remember like for cascaded diffusion models we need to use
this noise conditioning augmentation technique to reduce the company error but however this
technique has a chance to weaken the information from the low resolution models
thus we really need the text conditioning as extra information input to support the super
resolution models and second observation is that scaling the text encoder is extremely
efficient in terms of improving the performance of imagined and it has been shown that this is
even more important than scaling the diffusion model side and lastly comparing using the pre-train
large language model as the encoder versus the clip encoder human readers actually prefer the
large language model over the clip encoder on certain data sets
and this dynamic thresholding is a new technique introduced by imagined this is mainly to solve
the trade-off problem of using large classifier free guidance weights more specifically as
we also discussed in the previous part so when we use large classifier guidance weights there is
a chance that it gives us better text alignment but worse image quality it can be shown in this
figure so as we use large free guidance weights the clip score which corresponds to a better text
alignment increases however the i5d score also increases which corresponds to worse sample quality
so to elevate this issue because we really want both the sample quality
bad like the good sample quality as well as good text elements right so to elevate this trade-off
issue the hypothesis this paper made is that the reason why the sample quality decreases at
large guidance weights is that at large guidance weights usually it corresponds to very large
sample gradients in inference and then the generated samples have a chance to be saturated
because of the very large gradient updates so the solution they propose is this dynamic
sort of thresholding meaning that at each sampling step we adjust the pixel values of the samples
to be within a dynamic range and this dynamic range is computed over the statistics of the current
samples and these two panels shows the qualitative comparisons between statistics a static thresholding
and dynamic thresholding and you can see if you see this if we use this static thresholding
the images look kind of saturated while the dynamic thresholding the samples look more realistic
and another contribution of imagine is that they introduce a new benchmark especially for this text
to image evaluations so the motivation of introducing new benchmark is that for existing
datasets for example coco the text prompt is kind of limited and it's kind of easy so this
benchmark introduced more challenging prompts to evaluate text to image models across multiple
dimensions for example it tries to evaluate the ability of the model to facefully render
different colors numbers of objects spatial relations text in the scene unusual interactions
between objects and it also contains some complex prompts for example long and intricate
descriptions where was and even misspelled prompts to test the robustness of your model
and this feature shows several examples of the text prompts in this benchmark and the
corresponding generated images from imagine using these text prompts
and a bit more of the quantitative evaluations of imagine imagine got state-of-the-art automatic
evaluation scores on the cocoa dataset and it's also preferred over rhythm work by human readers
in both sample quality and image text alignment on the draw bench dataset and the reason work
compared by imagine includes this dow e2 slide vq-gam plus clip as well as the latent diffusion
models okay so besides text to image generation I also want to talk about the controllable
generation using diffusion models and a representative work is this diffusion auto encoders
which propose to incorporate a semantic meaningful latent variables to diffusion models
so more precisely so given an input image a semantic encoder is learned in this framework
to generate this z-scene and this z-scene is fit into a conditional diffusion models to further
predict the clean samples and this is this leads to some likes the bipod latent representation
similar to the dow e2 model where we have this z-scene with the hope that it can capture high
level semantics and we also have this x big t which is the inversion of this conditional ddim
sampler and the hope is that it captures the low-level stochastic variations of the images
and if we want to do unconditional sampling from this model optionally we can learn another
diffusion model in the latent space of the z-scene very similar to the latent diffusion models
we talked about in the last part and to support this unconditional generation task
and interestingly they found that by assuming a low-dimensional semantic vector z they are able
to learn like different semantic meanings for different dimension of this latent vector z
for example by changing the certain dimension of this latent z they are trying to identify
different semantic meanings such as the higher style the expression the the the age and also the
color of the of the of the hair and they also examine that if we fix the z-scene for each row
and we change the latent x big t in the conditional diffusion model we can see it only
corresponds to very tiny details in this image and perhaps other like perceptually invisible
features in the images so that ends the first part of the application thanks for listening
awesome thanks Richie for the nice introduction of the first group of applications here i'm going
to start with the second group of applications and in this part i will mostly focus on image editing
image to image translation and super resolution and semantic segmentation this is of the super
resolution i'll start with talking about this will call super resolution but we are repeated
refinements or sr3 which was proposed by sahari et al at google in image super resolution we can
consider this problem as training a conditional model p of x given y where y is the low resolution
image and x is the corresponding high resolution so we want to be able to generate high resolution
images given some input low resolution image in order to tackle this problem the authors proposed
to train a diffusion model a conditional diffusion model using this objective here in this objective
we have expectation over pairs of high resolution image x and no resolution image y we have
expectation over epsilon which is drawn from standard normal distribution and we have expectation
over time where time varies from for example zero to capital t corresponding to the diffusion
process we have this excellent setup this is a noise prediction network that takes
diffuse high resolution image x t the time as well as this y this is the low resolution image
that is provided as conditioning into epsilon prediction network and we train this epsilon
prediction network to predict the noise that was used in order to generate diffuse high resolution
image the authors in this paper propose to use different norms for minimizing this objective
they introduce l1 no l2 no and they observe that one can trade quality for diversity by using l1
l2 no since we are training a conditional model now you have in mind that we need to modify the
unit that is used for epsilon prediction and for the input of the unit we will have access to
diffuse high resolution image as well as this low resolution conditioning input since these two
images don't have the same special dimensions the author proposed to use just simple image resizing
algorithms to up sample the input low resolution image and concatenated with diffuse high resolution
image and the channel dimension and they provide both of them to the unit diffusion model or the
epsilon prediction model and the network is trained to predict the noise that was used when
generating the high resolution image this this method it is very high quality results for
super resolution for example here you can see super super resolution results with 64 by 64
pixel input when the output is 256 by 256 and you can see that this method here is shown in this
column it shows a really high quality result compared to for example regression models or
just simple image resizing algorithms or using by cubic interpolation and you can see that this
actually does a good job of generating low level details another work that I like to mention is
called palette image to mesh diffusion models this method is also this paper is also proposed by
same authors as the previous method so the leader of this area at all
with similar to super resolution many image to image translation applications can be
considered as training a conditional model x given y where y is the input image for example if you
consider a colorization problem x is the output color image and y is the grade level input image
so similar to the previous part or previous slide we're going to again use we're going to again
train a conditional diffusion model using this objective similarly we have again expectation
over pairs of input conditioning y for example y is again grade level image x is the output color
image we have expectation over epsilon drawn from standard normal distribution expecting
over time we're training a conditional diffusion model that takes input grade level image for
example conditioning time as well as the diffuse output image that we want to generate at the end
and then the model is trained to predict the noise that was used to generate diffuse samples
similar to previous part again we need to give a pair input to the unit model and here because
for example if we're attacking the color problem we're going to have this grade level image
and diffuse color image as input to unit and similarly it's trained to predict the noise that
was trained for that was used for generating diffuse samples the authors tried their image
to image translation image to image diffusion model on four different tasks including colorization
impending jpeg restoration and uncropping which is basically given this image they wanted to
extend the image and provide the copper part of this what they called uncropping and this paper
shows that actually diffusion models can achieve very good results on these four tasks we should
have in mind that this problem this particular paper assumes that you have access to pairs of
input and output data so they're they're training a conditional model assuming that they have input
image and the output image that we want to generate if we don't make that assumption what we can do
we can potentially take an unconditional model that is trained under for example natural images and
we can modify it for a particular task so as example of that approach I want to mention I'd
like to mention this paper called iterative latent variable refinement or ILVR for short
that was proposed by Joi et al at ICCB 2021 this paper proposes an approach where given a reference
image the authors like to modify the generative process of diffusion model such that the output
of the diffusion model can be similar to the reference image so again we have a reference image
and we want to modify the reverse generative SDEs or the reverse generative diffusion process
diffusion process such that we can generate images that correspond to a reference input image
so and the authors and this paper proposes to do this through using some unconditional model
that is not trained for this specific task it's just the unconditional model is trained to generate
realistic for example faces on this side so the basic idea is to modify the reverse denoting
process such that we can pull the samples towards the reference image so here you have the algorithm
proposed in this paper the algorithm starts from capital T goes to 1 so this is the reverse denoting
process at every step we draw a random nodes vector vector from standard normal distribution
we sample from the reverse denoising distribution so we generate this proposal of this color
x prime t minus 1 is the proposed denoist sample to run from the denoising model
why here represents the reference image so we're going to now use the forward diffusion
kernel to generate the diffused version of reference image so we're going to go forward in time for
this reference image and we're going to generate y t minus 1 so ideally what we want to do we want
to make this proposed denoist image x prime t minus 1 to be more similar to y t minus 1 so to do so
this paper proposes this simple operation here phi n represents some low pass filter right
so this operation is very simple we have this proposed denoist image x prime t minus 1 we subtract
the low pass filter applied to this x t minus 1 and we add back low pass filter output of y t
minus 1 so you can think of this operation as operation that takes x t minus 1 the x prime
t minus 1 this is the proposed denoist image it removes its low pass sorry low frequency content
so here we are removing the low frequency content of x prime t minus 1 and we adding back the low
frequency content of y t minus 1 and since we are putting the low frequency content of y t minus 1
this is the reference image into x prime the to x prime denoist the proposal sample what we are
doing we're basically making sure that in the reverse process we're generating a sample where
the low frequency content is similar to the reference image so the most of the structure is
very similar to the reference image so as I mentioned phi n is just a low pass filter
and in order to implement this the author's proposed to use a simply down sampling up sampling
operation where n represents the down sampling ratio used for in this operation for example
when n is equal to 2 it means we're going to just take reference image or take this input
down sample by factor of 2 and then up sample again by factor of 2 so which is which corresponds to
a low pass filter operation here you can see this reference these two reference images and how
we can generate images with different values for n so when n equals to 4 it means that we actually
take this during the generation we don't sample the for low pass filter we don't sample the image
by factor of 4 we don't sample it by factor of 4 since this factor is a small this means that most
of the structure in the generation will be similar to reference image so you can see that in fact
we're generating an image that is very similar to the reference image and as we increase this n
we can see that now different levels of details can can be generated through the diffusion model
and the more global characteristics like more global arrangements or low as low frequency content
of the image still remains the same as the reference image and now to show that actually
you can do this for different tasks image translation given a portrait image they can
generate a realistic image that corresponds to that portrait image they can do paint to image so
they can take all painting and generate realistic image and they can do some simple editing and for
something they can add back this watermark into yeah in this part I'd like to talk about how we can
take representation learned through diffusion models and use them for some downstream applications
such as semantic segmentation and I'm talking about a specific paper called label efficient
semantic segmentation with diffusion models that was proposed by Warren chalk at all at
2022 this paper proposes a simple approach for using the representation train in diffusion
model for semantic segmentation the others propose to take input image and diffuse it
by adding by following the forward diffusion process and they only go to small steps in
forward diffusion step which corresponds to adding just a bit of noise into input image
then they pass this diffuse image to the denoising diffusion model the unit model the epsilon
prediction model and they they extract representation form the internal representation
form in this unit at different resolutions of the unit decoder and then given these
representations they up sample all these intermediate representation so that they have
the same dimension a spatial dimensionality as input in so we have these up sampling layers
we have these feature maps that have the same dimensionality as the input image and now they
simply connected all these uh intermediate feature maps and they pass them to one by one
convolutions that would do semantic segmentation per pixel so you can think of these as a pixel
classifiers that just classify each pixel for each semantic object groups or semantic groups
so in order to train this model they're supposed to use a pre-train diffusion model and they're
only training this component here up sampling component doesn't have usually any training
parameters but most of the parameters the additional parameters are basically here in
these one by one convolutional networks this paper particularly shows that this approach is
labeled efficient using very few labeled instances they can train diffusion models on several
datasets as you can see on this slide and the other show that actually diffusion based segmentation
models can outperform mass auto encoders and can or VAE based models on this task which is very
interesting shows that actually representation during the diffusion models can be used for
downstream applications such as segmentation. In this slide I'd like to talk about the particular
paper particular method that is proposed for image editing it's called SDE edit this paper
was proposed by Ming Itala at Stanford University and it was proposed it was presented at ICLR 2022
the problem that this paper is trying to tackle is that given this stroke painting the authors
propose a simple approach to generate realistic images that corresponds to that stroke painting
the main intuition or the main idea in this paper is that the distribution of real images
and stroke painting images these are two different distribution these two distribution have some
mismatches they're not the same and in the in the data space they they are not completely overlapping
each other because of these differences but if you follow the forward diffusion process if we
have this distribution realistic images distribution stroke painted if we follow the forward diffusion
process these two distribution will start having overlaps with each other because of the definition
of forward diffusion process we know that actually if you have two distribution and you diffuse the
samples in those two distribution the distribution will start having overlap this forward diffusion
simply corresponds to adding noise into input stroke painting now that we know these two
distribution are overlapping we can use just a generative model train um real images to solve
the reverse sd reverse the noise in sdb that will start from this diffused stroke painting
and try to generate a realistic image that corresponds to this uh noisy input and the
authors show that they're actually using a generative model this is an unconditional model again
train unrealistic images they can come back to realistic images that uh where the colors here
are very similar to this stroke painting colors so um this is um i think a very very clever idea to
take stroke paintings and generate realistic images that correspond to those stroke paintings
however uh and the next thing actually we should have in mind that this train in a conditional
setting however we should have in mind that um this approach mostly relies on the color information
in order to to take this stroke painting and generate the corresponding image and this is a
bit different than for example methods that would use the semantic layout semantic mask of objects
in order to to tackle this problem so it has some advantage and disadvantages that we should have in
mind and that shows very interesting um results on different data sets here you can see stroke
paintings and for model strain on this on bedroom there's some church and cell of a and you can see
here in these two row how a generative model using sd8 not can be used to generate realistic
images that correspond to stroke paintings lastly in this part i'd like to talk about
particular work that we did at nvidia for adversarial robustness and in this particular
work we introduced um diffusion models for adversarial clarification so the basic problem
we want and this is that given an adversially perturbed image we want to see if we can use
diffusion models to remove adversarial perturbations and clean this image such that when we apply
classifier on these adversially perturbed images we can actually get robust classification uh
output so the this the proposed idea here is similar to st edit mostly applied for adversarial
clarification given this adversarial perturbed image uh what we propose to do is we propose to
follow the forward ste which correspond to basically adding noise and diffusing the input
adversarial perturbed image using just a forward diffusion kernel so we go to particular times
that t star we call and we and we simply diffuse the input adversarial perturbed image we know that
by adding noise we can now wash out all these adversarial perturbations that are applied
into image now that we have this noisy image we use the reverse generative ste or reverse
noisy ste to start from this noisy input and generate clean image that corresponds to this
noisy image and we know that through this process we can now remove all the noise that was injected
as well as all the adversarial perturbations presented here in this image now that we have this
clean image we or purified image we can just pass it to classifier and hopefully make a robust
classification prediction and like any adversarial difference mechanism we need to be able to attack
this model in order to evaluate our performance we need to be able to attack this model so in
this paper we also show how we can attack this model by backbomb gating end to end through
classifier as well as our purification algorithm and this involves basically backbomb gating through
this reverse ste so we showed in this paper how we can do this and how we can attack this mechanism
end to end on the left side in this slide you can see an example of adversarial perturbed images
and first column on a set of a data set here we intentionally increase the magnitude of
adversarial perturbations so that they are visible to us these two groups are representing
this two images are representing adversarial perturbation for a smining class and these two
represent adversarial perturbation for eyeglasses here you can see diffuse samples that are
generated by following the forward diffusion this is simply corresponds to sampling from diffusion
kernel and then here in these two columns you can see samples that are generated when we're solving
the reverse genitive ste and as you can see at time equals to zero we can remove not only the
adversarial perturbations as well as all the nodes that was injected through forward diffusion
process and you can see that our generated energy equals to zero are very similar to input clean
original images and we can see that the semantic attributes of these images are very similar to
semantic attributes of the original images the nice thing about using a genitive model for
adversarial purification is that these modes are not trained for specific attacks and specific
classifiers so at the test time we can just apply them for unseen adversarial attacks in
comparison to the state of dark methods that are designed for similar situations for unseen threads
we actually see that our proposed diffusion purification method ought to perform these
methods by a large margin and we believe that in fact diffusion models can can be very strong
models for designing adversarial purification techniques and this is primarily because diffusion
models can generate very high quality images and potential can be used for removing all the
artifacts that are generated by adversarial perturbations. This brings me to the end of
the second part of applications next Carson will continue with the third part of applications.
Thanks.
All right I will not talk about video synthesis medical imaging 3d generation and discrete
state diffusion models. Let's get started with video generation. Here are samples from a text
conditioned video diffusion model by Jonathan Howe et al where we condition on the string fireworks
so I think these samples look pretty convincing. So there are actually and generate different
video generation tasks for instance there is unconditional generation where we want to
generate all frames of the video from scratch without conditioning on anything there is a
future prediction where we want to generate future frames conditioning on one or more past
frames we can also do past prediction the other way around and there is also interpolation
when we have some frames and we want to generate in between frames just for instance useful to
increase the frame rate of the video. All these generation tasks can basically fall
under one modeling framework in all cases we basically want to learn a model of form
setter of xt1 to xtk given x tau 1 to x tau m and now those t's and tals
denote the times for the frames that you want to generate and for the frames that we condition on
so for future predictions these tals will already be smaller than the t's or unconditional
generation and we wouldn't have any tals and so on and so forth. Something we see in multiple
of these recent works is that they try to learn one diffusion model for everything so what they do
is they concatenate and combine both the frames to be predicted and the conditioning frames together
and then some of these frames are masked out so ones to be predicted and yeah based on the
conditioning frames those are then generated and varying the masking and conditioning combinations
during training we can train one model for these different tasks so during training we would also
tell the model which frames are masked out and we would fit the model time position encodings
to encode the times for the different frames that's visualized here for instance. In terms of
architecture these models are still using these unit which we already know from the image-based
diffusion models but there's no like small detail so of course now our data is higher
dimension because in addition to the image and height and width dimensions as well as the channel
dimensions we now also have the time dimensions with the number of frames so the data is essentially
four-dimensional so how can we deal with that one way is to use now 3d convolution instead of
2d convolutions to run convolutions over height width and the frames this can be computationally
expensive another option is for instance to keep spatial 2d convolutions and use attention
layers along the frame axis some papers do that this has the additional advantage that
ignoring those attention layers a model can be trained additionally on pure image data which is
kind of nice so let's see some results it turns out that these video generation diffusion models
they can actually generate really long-term video in a high-watt manner which is quite impressive
and there we precisely leverage these these masking schemes that we just had and this
these generalized video diffusion frameworks so one thing you can do for instance is we generate
future frames in a sparse manner by conditioning on frames far back this gives us long-term
consistency and then we interpolate the in-between frames afterwards so we kind of generate the
video in a stage-wise hierarchical manner and with that it's possible to actually generate
really long like one hour coherent videos which is quite impressive so here are some samples from
this recent Harvey at our work all right let us not talk about another application of diffusion
models which is solving inverse problems in medical imaging another very relevant application
so medical imaging may refer to computer tomography or medical or magnetic resonance imaging
in those cases we're basically interested in like an image x but that is not what we are actually
measuring from like the CT scanner or MRI scanner so let's consider the measurement process for
instance in computer tomography the forward measurement process can be modeled in the following
form given the image we are basically performing a radon transform which gives us a sinogram and
then maybe this is sparsely sampled so we end up this is sparsely sampled sinogram right and now
the task is that needs to be solved to reconstruct the image given this measurement y so this is an
inverse problem it's similar in magnetic resonance imaging just that the forward
process is now basically modeled with Fourier transform and which is then sparsely sampled
so and this is where diffusion models now come in so they can actually be really used in this task
and the highlight idea is here to learn in a generative diffusion model as a prior over the
images we want to reconstruct and then while sampling from this diffusion model we guide
synthesis condition while conditioning on the sparse observations that we have this is the idea
and it turns out that doing this actually performs really really well and even this out
performs even fully supervised methods sometimes what does it mean specifically the thing is when
we train this diffusion model over these mct or mri images we really just need the images we do not
need paired image measurement data to train this so yeah there is actually a lot of work in that
direction because this is a really high impact application of course and yeah here are some
citations if you are interested in learning more about this
let's move on to the next application topic which is 3d shape generation also 3d shape
generation has recently been tackled with diffusion models so let us consider this work by
zoo at our for instance here 3d shapes are represented as point clouds so point clouds
have the advantage that they can be diffused really easily and intuitively we see this here at
the bottom right so where the diffusion actually goes from the right to the left we have a bunch
of points and they are perturbed in 3d towards this yeah Gaussian noise ball kind of and the
generation goes into the other direction so in those cases the architectures that we use to
implement the denoiser network are like typical point state of the art modern point cloud
processing networks like no point net advanced versions of point and point voxel cms and so on
and so forth how does this look like then yeah here's another animation
I think this is quite nice so yeah we can generate these pretty good shapes
we can also train conditional shape completion diffusion models very condition for instance
on like measure depth or like some sparse points like this and then complete those shapes we can
do that in a multimodal fashion for instance in this example we have those um legs of the chair
given and now we have like different plausible um completions of the chair here
and something that is also quite cool is that even works on real data I think here the model
was trained only on synthetic shape net data and yet we can fit the model images that we
take these depth images and generate plausible 3d objects very nice
all right finally I would like to talk about discrete state diffusion models this is less
of an application but it is a slightly different type of diffusion model but I think it is worth
mentioning as part of this tutorial so so far we have only been considered continuous diffusion
entity noising processors which I mean with that is we basically kind of assume our data is of
a continuous nature and we could add a little bit of Gaussian noise to it in a meaningful way so
both our fixed forward diffusion process and also our reverse generative process are usually were
usually implemented as Gaussian distributions like here but what if our data is discrete
categorical then continuous perturbations are not meaningful they are not possible imagine for
instance our data is text data or you know pixel wise segmentation labels or discrete
image encodings then yeah if our data is discrete adding Gaussian continuous noise to it doesn't
really make much sense so can we also generalize this diffusion concept to like discrete state
situations in fact there are categorical diffusion models and in those cases the forward
diffusion process or like the perturbation now is defined using categorical distributions
for instance let us consider the perturbation corner q of xt given xt minus 1 that is supposed
to perturb the discrete data so this can now be a categorical distribution where the probability
to sample one of the t-juice is now given by some transition matrix q multiplied together with
the state we are in xt minus 1 so the probability to sample like the new state xt
so this xt is then usually a one hot state vector describing the state we're in and yeah
this transition matrix multiplied to it will then give us probabilities to sample the next state
so with that we can perturb complex distributions categorical distributions towards like very
random discrete distributions if we choose this transition matrix accordingly so for
instance in this example if we look at the right this may now be a complex data distribution
we can perturb this towards a uniform discrete distribution over these three different states
one three one two three and then the reverse process for generation and which is then
implemented through a neural network we can also parameterize this as a categorical distribution
so in fact there are different options for this perturbation process this forward perturbation
process one thing is we can use uniform categorical diffusion where we pull everything towards a
yeah uniform distribution over the different categories like i've just shown
we can also progressively kind of mask out the data where we pull everything into one
particular state we can also analytically sample from such a distribution so it's also well suited
for diffusion model we can also tailor our diffusion processes to ordinal data and use
something like a discretized Gaussian diffusion process that's also possible
how does this look like for instance so here now i have the data distribution it's a bit complex but
so this is basically each pixel of this image represents one categorical variable and now the
color of this pixel represents which category we are in so now if i would do like this uniform
to go into diffusion i would kind of you know yeah it would look like this where i would
transition into different states everywhere in the image i could also do something like
Gaussian diffusion where it's more like it's this ordinal thing where it's more based i transition
to neighboring states and then there was also this absorbing diffusion where i kind of progressively
mask out or absorb my states sort of um these are different ways to do this and then in the
reverse process this may for instance look like this so here on the far right this is the stationary
distribution of this categorical distribution from which i can sample analytically and then
denoising kind of progressively denoises this back towards the data distribution
so yeah one can use this and some papers have explored such discrete state diffusion models
for instance we can also apply this on images by modeling the pixel values of images as discrete
states to be in and one can do this that from uniform uniformly distributed pixel values
right here from this all gray kind of state or all masked out state
another application is to use this in a discrete latent space so in this work for instance images
encoded using vector quantization techniques into visual tokens in a discrete latent space
and then we can use something like discrete diffusion models and similar techniques to model
the distribution over these visual tokens this is also something one can do one can also use
discrete state diffusion models to generate segmentation maps which are also categorical
distributions in pixel space and yeah that concludes my part and with that i would like to
pass a mic back to Arash who will now conclude our tutorial thank you very much
also thank you for being with us this basically brings us to the last part conclusions
open problems and final remarks so today was a big day we learned about diffusion models at the
beginning of this video i started talking about denoising diffusion property models which is a
part which is a type of discrete time diffusion models i showed you how these discrete time
diffusion models can be described using two processes a forward diffusion process that
starts from data and generates nodes by adding nodes into the input and then reverse denoising
process that learns to generate data by starting from noise and denoising the input image one step
at a time i also talked about how we can train these diffusion models by simply generating
diffuse samples and training network to predict that to predict the noise that was used to generate
diffuse input images in the second part Carson talked about the score-based generative modeling
with differential equation which corresponds to continuous time diffusion models specifically
Carson talked about how we can consider diffusion models in the limit of infinite number of steps
and how we can define or how we can describe these forward and reverse processes using
stochastic differential equations or STEs he also talked about probability flow
ordinary differential equations or ODEs which describe a deterministic mapping between noisy
between the noise and data distribution and data distribution the nice thing about working
with stochastic differential equations or ordinary differential equations is that we can
actually use the same training that was used for training different discrete time diffusion models
in the previous slide however at the test time we are free to choose different discretization
or different OD or ST solvers that have been studied widely in different areas of science
and this allows us basically to change the sampling time by using for example OD or ST
solvers that don't require a lot of functional validations in the third part Rucci talked about
advanced topics in diffusion models she mostly focused on accelerating sampling from diffusion
models and she studied this from three different perspectives including how we can define forward
processes that accelerate sampling from diffusion models how we can come up with better reverse
processes or how we can come with better denoising models that allows us to access sampling from
diffusion models beyond that she also talked about how we can scale up the fusion models
to generate high resolution images in conditional and conditional setting actually
specifically talked about cascaded models and guided diffusion models that are heavily used
in the current state-of-the-art image-to-text diffusion models such as Imagine or Dunge2
after talk about fundamental topics all three of us talked about various
computer vision applications that have been recently proposed and mostly applications
that rely on diffusion models at their core recently so now that we know about diffusion
models and we know how we can use these models in practical applications let's talk about some
open problems I do hope that now we could make you interested in this topic and now that you know
the some fundamentals in this area maybe you can think about open problems that exist in this space
and together we can tackle some of these the first problem I want to first of problems I want to
mention are more on the technical side and later I will talk about more applied questions that
rise as in practice so if you remember at the beginning of the talk most Carson and I talked
about how diffusion models can be constructed as a special form of MIEs or continuous time
normalizing flows but we exactly don't know why diffusion models do much better than VAs and
continuous time normalizing flows if we can understand this maybe we can take the lessons
learned from diffusion ones and why they do so much better than VAs and continuous time normalizing
flows in order to improve these frameworks meaning we can maybe use the lessons learned
from diffusion ones to improve VAEs or normalizing flows even though there has been a tremendous
progress in the community for accelerating something from diffusion models we can still
do in the best case scenario we can still do actually the sampling using four to 10 steps
on the small matrices such as psi part 10 however the main question still remains on how we can get
one step samples for the diffusion models and this can be very crucial for interactive applications
where a user interacts with the diffusion model and this we can reduce the latency
that usually uses observe when they are using generative models so one of the existing problems
have to define one step samplers and part of the solution might be to come up with a better
diffusion process that are intrinsically faster to generate sample from
diffusion models very similar to VAEs or GANS can be constructed as latent variable models
but their latent space is very different for example GANS we know in the latent space often
have semantic meaning and using like latent space manipulations we can actually come up with image
editing or image manipulation and manipulation frameworks but in diffusion models the latent
space does not have semantics and it's very tricky to come up with latent space semantic
manipulation diffusion models so part of problem here is how can we define
semantically meaningful latent space for diffusion models that allow us to do semantic manipulations
in this talk we mostly focus on generative applications but one open problem is how we
can use diffusion models for discriminative applications for example one way of using the
diffusion models might be for representation learning and we might be able to tackle high-level
tasks such as image classification versus low-level tasks such as semantic image segmentation and these
two may require different traits of and then we are trying to use diffusion models to address these
another group of applications that may benefit from diffusion models is uncertainty estimation
one question is how can we use on diffusion models to do uncertainty estimation in downstream
discriminative applications and finally one question that remains open is how we can
define joint discriminator generator sorry joint discriminator generator models
that not only classify images or input they also can generate similar inputs
so the committee mostly have been using unit architectures for modeling
score model in the score function in diffusion models but one question is whether we can go
beyond units and come up with better architectures for diffusion models and one specific open area
is how we can feed time input or other conditioning into diffusion models and how we can potentially
improve the sampling efficiency or how we can reduce the latency of sampling from diffusion
models using better network design so far in this talk we mostly focused on image generation
but we may be interested in generating other types of data for example 3d data that has
different forms of representation for example it can be represented by stance function
measures boxes or volumetric representation or we might be interested in generating video
text graph which have their own characteristics and even these characteristics we actually may
need to come up with specific diffusion models for these particular modalities
one area of research is to do composition and controllable generation and this will allow
us to go beyond images and be able to generate larger scenes that are composed of multiple
for example objects and also this will add the techniques problem will allow us to have
fine grain control in generation so one interesting open problem is how we can
achieve composition and controllable generation using diffusion models and finally I think if we
look back look back to the vision community and the problems that we solved in the past few years
we see the most of the applications try to solve most of the applications that rely on
generative models they try to solve the most of the some solve with generative adversarial
networks so maybe it's a time for us to start revisiting those applications and see whether
they can benefit from the the nice properties that diffusion models have so one open question is
which applications will benefit most from diffusion models not given that we have such
amazing strong tool this brings me to the final slide I want to say thank you for being with us
today this was this is a very long video and I do hope that we could provide some useful and
fundamental background on diffusion models and how often they are used in practice
all three of us are active on Twitter if you are interested in knowing about follow-up works
that we we build on top of diffusion models please make sure that you follow us and lastly
I want to mention that all the content on this week and this video including slides will be
available on this video on this website if you happen to enjoy this video I would like to ask
you to share this video with your colleagues and collaborators and hopefully together we can
come with more people to start looking into diffusion models and applying them to various
interesting applications thanks a lot

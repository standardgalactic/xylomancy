So I made a video early last month about how I thought the current trend of large language model,
AIs, would impact the software development industry over the next handful of years.
Since that time, there's been a lot more discussion about that,
a lot of it driven by the release of ChadTBT40.
Now that release has caused a lot of people to buy even more into the hype.
Some of those people are in the comments on this channel, some have even been for major news outlets,
and that makes sense. The hype says that within a handful of years we will have human form factor
robots with roughly human intelligence for what could be the current price of a luxury car.
Or the haters say that LLMs will soon be as dead as NFTs.
As always, or at least almost always, the truth surely lies somewhere in between, but where?
We know there's going to be disruption in the job market. Indeed,
AIs already caused issues in the job market. I've talked about that several times.
We're starting to see companies reporting that they're using fewer programmers due to AI,
or holding off on hiring due to AI. For example, BP is saying they're now needing 70% fewer
coders, looks like contractors. And yet the number of times AI was mentioned on earnings calls this
past quarter was a fraction of the number of times it was mentioned in any of the previous
four quarters. So which is it? Is it cutting into programmers a lot, or is it dying out?
Right now we don't know. It's possible the staff reductions are only temporary,
and that the code quality will be poor enough that they need to hire more programmers to fix it.
We won't know that for a while, and neither will those companies.
Lots of bugs take a while to get noticed, especially when you're not actually trying to find them.
And we don't know how the numbers of programmers reduced at companies like BP compared to the
numbers of programmers that are being hired because of AI. So at the moment, while things are in flux,
and companies are making bets, and we don't know which way the bets will turn out,
what do we do? I say we try to look further ahead. We try to look at the fundamentals and try to
guess how we think it's most likely to end up, if not when. If it's a bubble, it will pop. They
always do, although it's basically impossible to predict when they will pop. If it's not a bubble,
and the hype is correct, it's going to be a huge societal disruption. And if that's the case,
it's going to be really hard to plan for, or both. Back in 99,000, there was a bubble in tech startups,
and it popped, and there was a drop in overall tech sector jobs that didn't start growing again
until about 2004. But the underlying tech was real, and it was valuable, and it enabled the
internet as we know it today. The promise of the internet back then took 15 or years or so
from the start of the bubble to really take off. And maybe that's the kind of time we're talking
about. It's kind of hard to know yet, which means we need to know what we can about whether this
is a hype bubble and will pop soon, or a tech that's fundamentally disruptive. And where we
have to start that investigation, because it's the best data point we have right now, is with Chat
GPT 4.0. And based on what I've seen so far, I think it's pretty clear that
this is the Internet of Bugs. My name is Carl, and this was originally going to be an explanation
of what I thought about Chat GPT 4.0 and why. And it still is, but it ended up being a little
more ranty at the end. Hopefully you'll stick with me till then. So to start with, Chat GPT 4.0 is
much faster than I expected. It's very fast, and that's great. Aside from that, though, I'm not
really impressed. The thing that everyone seems excited about is the voice interface. And I think
that most of the people excited about that haven't really been paying attention. As I explained in my
AI agent video, before 4.0 came out, we had voice and camera graphics interfaces to Chat GPT and
other LLMs for a long time now. Below, I've linked instructions on hooking Chat GPT up to a voice
interface. From February of last year, the functionality works out of the box now, and that's
great. It's very convenient, and it's faster. But it's nothing new, and it's not really an
advancement. Doing tasks cheaper and faster isn't nearly as disruptive if the tasks aren't difficult,
or they aren't done well, or they aren't done correctly. And doing a wide variety of tasks
correctly determines how autonomous an AI can be, and therefore how many and what kinds of jobs it
might be able to displace. And most relevant to my expertise, how it might impact software jobs,
and how the jobs that have already been impacted are likely to stay impacted or likely to come back.
So the next question is, what do we know about 4.0 doing tasks better or more correctly?
My personal experiences with it have been disappointing. In my last video, I talked about
how a perplexity.ai found a Steve Jobs quote from 1983 for me, but 4.0 told me it didn't exist,
and for the record, Google couldn't find it either, even if I gave Google the exact quote to search for.
But I started my career as a physicist, and physics and science is all about experiments and
evidence. And that's really served me well over the years, believe it or not, both in being able
to separate height from reality. But also, when I'm in my day-to-day work and I'm troubleshooting or
debugging, using evidence to actually narrow down what the real problem is and finding and fixing
it and figuring out what experiments to run to make the bug show up has been incredibly useful.
I do my best to gather the evidence I can find, I look at the evidence I've gathered,
I draw conclusions. If I see new evidence, I'll revise the conclusions. This is a pretty much
constant process. It has been pretty much my whole career. When I hear news stories or read
articles that are relevant to technology trends, I see if it's information that changes my guesses
about what's popular or what's becoming more or less important. Even before I started this
channel, I did this. It's important for me to decide what I'm going to spend my time on,
and, being frank, if that's not something that sounds like you'd want to be keeping up with for
the rest of your career, then maybe a technology career is not for you. So briefly, and so I can
point people to this section of video when they spout garbage in my comments, what counts as evidence.
One, researchers publishing peer-reviewed papers are the gold standard of evidence.
They aren't always correct in hindsight. People make mistakes. Some falsify even
data. But that's as good as evidence ever gets, and the papers that are wrong eventually get found
out. Two, benchmarks are evidence. In fact, that's the whole point of benchmarks. Benchmarks are
generally what are used in most of the peer-reviewed papers that we have about LLM performance.
Some people in the comments have tried to argue that benchmarks aren't evidence or don't count
as evidence or that they can't be taking at face value for some reason. What I can tell you is
lots of researchers use those benchmarks as evidence in paper after paper. Many of those papers
are linked in this video, and as long as they keep doing that, so will I, and I won't care about your
opinion. Three, firsthand observation of facts, not speculation, not things that might happen in the
future, but things that have actually happened count as evidence if they come from unbiased sources
that have relevant experience. And lastly, and least usefully, trends actually are evidence. It's
pretty crappy evidence, but when it comes to predicting the future, that's often all we've got.
But it only counts if the circumstances in the future are the same as the circumstances in the
past. But if the circumstances might be materially different, then I don't trust the trend will
continue, and I don't think that you should either, and I don't count that as evidence. So to be explicit,
a commenter on a YouTube video's opinion on the future doesn't count unless it's backed up with
sources. A commenter's opinion about a video or an article doesn't count unless they back it up with
sources or facts. But the same is true of me. I try to back up the information I give you and the
conclusions I draw with sources. My video descriptions usually have lots of them. When I talk about
things I'm an expert in, I try to give examples and sources and citations and further reading,
and I try to explain what scenarios and experiences I've seen that lead me to draw the conclusions
that I have. At the time of this recording, there are close to 60 URLs in the list of references
for this video. So back to chat GPT-40. The consensus is it's way faster. I've been told it's much
better at many non-English languages, although I'm not an expert in that. It interacts with
sound and images without having to have preprocessors and post processors to convert, and that's cool
and convenient. But when it comes to accuracy, it's sometimes better and sometimes worse.
So MMLU is 2.2% better than 4T. GPQA, which is a science benchmark, is 5.6% better. But on the
drop benchmark, which is complex reasoning and math, it's 2.6% worse than 4T. And there's a new
benchmark called SEAL at which 4.0 is actually worse than 4T. It's a very promising new benchmark.
Now don't take my word for it. Here's a link to a tweet from Andre Karpathy, who's the former
Tesla director of AI. Hope I got that name even close to right. He's an open AI founding team member.
I talked about him in my last video. There were some complaints in the comments on my earlier videos
about some benchmarks that I graph that were on a scale of 0 to 100%. SEAL shows the same kinds
of trends, but it doesn't have that limitation. And now that it is this paper about how chat
GPT's behavior is changing over time, note that this paper only compares 3.5 and 4. And so I'm
looking forward to when they add 4.0 to it. But according to this, there's actually not a lot
of improvement between 3.5 and 4 even. So you can choose to believe that 4.0 is better if you want,
but the evidence is mixed. So it can't be dramatically better, at least not in the realm of
how well it's able to correctly perform tasks. And according to some research on some benchmarks,
there hasn't been a lot of improvement since 3.5. So if that's the evidence we have about 4.0,
what does it tell us about the future? So this graphic, I guess this counts as evidence,
it counts as a fact, but it's evidence of the fact that they're going to be spending a lot
of money training GPT5. It doesn't actually tell us anything about how much better or worse it's
going to be at what tasks it can do and how well it can do them, despite what people on the internet
tell you. We know that there's a trend that the models have been getting exponentially faster and
cheaper for years, and that seems to have continued through 4.0, so it's likely to continue for some
time, and that's good for lots of reasons. But when it comes to tasks and correctness,
we know there's mixed evidence. And so we don't really know how to extrapolate from that. So the
best method of prediction in the future is about extrapolating from trends, what other trends are
there that we can draw from? It turns out there are two important ones, and they're not actually
discussed very much, which is interesting. The first thing we don't talk about much is a bunch
of experts who have been doing relevant research for a very long time, but very few people in the
tech industry have paid any attention to them at all because they're not part of the tech industry.
These people are psychologists and ethicists and philosophers who study not what neural networks
can do, but why humans are predisposed to be particularly gullible about the sentience of AI.
It turns out that the human brain seems to be hardwired to believe that things that we interact
with have thoughts and feelings and desires and such and such. I'll put links down below,
there's a very long history of people attributing thoughts and wants and desires to
everything from weather and nature to tools to pet rocks.
And not only do we naturally tend to believe that non-human things have thoughts and feelings,
but LLMs have been giving attributes that make us even more likely to believe it. There's a thing
called the ELISA effect, which has been known since the 70s, where a chat interface causes
powerful delusional thinking in humans that has been likened to a slow acting poison.
So there's a term that has been coined by researchers that's called dark patterns.
It describes user interfaces in products that trick people or trick the brain into behaving
in ways that the producer of the product wants that is contrary to the interests of the consumer
or the user. Dark patterns are a huge topic and researchers are just starting to study the dark
patterns in LLM chatbots, paper below. But two recent other papers on dark patterns are particularly
relevant to chat GPT4O. One of them is about how synthetic voices impact human decision making,
and the other is about how cuteness is a dark pattern. Does the fact that 4O has not only
synthetic voices but cute affects like giggling? Does that influence what people think of 4O?
That specific research hasn't been done yet, but based on past findings, I expect it's going to be
very revealing when it happens. But we know that, intentionally or not, LLM chatbots have been designed
in a way known by psychologists and ethicists to trick humans into believing that they're intelligent,
and that trend is getting worse and shows no indication of getting better.
One more trend that's relevant to all this, it's been going on for at least eight years with respect
to AI specifically, and a whole lot longer in general. So for once, I'm only going to go back
as far as 2016 for brevity's sake. I could go back farther, but it would be less relevant.
So last year, we found out in a lawsuit deposition that Tesla faked a self-driving demo in 2016.
The Independent, which is an outlet in the UK, recreated a Tesla demo in 2022 and found that
actually crashed right into a cutout of a child as opposed to what the Tesla demo did.
Okay, enough of Tesla, let's talk about Google for a minute. So in May of 2018, Google famously
faked its duplex AI demo. Oh, and Google faked their Gemini AI demo in December of 2023, so they
didn't learn much. Recently, Google claimed that their deep mind created 2.2 million new materials.
BitActual researchers said, quote, we examine the claims of this work and unfortunately find scant
evidence for compounds that fulfill the trifecta of novelty, credibility, and utility. In other words,
very few of the 2.2 million compounds that Google claimed are of any use or haven't already been
discovered. A Google BP recently released a thing that said, quote, in addition to designing AI
overviews to optimize for accuracy, we tested the feature extensively before launch. This included
robust red teaming efforts, evaluations with samples of typical user queries, and tests on
proportion of search traffic to see how it performed. And yet Google AI said that we should
eat at least one small rock per day and add about an eighth of a cup of gluta pizza sauce.
So we can really trust Google. Let's talk about Amazon. So Amazon's AI checkout technology,
the let's walk out technology turned out to be thousands of remote workers in India instead
of an actual AI. Ironically, that was done via Amazon's online task platform that's called
Mechanical Turk, which is named after a famous fraud from the 1770s, where a chess-playing robot
just turned out to be a man hiding in a box, pulling its strings. Usually I don't go back to the
1770s, but I guess it happens sometimes. And there have been a bunch of times when something
that was said to be AI just turned out to be a bunch of remote people doing the work. So GM's
cruise self-driving car technology used 1.5 remote humans for every vehicle on the road
to, quote, remotely control the car after receiving a cellular signal that it was having problems.
Facebook had a Siri competitor called M, and it was supposedly supervised by humans. Reportedly,
though, quote, in practice over 70% of requests were answered by human operators. It was shut down
in 2018. Supposedly, it was very expensive. The SEC has recently started charging companies with
what they call AI washing, which is when companies say that they're doing things with AI when there's
actually no AI involved. And then there's things that do use AI, but that companies insist on lying
about how well it does the AI. So on January 23rd, 2024, Microsoft said, quote, Microsoft aims to
provide transparency in how its AI systems operate, allowing users and stakeholders to
understand the decision-making processes and outcomes. 48 hours after that, Microsoft's products
were used to make viral, deep, fake, non-consensual porn of Taylor Swift. After the Taylor Swift
deep fake porn went viral, it was reported, quote, a Microsoft AI engineering leader says he discovered
vulnerabilities in the OpenAI's Dolly 3 image generator in early December, allowing users to
bypass safety guardrails to create violent and explicit images, and that the company impeded
his previous attempt to bring public attention to the issue, so much for transparency.
The Rabbit R1 demo showed a bunch of things that reviewers said just don't work,
and it turned out to be just an Android app. The Humane AI pin gave false information in its
demo video, and then the company quietly re-edited the demo with new audio to make it look like it
gave the right answers. The AI pin was famously called the worst product a particular reviewer
had ever reviewed. What about OpenAI though? Have they been caught lying about demos?
Quote, perhaps the most widely touted of GPT-4's at-launch zero-shot capabilities has been
it's reported 90th percentile performance on the uniform bar exam. Well, new reports say it was
actually in the 15th percentile of those that took the test for the first time. Turns out that what
OpenAI seems to have done was arranged it so that their AI got compared to a bunch of people that
had taken the test before and failed the test before and were really likely to fail it again,
and it got better than 90 percent of people that were likely to fail it again.
At least one of OpenAI's Sora demo videos was done by an FX group called Shy Kids. Link below
for that. There was a recent interview with a former OpenAI board member who said that
Sam Altman had created, quote, a toxic culture of lying. Huh. Oh, and then moving on from OpenAI,
there was this thing called Devon. The company that made Devon kind of lied about that.
There's a video about that you might have heard of it. And there were other examples I didn't
put in this list because they were less directly related to AI. And keep in mind that's just the
companies that have been both caught and that were high enough profile to actually make headlines
and get reported about. I've been part of many software demos and many product launches over
the last 35 years. And in my professional opinion, in my experience, lots of demos lie.
There's nobody know for sure, but based on my past experience, I'd guess that maybe
for every demo that gets exposed, at least a couple of demos get away with it, maybe more.
And yet we have people insisting that this is all real. A Wired.com article called It's Time to
Believe the AI Hype came out on the 17th of May, just after the Chad GPT-40 demo. It tried out the
same old joke about a friend that takes another friend to a comedy club to see a dog telling jokes.
And the first friend says, what do you think? And the second friend says,
well, the joke's bombed. I'm not that impressed. The article then says, folks, when dogs talk,
we're talking biblical disruption. And maybe, but let me ask you this. If you're confronted with a
talking dog at a comedy club, what's more likely? That there's a dog that's actually talking or
that, like Amazon Fresh, what's supposed to be an unprecedented breakthrough is really just a bunch
of people behind the scenes trying to trick you. I know what I'd bet on. And to top it off, the
concluding paragraph of that article insists, quote, this is a direct quote, but the demos aren't lying.
Except for, you know, Tesla self-driving and GM's crew self-driving and Google Duplex and
Google Gemini and Facebook's M Chatbot and OpenAI's Sora and Chad GPT's Forrest Bar exam,
and Amazon Fresh's Just Walk Out and RabbitR1 and HumanAI's PIN. Oh, and Devin again.
Deep breath. So, there's no clear evidence of accuracy and task performance getting better.
There is clear evidence that the features being added to these products are attributes like voices
that are known to McHughans more likely to believe that the products actually think.
There is clear evidence that the companies have been lying about AI's capabilities for years,
not all the companies, but many of them. And there's clear evidence that journalists, again,
not all of them, but many of them, have and will continue to make irresponsible statements like
the demos aren't lying, all evidence to the contrary. But we have to make decisions about
what we're going to spend our time on. And we have to decide what we're going to learn
and what we're going to avoid and whether we want to switch majors or switch careers or what.
What's going to happen with AI is one of the bigger questions in software careers right now.
In tech, maybe even in the world, and I'd really love to know what's going to happen. And if I did,
I would tell you, but I don't. And honestly, nobody does. What I know is that there's currently no
clear evidence that we're going to get human level artificial intelligence anytime soon.
And what we really know for definite sure is that many of the people that are telling us
how great it's going to be, not only have a financial incentive to lie about that,
they've been lying about it for years and have been lying so obviously about it that
they've been caught lying about it over and over. And that many people we should be able to trust,
like journalists who should know better, keep saying things like the demos aren't lying,
even though many, many of the AI demos have been lying and have been proven to be lying
over and over since at least 2016. So we're on our own, and we have to make up our own minds.
And like generations of scientists going back to at least the 16th century,
I'm going to choose to follow the evidence. And I'm going to choose to reject the narrative
that's being promoted by the people who have a toxic culture of lying. But hey, you do you.
Never forget, the internet is full of bugs. Anyways, as differently, probably thinks you're
so stupid that you would believe that dogs can actually tell jokes. Let's be careful out there.
